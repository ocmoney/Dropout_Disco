{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c26386a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data: [(['anarchism', 'originated', 'as', 'term', 'of', 'abuse'], 'a'), (['originated', 'as', 'a', 'of', 'abuse', 'first'], 'term'), (['as', 'a', 'term', 'abuse', 'first', 'used'], 'of'), (['a', 'term', 'of', 'first', 'used', 'against'], 'abuse'), (['term', 'of', 'abuse', 'used', 'against', 'early'], 'first')]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_corpus(file_path, window_size=2):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        words = f.read().strip().split()[:200000]  # Split the text into words\n",
    "        \n",
    "\n",
    "    data = []\n",
    "    for i in range(window_size, len(words) - window_size):\n",
    "        context = words[i - window_size:i] + words[i + 1:i + window_size + 1]\n",
    "        target = words[i]\n",
    "        data.append((context, target))\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "file_path = \"text8.txt\"\n",
    "window_size = 3\n",
    "data = preprocess_corpus(file_path, window_size)\n",
    "print(f\"Sample data: {data[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e076074",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def build_vocab(self, corpus):\n",
    "        for word in corpus:\n",
    "            if word not in self.word2idx:\n",
    "                self.word2idx[word] = len(self.idx2word)\n",
    "                self.idx2word.append(word)\n",
    "\n",
    "# Build vocabulary from the corpus\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab([word for context, target in data for word in context + [target]])\n",
    "print(f\"Vocabulary size: {len(vocab.word2idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vocab = {\n",
    "    \"Hello\": 72,\n",
    "    \"my\": 44,\n",
    "    \"name\": 21,\n",
    "    \"is\": 93,\n",
    "    \"Bes\": 11\n",
    "}\n",
    "\n",
    "sentence = [\"Hello\", \"my\", \"is\", \"Bes\"]\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.emb = torch.nn.Embedding(128, 9)\n",
    "        self.linear = torch.nn.Linear(9, 128)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embs = self.emb(inputs)\n",
    "        embs = embs.mean(dim=1)\n",
    "        out = self.linear(embs)\n",
    "        probs = F.log_softmax(out, dim=1)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e9b48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 19.655333042144775\n",
      "Epoch: 2, Loss: 19.640313148498535\n",
      "Epoch: 3, Loss: 19.62529754638672\n",
      "Epoch: 4, Loss: 19.61028528213501\n",
      "Epoch: 5, Loss: 19.595277309417725\n",
      "Epoch: 6, Loss: 19.58027219772339\n",
      "Epoch: 7, Loss: 19.565271377563477\n",
      "Epoch: 8, Loss: 19.550273895263672\n",
      "Epoch: 9, Loss: 19.53528118133545\n",
      "Epoch: 10, Loss: 19.520292282104492\n"
     ]
    }
   ],
   "source": [
    "def train(model, sentence, vocab, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(len(sentence)):\n",
    "            context = sentence[:i] + sentence[i+1:]\n",
    "            target = sentence[i]\n",
    "\n",
    "            context_idxs = torch.tensor([vocab[w] for w in context], dtype=torch.long)\n",
    "            target_idx = torch.tensor([vocab[target]], dtype=torch.long)\n",
    "\n",
    "            model.zero_grad()\n",
    "            log_probs = model(context_idxs.unsqueeze(0))\n",
    "\n",
    "            loss = F.nll_loss(log_probs, target_idx)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {total_loss}\")\n",
    "\n",
    "# Example usage\n",
    "model = CBOW()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "train(model, sentence, vocab, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74bba27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def build_vocab(self, corpus):\n",
    "        for word in corpus:\n",
    "            if word not in self.word2idx:\n",
    "                self.word2idx[word] = len(self.idx2word)\n",
    "                self.idx2word.append(word)\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.emb = torch.nn.Embedding(vocab_size, 9)\n",
    "        self.linear = torch.nn.Linear(9, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embs = self.emb(inputs)\n",
    "        embs = embs.mean(dim=1)\n",
    "        out = self.linear(embs)\n",
    "        probs = F.log_softmax(out, dim=1)\n",
    "        return probs\n",
    "\n",
    "def preprocess_corpus(file_path, window_size=2):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        words = f.read().strip().split()[:200000]  # Split the text into words\n",
    "        \n",
    "\n",
    "    data = []\n",
    "    for i in range(window_size, len(words) - window_size):\n",
    "        context = words[i - window_size:i] + words[i + 1:i + window_size + 1]\n",
    "        target = words[i]\n",
    "        data.append((context, target))\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6d24bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 19062\n",
      "Epoch: 1, Loss: 1810987.8712714612\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m CBOW(vocab_size)\n\u001b[0;32m     32\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m train(model, data, vocab, optimizer, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data, vocab, optimizer, epochs)\u001b[0m\n\u001b[0;32m      7\u001b[0m target_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([vocab\u001b[38;5;241m.\u001b[39mword2idx[target]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 10\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m model(context_idxs)\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(log_probs, target_idx)\n\u001b[0;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\ollie\\anaconda3\\envs\\Institue\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ollie\\anaconda3\\envs\\Institue\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m, in \u001b[0;36mCBOW.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m---> 22\u001b[0m     embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb(inputs)\n\u001b[0;32m     23\u001b[0m     embs \u001b[38;5;241m=\u001b[39m embs\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(embs)\n",
      "File \u001b[1;32mc:\\Users\\ollie\\anaconda3\\envs\\Institue\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ollie\\anaconda3\\envs\\Institue\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ollie\\anaconda3\\envs\\Institue\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx,\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq,\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[0;32m    198\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ollie\\anaconda3\\envs\\Institue\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, data, vocab, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for context, target in data:\n",
    "            context_idxs = torch.tensor([vocab.word2idx[w] for w in context], dtype=torch.long).unsqueeze(0)\n",
    "            target_idx = torch.tensor([vocab.word2idx[target]], dtype=torch.long)\n",
    "\n",
    "            model.zero_grad()\n",
    "            log_probs = model(context_idxs)\n",
    "\n",
    "            loss = F.nll_loss(log_probs, target_idx)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {total_loss}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"text8.txt\"\n",
    "window_size = 2\n",
    "data = preprocess_corpus(file_path, window_size)\n",
    "\n",
    "# Build vocabulary from the corpus\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab([word for context, target in data for word in context + [target]])\n",
    "print(f\"Vocabulary size: {len(vocab.word2idx)}\")\n",
    "\n",
    "vocab_size = len(vocab.word2idx)\n",
    "model = CBOW(vocab_size)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "train(model, data, vocab, optimizer, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9699979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def build_vocab(self, corpus):\n",
    "        for word in corpus:\n",
    "            if word not in self.word2idx:\n",
    "                self.word2idx[word] = len(self.idx2word)\n",
    "                self.idx2word.append(word)\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.emb = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embs = self.emb(inputs)\n",
    "        embs = embs.mean(dim=1)\n",
    "        out = self.linear(embs)\n",
    "        probs = F.log_softmax(out, dim=1)\n",
    "        return probs\n",
    "\n",
    "def preprocess_corpus(file_path, window_size=2):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        words = f.read().strip().split()[:1000000]  # Split the text into words\n",
    "        \n",
    "\n",
    "    data = []\n",
    "    for i in range(window_size, len(words) - window_size):\n",
    "        context = words[i - window_size:i] + words[i + 1:i + window_size + 1]\n",
    "        target = words[i]\n",
    "        data.append((context, target))\n",
    "    return data\n",
    "\n",
    "def create_batches(data, vocab, batch_size=32):\n",
    "    contexts, targets = [], []\n",
    "    for context, target in data:\n",
    "        context_idxs = [vocab.word2idx[w] for w in context]\n",
    "        target_idx = vocab.word2idx[target]\n",
    "        contexts.append(context_idxs)\n",
    "        targets.append(target_idx)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    contexts = torch.tensor(contexts, dtype=torch.long)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    # Create batches\n",
    "    num_batches = len(contexts) // batch_size\n",
    "    batches = []\n",
    "    for i in range(num_batches):\n",
    "        batch_contexts = contexts[i*batch_size:(i+1)*batch_size]\n",
    "        batch_targets = targets[i*batch_size:(i+1)*batch_size]\n",
    "        batches.append((batch_contexts, batch_targets))\n",
    "\n",
    "    # Handle the last batch if it's smaller than batch_size\n",
    "    if len(contexts) % batch_size != 0:\n",
    "        batch_contexts = contexts[num_batches*batch_size:]\n",
    "        batch_targets = targets[num_batches*batch_size:]\n",
    "        batches.append((batch_contexts, batch_targets))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff390588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 78382\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m CBOW(vocab_size, embedding_dim)\n\u001b[0;32m     29\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m train(model, data, vocab, optimizer, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data, vocab, optimizer, epochs, batch_size)\u001b[0m\n\u001b[0;32m      8\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m model(batch_contexts)\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(log_probs, batch_targets)\n\u001b[1;32m---> 10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     12\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\ollie\\anaconda3\\envs\\Institue\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ollie\\anaconda3\\envs\\Institue\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ollie\\anaconda3\\envs\\Institue\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, data, vocab, optimizer, epochs=10, batch_size=32):\n",
    "    model.train()\n",
    "    batches = create_batches(data, vocab, batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_contexts, batch_targets in batches:\n",
    "            model.zero_grad()\n",
    "            log_probs = model(batch_contexts)\n",
    "            loss = F.nll_loss(log_probs, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {total_loss}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"text8.txt\"\n",
    "window_size = 2\n",
    "data = preprocess_corpus(file_path, window_size)\n",
    "\n",
    "# Build vocabulary from the corpus\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab([word for context, target in data for word in context + [target]])\n",
    "print(f\"Vocabulary size: {len(vocab.word2idx)}\")\n",
    "\n",
    "vocab_size = len(vocab.word2idx)\n",
    "embedding_dim = 10  # Choose an appropriate embedding dimension\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "train(model, data, vocab, optimizer, epochs=10, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91ceffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words similar to 'finding':\n",
      "  this: 1.0000\n",
      "  is: 1.0000\n",
      "  a: 1.0000\n",
      "  text: 1.0000\n",
      "  corpus: 1.0000\n",
      "Top 5 words similar to 'sample':\n",
      "  this: 1.0000\n",
      "  is: 1.0000\n",
      "  a: 1.0000\n",
      "  text: 1.0000\n",
      "  corpus: 1.0000\n",
      "Top 5 words similar to 'most':\n",
      "  this: 1.0000\n",
      "  is: 1.0000\n",
      "  a: 1.0000\n",
      "  text: 1.0000\n",
      "  corpus: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the words to find similar words for\n",
    "target_words = [\"finding\", \"sample\", \"most\"]\n",
    "\n",
    "# Function to get word embeddings\n",
    "def get_embedding(word):\n",
    "    idx = vocab.get(word)\n",
    "    if idx is None:\n",
    "        print(f\"'{word}' is not in the vocabulary.\")\n",
    "        return None\n",
    "    return model.emb.weight[idx]\n",
    "\n",
    "# Function to find the most similar words\n",
    "def find_most_similar(word, top_n=5, excluded_words=None):\n",
    "    embedding = get_embedding(word)\n",
    "    if embedding is None:\n",
    "        return\n",
    "\n",
    "    cos_sim = nn.CosineSimilarity(dim=0)\n",
    "    similarities = {}\n",
    "    for other_word, other_idx in vocab.items():\n",
    "        if other_word != word:\n",
    "            other_embedding = model.emb.weight[other_idx]\n",
    "            similarity = cos_sim(embedding, other_embedding)\n",
    "            similarities[other_word] = similarity.item()\n",
    "\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    print(f\"Top {top_n} words similar to '{word}':\")\n",
    "    count = 0\n",
    "    for similar_word, similarity_score in sorted_similarities:\n",
    "        if excluded_words is None or similar_word not in excluded_words:\n",
    "            print(f\"  {similar_word}: {similarity_score:.4f}\")\n",
    "            count += 1\n",
    "            if count >= top_n:\n",
    "                break\n",
    "\n",
    "# Example usage\n",
    "for word in target_words:\n",
    "    find_most_similar(word, top_n=5, excluded_words=target_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8624e6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random words from vocabulary:\n",
      "finding\n",
      "sample\n",
      "most\n",
      "find\n",
      "their\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# The vocab variable is assumed to be defined in previous cells.\n",
    "# Ensure vocab is a dictionary with words as keys.\n",
    "# vocab = {\"word1\": index1, \"word2\": index2, ...}\n",
    "\n",
    "def print_random_words(vocab, num_words=10):\n",
    "    \"\"\"Prints a specified number of random words from the vocabulary.\"\"\"\n",
    "    words = list(vocab.keys())  # Get a list of words from the vocab dictionary\n",
    "    if num_words > len(words):\n",
    "        num_words = len(words)  # Adjust if requesting more words than available\n",
    "    random_words = random.sample(words, num_words)  # Select random words\n",
    "    print(\"Random words from vocabulary:\")\n",
    "    for word in random_words:\n",
    "        print(word)\n",
    "\n",
    "# Example usage: Print 10 random words from the vocabulary\n",
    "print_random_words(vocab, num_words=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Institue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
