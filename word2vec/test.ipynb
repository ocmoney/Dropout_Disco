{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "357cb9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# CBOW Model\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        # Average the embeddings of the context words\n",
    "        embedded = self.embeddings(context).mean(dim=1)\n",
    "        output = self.linear(embedded)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2c0997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, data, vocab):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        context_indices = torch.tensor([self.vocab.word2idx[word] for word in context], dtype=torch.long)\n",
    "        target_index = torch.tensor(self.vocab.word2idx[target], dtype=torch.long)\n",
    "        return context_indices, target_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba1b07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cbow(model, dataloader, criterion, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for context, target in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(context)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a11177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data: [(['anarchism', 'originated', 'as', 'term', 'of', 'abuse'], 'a'), (['originated', 'as', 'a', 'of', 'abuse', 'first'], 'term'), (['as', 'a', 'term', 'abuse', 'first', 'used'], 'of'), (['a', 'term', 'of', 'first', 'used', 'against'], 'abuse'), (['term', 'of', 'abuse', 'used', 'against', 'early'], 'first')]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_corpus(file_path, window_size=2):\n",
    "    \"\"\"\n",
    "    Preprocess the text corpus to generate context-target pairs for CBOW.\n",
    "    Args:\n",
    "        file_path (str): Path to the text file.\n",
    "        window_size (int): Number of context words on each side of the target word.\n",
    "    Returns:\n",
    "        list: A list of (context, target) pairs.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        words = f.read().strip().split()[:200000]  # Split the text into words\n",
    "        \n",
    "\n",
    "    data = []\n",
    "    for i in range(window_size, len(words) - window_size):\n",
    "        context = words[i - window_size:i] + words[i + 1:i + window_size + 1]\n",
    "        target = words[i]\n",
    "        data.append((context, target))\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "file_path = \"text8.txt\"\n",
    "window_size = 3\n",
    "data = preprocess_corpus(file_path, window_size)\n",
    "print(f\"Sample data: {data[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22214d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 19062\n"
     ]
    }
   ],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def build_vocab(self, corpus):\n",
    "        for word in corpus:\n",
    "            if word not in self.word2idx:\n",
    "                self.word2idx[word] = len(self.idx2word)\n",
    "                self.idx2word.append(word)\n",
    "\n",
    "# Build vocabulary from the corpus\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab([word for context, target in data for word in context + [target]])\n",
    "print(f\"Vocabulary size: {len(vocab.word2idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ca3e5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample vocabulary: [('anarchism', 0), ('originated', 1), ('as', 2), ('term', 3), ('of', 4), ('abuse', 5), ('a', 6), ('first', 7), ('used', 8), ('against', 9), ('early', 10), ('working', 11), ('class', 12), ('radicals', 13), ('including', 14), ('the', 15), ('diggers', 16), ('english', 17), ('revolution', 18), ('and', 19)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample vocabulary: {list(vocab.word2idx.items())[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad53f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = CBOWDataset(data, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67628c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 45070.7111\n",
      "Epoch 2, Loss: 31505.8954\n",
      "Epoch 3, Loss: 22748.1758\n",
      "Epoch 4, Loss: 17917.5155\n",
      "Epoch 5, Loss: 15470.3418\n",
      "Epoch 6, Loss: 13952.4802\n",
      "Epoch 7, Loss: 12993.0047\n",
      "Epoch 8, Loss: 12226.2891\n",
      "Epoch 9, Loss: 11671.6757\n",
      "Epoch 10, Loss: 11209.1986\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 128\n",
    "epochs = 10\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CBOW(len(vocab.word2idx), embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "train_cbow(model, dataloader, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa778ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'run':\n",
      "vermeer: 0.3750\n",
      "correspond: 0.3543\n",
      "sake: 0.3511\n",
      "wounds: 0.3493\n",
      "genuine: 0.3446\n",
      "theaters: 0.3357\n",
      "diminish: 0.3269\n",
      "decree: 0.3172\n",
      "barnard: 0.3123\n",
      "transected: 0.3117\n",
      "wealth: 0.3079\n",
      "antipathy: 0.3073\n",
      "barbed: 0.3065\n",
      "peers: 0.3061\n",
      "theoretical: 0.3024\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(word):\n",
    "    idx = vocab.word2idx.get(word, vocab.word2idx.get(\"<UNK>\"))  # Handle unknown words\n",
    "    return model.embeddings.weight[idx]\n",
    "\n",
    "def find_most_similar(word, top_n=5):\n",
    "    \"\"\"\n",
    "    Find the top_n most similar words to the input word based on cosine similarity.\n",
    "    Args:\n",
    "        word (str): The input word.\n",
    "        top_n (int): Number of most similar words to return.\n",
    "    \"\"\"\n",
    "    input_embedding = get_embedding(word)\n",
    "    all_embeddings = model.embeddings.weight\n",
    "    cos = nn.CosineSimilarity(dim=1)\n",
    "    similarities = cos(all_embeddings, input_embedding.unsqueeze(0))  # Compute similarities\n",
    "    top_indices = similarities.argsort(descending=True)[:top_n + 1]  # +1 to exclude the word itself\n",
    "\n",
    "    print(f\"Most similar words to '{word}':\")\n",
    "    for idx in top_indices:\n",
    "        similar_word = vocab.idx2word[idx.item()]\n",
    "        if similar_word != word:  # Exclude the word itself\n",
    "            print(f\"{similar_word}: {similarities[idx].item():.4f}\")\n",
    "\n",
    "# Example usage\n",
    "find_most_similar(\"run\", top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3da72db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between king and anarchy: 0.0081\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(word):\n",
    "    idx = vocab.word2idx[word]\n",
    "    return model.embeddings.weight[idx]\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=0)\n",
    "similarity = cos(get_embedding(\"chaos\"), get_embedding(\"anarchy\"))\n",
    "print(f\"Similarity between king and anarchy: {similarity.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
