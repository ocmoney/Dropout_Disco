{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "254f8697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dead</th>\n",
       "      <th>type</th>\n",
       "      <th>by</th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "      <th>parent</th>\n",
       "      <th>kids</th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>descendants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34202102</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>viewtransform</td>\n",
       "      <td>2023-01-01 00:07:29</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[34202608]</td>\n",
       "      <td>https://www.youtube.com/watch?v=Sz1n0RHwLqA</td>\n",
       "      <td>5</td>\n",
       "      <td>The physics of entropy and the origin of life ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34202107</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>TheBrokenRail</td>\n",
       "      <td>2023-01-01 00:08:15</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[34203377, 34205953, 34206965, 34204095, 34203...</td>\n",
       "      <td>https://thebrokenrail.com/2022/12/31/xfinity-s...</td>\n",
       "      <td>144</td>\n",
       "      <td>Xfinity Stream on Linux: A Tale of Widevine, C...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34202114</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>forte124</td>\n",
       "      <td>2023-01-01 00:09:17</td>\n",
       "      <td>What types of businesses most likely fall in t...</td>\n",
       "      <td>None</td>\n",
       "      <td>[34203714]</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>Ask HN: Examples of successful, small companie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34202138</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>todsacerdoti</td>\n",
       "      <td>2023-01-01 00:12:29</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.youtube.com/watch?v=q2A-MkGjvmI</td>\n",
       "      <td>4</td>\n",
       "      <td>Let’s try ChatGPT. Is it any good? (Bisqwit)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34202154</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>lisper</td>\n",
       "      <td>2023-01-01 00:14:17</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.amazon.com/Because-Internet-Unders...</td>\n",
       "      <td>2</td>\n",
       "      <td>Because Internet: Understanding the New Rules ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549327</th>\n",
       "      <td>40826340</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>bino47</td>\n",
       "      <td>2024-06-29 00:13:29</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[40826341]</td>\n",
       "      <td>https://www.langui.io</td>\n",
       "      <td>1</td>\n",
       "      <td>New AI Language Learning App Looking for Beta ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549328</th>\n",
       "      <td>40826360</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>thunderbong</td>\n",
       "      <td>2024-06-29 00:16:22</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://blog.brownplt.org/2024/06/27/different...</td>\n",
       "      <td>2</td>\n",
       "      <td>Differential Analysis: A Summary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549329</th>\n",
       "      <td>40826370</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>thunderbong</td>\n",
       "      <td>2024-06-29 00:18:24</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[40826580]</td>\n",
       "      <td>https://www.relational-algebra.dev/ra-primer/i...</td>\n",
       "      <td>13</td>\n",
       "      <td>Relational Algebra Primer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549330</th>\n",
       "      <td>40826421</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>zuhayeer</td>\n",
       "      <td>2024-06-29 00:26:26</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[40827602, 40837042, 40827522, 40828446, 40827...</td>\n",
       "      <td>https://www.msn.com/en-us/money/companies/empl...</td>\n",
       "      <td>22</td>\n",
       "      <td>Nvidia Employees Are Now Multi-Millionaires in...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549331</th>\n",
       "      <td>40826456</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>Hixon10</td>\n",
       "      <td>2024-06-29 00:31:06</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://opensource.googleblog.com/2024/06/comm...</td>\n",
       "      <td>1</td>\n",
       "      <td>CEL – Common Expressions for Portable Policy a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549332 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  dead   type             by                time  \\\n",
       "0       34202102  None  story  viewtransform 2023-01-01 00:07:29   \n",
       "1       34202107  None  story  TheBrokenRail 2023-01-01 00:08:15   \n",
       "2       34202114  None  story       forte124 2023-01-01 00:09:17   \n",
       "3       34202138  None  story   todsacerdoti 2023-01-01 00:12:29   \n",
       "4       34202154  None  story         lisper 2023-01-01 00:14:17   \n",
       "...          ...   ...    ...            ...                 ...   \n",
       "549327  40826340  None  story         bino47 2024-06-29 00:13:29   \n",
       "549328  40826360  None  story    thunderbong 2024-06-29 00:16:22   \n",
       "549329  40826370  None  story    thunderbong 2024-06-29 00:18:24   \n",
       "549330  40826421  None  story       zuhayeer 2024-06-29 00:26:26   \n",
       "549331  40826456  None  story        Hixon10 2024-06-29 00:31:06   \n",
       "\n",
       "                                                     text parent  \\\n",
       "0                                                    None   None   \n",
       "1                                                    None   None   \n",
       "2       What types of businesses most likely fall in t...   None   \n",
       "3                                                    None   None   \n",
       "4                                                    None   None   \n",
       "...                                                   ...    ...   \n",
       "549327                                               None   None   \n",
       "549328                                               None   None   \n",
       "549329                                               None   None   \n",
       "549330                                               None   None   \n",
       "549331                                               None   None   \n",
       "\n",
       "                                                     kids  \\\n",
       "0                                              [34202608]   \n",
       "1       [34203377, 34205953, 34206965, 34204095, 34203...   \n",
       "2                                              [34203714]   \n",
       "3                                                    None   \n",
       "4                                                    None   \n",
       "...                                                   ...   \n",
       "549327                                         [40826341]   \n",
       "549328                                               None   \n",
       "549329                                         [40826580]   \n",
       "549330  [40827602, 40837042, 40827522, 40828446, 40827...   \n",
       "549331                                               None   \n",
       "\n",
       "                                                      url  score  \\\n",
       "0             https://www.youtube.com/watch?v=Sz1n0RHwLqA      5   \n",
       "1       https://thebrokenrail.com/2022/12/31/xfinity-s...    144   \n",
       "2                                                    None      4   \n",
       "3             https://www.youtube.com/watch?v=q2A-MkGjvmI      4   \n",
       "4       https://www.amazon.com/Because-Internet-Unders...      2   \n",
       "...                                                   ...    ...   \n",
       "549327                              https://www.langui.io      1   \n",
       "549328  https://blog.brownplt.org/2024/06/27/different...      2   \n",
       "549329  https://www.relational-algebra.dev/ra-primer/i...     13   \n",
       "549330  https://www.msn.com/en-us/money/companies/empl...     22   \n",
       "549331  https://opensource.googleblog.com/2024/06/comm...      1   \n",
       "\n",
       "                                                    title  descendants  \n",
       "0       The physics of entropy and the origin of life ...            1  \n",
       "1       Xfinity Stream on Linux: A Tale of Widevine, C...           71  \n",
       "2       Ask HN: Examples of successful, small companie...            1  \n",
       "3            Let’s try ChatGPT. Is it any good? (Bisqwit)            0  \n",
       "4       Because Internet: Understanding the New Rules ...            0  \n",
       "...                                                   ...          ...  \n",
       "549327  New AI Language Learning App Looking for Beta ...            0  \n",
       "549328                   Differential Analysis: A Summary            0  \n",
       "549329                          Relational Algebra Primer            1  \n",
       "549330  Nvidia Employees Are Now Multi-Millionaires in...           11  \n",
       "549331  CEL – Common Expressions for Portable Policy a...            0  \n",
       "\n",
       "[549332 rows x 12 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the database URI directly\n",
    "# !! In real projects, manage credentials securely (e.g., env variables, secrets manager) !!\n",
    "DB_URI = \"postgresql://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\"\n",
    "\n",
    "engine = create_engine(DB_URI)\n",
    "# --- Optional: Set up logging ---\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# Example: Show tables (PostgreSQL metadata)\n",
    "res = pd.read_sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM \"hacker_news\".\"items\" a\n",
    "    WHERE a.type = 'story'\n",
    "        AND a.time >= '2023-01-01 00:00:00'\n",
    "        AND a.dead IS NOT TRUE\n",
    "        AND LENGTH(a.title) > 0\n",
    "        --LIMIT 10\n",
    "\"\"\", engine)\n",
    "\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a76ff6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The physics of entropy and the origin of life ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xfinity Stream on Linux: A Tale of Widevine, C...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ask HN: Examples of successful, small companie...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let’s try ChatGPT. Is it any good? (Bisqwit)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Because Internet: Understanding the New Rules ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Solar thermal storage using lunar regolith</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The craft of SwiftUI API design: Progressive d...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Worst interview questions for software developers</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Running Advent of Code on a $2 microcontroller</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OpenBSD KDE Status Report 2022</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Australia mandates Covid tests for Chinese tou...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Position with the most possible checkmates in 1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>As a Boy in Rural Mexico, His Life Was Changed...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Alt-Right Manipulated My Comic. Then AI Cl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SWA meltdown: technical debt from short term f...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Defining “Enough”</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A Particle That May Fill ‘Empty’ Space</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AMD Continues Working Toward HDR Display Suppo...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Serverless data pipelines for Data Engineers, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Four-year-old Poppy becomes trapped in skill t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>My Thoughts about Editors in 2022</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Best major for good money while also having a ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Competitive programming in Haskell: better bin...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The Computer Made Me Do It – ChatGPT Writes an...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Alan MacMasters: How the great online toaster ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Matrix Community Year in Review 2022</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The Third Magic</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Working on Composite Video Output for the MEGA...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Greatest innovations of 2022: The 35th annual ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The Lisa Computer: A Retrospective [pdf]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Ray Tracer Construction Kit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Ask HN: Any advice on becoming more organized?</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Tipp10 – Free Touch Typing Tutor</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Investing for a World Transformed by AI</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Typesafe DB Like Prisma for Rust</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Meta set to make divisive decision on Trump’s ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MilkyTracker: Open-source, multi-platform appl...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>As Big Tech retrenches, a tech talent shift ac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>The Dark Risk of Large Language Models</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Twitter Search Cheat Sheet (2018)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Show HN: I created my AI clone based on 600.00...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Show HN: Quick Steps in Lambdacalc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>When Place of Birth Is at Sea</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Ask HN: What does 'focus' mean to you?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>How to Go Fast</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>The Eureka Theory of History Is Wrong</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Ultimate guide to creating disks for retro com...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Wikimedia's Abstract Wikipedia project “at sub...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>How and why to properly write copyright statem...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>PSA: It's 2023, update the 'Copyright 2022' in...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  score\n",
       "0   The physics of entropy and the origin of life ...      5\n",
       "1   Xfinity Stream on Linux: A Tale of Widevine, C...    144\n",
       "2   Ask HN: Examples of successful, small companie...      4\n",
       "3        Let’s try ChatGPT. Is it any good? (Bisqwit)      4\n",
       "4   Because Internet: Understanding the New Rules ...      2\n",
       "5          Solar thermal storage using lunar regolith      2\n",
       "6   The craft of SwiftUI API design: Progressive d...      4\n",
       "7   Worst interview questions for software developers    154\n",
       "8      Running Advent of Code on a $2 microcontroller     90\n",
       "9                      OpenBSD KDE Status Report 2022      5\n",
       "10  Australia mandates Covid tests for Chinese tou...      6\n",
       "11    Position with the most possible checkmates in 1      3\n",
       "12  As a Boy in Rural Mexico, His Life Was Changed...      3\n",
       "13  The Alt-Right Manipulated My Comic. Then AI Cl...      4\n",
       "14  SWA meltdown: technical debt from short term f...     10\n",
       "15                                  Defining “Enough”      3\n",
       "16             A Particle That May Fill ‘Empty’ Space      5\n",
       "17  AMD Continues Working Toward HDR Display Suppo...     13\n",
       "18  Serverless data pipelines for Data Engineers, ...      3\n",
       "19  Four-year-old Poppy becomes trapped in skill t...      2\n",
       "20                  My Thoughts about Editors in 2022      5\n",
       "21  Best major for good money while also having a ...      5\n",
       "22  Competitive programming in Haskell: better bin...    131\n",
       "23  The Computer Made Me Do It – ChatGPT Writes an...      2\n",
       "24  Alan MacMasters: How the great online toaster ...     11\n",
       "25               Matrix Community Year in Review 2022     86\n",
       "26                                    The Third Magic      4\n",
       "27  Working on Composite Video Output for the MEGA...      6\n",
       "28  Greatest innovations of 2022: The 35th annual ...      4\n",
       "29           The Lisa Computer: A Retrospective [pdf]      3\n",
       "30                        Ray Tracer Construction Kit      2\n",
       "31     Ask HN: Any advice on becoming more organized?     15\n",
       "32                   Tipp10 – Free Touch Typing Tutor      2\n",
       "33            Investing for a World Transformed by AI      1\n",
       "34                   Typesafe DB Like Prisma for Rust      4\n",
       "35  Meta set to make divisive decision on Trump’s ...      4\n",
       "36  MilkyTracker: Open-source, multi-platform appl...      2\n",
       "37  As Big Tech retrenches, a tech talent shift ac...      2\n",
       "38             The Dark Risk of Large Language Models      2\n",
       "39                  Twitter Search Cheat Sheet (2018)      1\n",
       "40  Show HN: I created my AI clone based on 600.00...      3\n",
       "41                 Show HN: Quick Steps in Lambdacalc      2\n",
       "42                      When Place of Birth Is at Sea      4\n",
       "43             Ask HN: What does 'focus' mean to you?      1\n",
       "44                                     How to Go Fast      2\n",
       "45              The Eureka Theory of History Is Wrong     51\n",
       "46  Ultimate guide to creating disks for retro com...      1\n",
       "47  Wikimedia's Abstract Wikipedia project “at sub...      4\n",
       "48  How and why to properly write copyright statem...     14\n",
       "49  PSA: It's 2023, update the 'Copyright 2022' in...     19"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_and_scores = res.loc[:, ['title', 'score']].copy()\n",
    "titles_and_scores.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fee70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from utils import logger  # Import the `logger` module from `utils`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "edafee0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word2vec.vocabulary.Vocabulary"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f553a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from word2vec.vocabulary import Vocabulary as vocab\n",
    "class TextToRegressionModel(nn.Module):\n",
    "    def __init__(self, vocab_path, cbow_model_path, input_dim, hidden_dims=[128, 64, 32], dropout=0.2):\n",
    "        \"\"\"\n",
    "        Combines vocabulary, CBOW embeddings, and MLP regression model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_path (str): Path to the saved vocabulary JSON.\n",
    "            cbow_model_path (str): Path to the saved CBOW model state.\n",
    "            input_dim (int): Dimension of the input embeddings.\n",
    "            hidden_dims (List[int]): List of hidden layer dimensions.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Load vocabulary\n",
    "        self.vocab = vocab.load_vocab(vocab_path)\n",
    "        \n",
    "        # Load CBOW model and extract embedding layer\n",
    "        cbow_state = torch.load(cbow_model_path, map_location=torch.device('cpu'))\n",
    "        self.embedding = nn.Embedding.from_pretrained(cbow_state['embeddings.weight'])\n",
    "        \n",
    "        # Initialize MLP layers\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Add final output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        # Combine all layers\n",
    "        self.regression_model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is already embedded and averaged from the collate function\n",
    "        return self.regression_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff21790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, targets, vocab):\n",
    "        \"\"\"\n",
    "        Custom Dataset for text regression.\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): List of input texts.\n",
    "            targets (List[float]): List of target regression values.\n",
    "            vocab (Vocabulary): Vocabulary object for tokenization.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        target = self.targets[idx]\n",
    "        # Convert text to lowercase and split into tokens\n",
    "        tokens = text.lower().split()\n",
    "        # Get indices for each token, handling unknown words\n",
    "        indices = [self.vocab.get_index(token) for token in tokens]\n",
    "        return torch.tensor(indices, dtype=torch.long), target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "785161a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_collate_fn(model, device):\n",
    "    def collate_fn(batch):\n",
    "        # Separate the sequences and targets\n",
    "        sequences, targets = zip(*batch)\n",
    "        \n",
    "        # Convert targets to tensor and move to the correct device\n",
    "        targets = torch.stack(targets).to(device)\n",
    "        \n",
    "        # Process each sequence through the model's embedding layer\n",
    "        embedded_sequences = []\n",
    "        for seq in sequences:\n",
    "            # Get embeddings for the sequence\n",
    "            embeddings = model.embedding(seq.to(device))  # Move seq to the correct device\n",
    "            # Average the embeddings\n",
    "            avg_embedding = embeddings.mean(dim=0)\n",
    "            embedded_sequences.append(avg_embedding)\n",
    "        \n",
    "        # Stack the averaged embeddings\n",
    "        embedded_batch = torch.stack(embedded_sequences).to(device)\n",
    "        \n",
    "        return embedded_batch, targets\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2bc353d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf2b9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming titles_and_scores is your DataFrame\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(titles_and_scores, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['title'].tolist(),\n",
    "    targets=train_df['score'].tolist(),\n",
    "    vocab=vocab  # Your existing vocabulary object\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    texts=test_df['title'].tolist(),\n",
    "    targets=test_df['score'].tolist(),\n",
    "    vocab=vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "332db596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.TextDataset object at 0x762d80aa87f0>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d614ab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "2025-04-16 14:49:06 | DropoutDisco | INFO     | [vocabulary.py:110] | Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-16 14:49:06 | DropoutDisco | INFO     | [vocabulary.py:123] | 📚 Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:📚 Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 18.7210\n",
      "Epoch 2/10, Loss: 18.6745\n",
      "Epoch 3/10, Loss: 18.6659\n",
      "Epoch 4/10, Loss: 18.6647\n",
      "Epoch 5/10, Loss: 18.6631\n",
      "Epoch 6/10, Loss: 18.6559\n"
     ]
    }
   ],
   "source": [
    "# Detect the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = TextToRegressionModel(\n",
    "    vocab_path=\"../models/word2vec/text8_vocab_NWAll_MF5.json\",  # Replace with your actual path\n",
    "    cbow_model_path=\"../models/word2vec/CBOW_D128_W5_NWAll_MF5_E15_LR0.001_BS512/model_state.pth\",  # Replace with your actual path\n",
    "    input_dim=128,  # Match your CBOW embedding dimension\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['title'].tolist(),\n",
    "    targets=train_df['score'].tolist(),\n",
    "    vocab=model.vocab  # Use the model's vocabulary\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    texts=test_df['title'].tolist(),\n",
    "    targets=test_df['score'].tolist(),\n",
    "    vocab=model.vocab\n",
    ")\n",
    "\n",
    "# Create dataloaders with the custom collate function\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=make_collate_fn(model, device)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=make_collate_fn(model, device)\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf1567",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_loss += criterion(outputs.squeeze(), targets).item()\n",
    "        \n",
    "        predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "        actuals.extend(targets.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f'Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "# Calculate additional metrics\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(actuals, predictions)\n",
    "\n",
    "print(f'MSE: {mse:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'R2 Score: {r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837eb34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_score(model, text, device):\n",
    "    \"\"\"\n",
    "    Predict score for a single text input.\n",
    "    \n",
    "    Args:\n",
    "        model (TextToRegressionModel): Trained model\n",
    "        text (str): Input text to predict score for\n",
    "        device (str): Device to run prediction on\n",
    "        \n",
    "    Returns:\n",
    "        float: Predicted score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Preprocess the text\n",
    "        tokens = text.lower().split()\n",
    "        indices = [model.vocab.get_index(token) for token in tokens]\n",
    "        token_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get embeddings and average\n",
    "        embeddings = model.embedding(token_tensor)\n",
    "        avg_embedding = embeddings.mean(dim=1)\n",
    "        \n",
    "        # Get prediction\n",
    "        prediction = model.regression_model(avg_embedding)\n",
    "        return prediction.item()\n",
    "    \n",
    "predict_score(model, \"technology\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fbd084",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head(20)\n",
    "\n",
    "# find the entry in res df including the title \"the best way to learn...\"\n",
    "# Search for the title in the res DataFrame\n",
    "#matching_entries = res[res['title'].str.contains(\"\", case=False, na=False)]\n",
    "#print(matching_entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06cd734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c37ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7707bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
