{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254f8697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:07:29</td>\n",
       "      <td>The physics of entropy and the origin of life ...</td>\n",
       "      <td>https://www.youtube.com/watch?v=Sz1n0RHwLqA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 00:08:15</td>\n",
       "      <td>Xfinity Stream on Linux: A Tale of Widevine, C...</td>\n",
       "      <td>https://thebrokenrail.com/2022/12/31/xfinity-s...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 00:09:17</td>\n",
       "      <td>Ask HN: Examples of successful, small companie...</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 00:12:29</td>\n",
       "      <td>Let‚Äôs try ChatGPT. Is it any good? (Bisqwit)</td>\n",
       "      <td>https://www.youtube.com/watch?v=q2A-MkGjvmI</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 00:14:17</td>\n",
       "      <td>Because Internet: Understanding the New Rules ...</td>\n",
       "      <td>https://www.amazon.com/Because-Internet-Unders...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549327</th>\n",
       "      <td>2024-06-22 05:31:18</td>\n",
       "      <td>Solving Maxwell's Equations with Non-Trainable...</td>\n",
       "      <td>https://arxiv.org/abs/2405.00814</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549328</th>\n",
       "      <td>2024-06-22 05:37:07</td>\n",
       "      <td>Khronos: glTF Interactivity Specification Rele...</td>\n",
       "      <td>https://www.khronos.org/blog/gltf-interactivit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549329</th>\n",
       "      <td>2024-06-22 05:39:21</td>\n",
       "      <td>Family whose roof was damaged by space debris ...</td>\n",
       "      <td>https://arstechnica.com/space/2024/06/family-w...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549330</th>\n",
       "      <td>2024-06-22 05:41:12</td>\n",
       "      <td>YouTube confirms crackdown on VPN users access...</td>\n",
       "      <td>https://techcrunch.com/2024/06/20/youtube-conf...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549331</th>\n",
       "      <td>2024-06-22 05:46:01</td>\n",
       "      <td>How tiny fossils may help us prepare for big e...</td>\n",
       "      <td>https://www.youtube.com/watch?v=BIK3jfFvm_8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549332 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      time                                              title  \\\n",
       "0      2023-01-01 00:07:29  The physics of entropy and the origin of life ...   \n",
       "1      2023-01-01 00:08:15  Xfinity Stream on Linux: A Tale of Widevine, C...   \n",
       "2      2023-01-01 00:09:17  Ask HN: Examples of successful, small companie...   \n",
       "3      2023-01-01 00:12:29       Let‚Äôs try ChatGPT. Is it any good? (Bisqwit)   \n",
       "4      2023-01-01 00:14:17  Because Internet: Understanding the New Rules ...   \n",
       "...                    ...                                                ...   \n",
       "549327 2024-06-22 05:31:18  Solving Maxwell's Equations with Non-Trainable...   \n",
       "549328 2024-06-22 05:37:07  Khronos: glTF Interactivity Specification Rele...   \n",
       "549329 2024-06-22 05:39:21  Family whose roof was damaged by space debris ...   \n",
       "549330 2024-06-22 05:41:12  YouTube confirms crackdown on VPN users access...   \n",
       "549331 2024-06-22 05:46:01  How tiny fossils may help us prepare for big e...   \n",
       "\n",
       "                                                      url  score  \n",
       "0             https://www.youtube.com/watch?v=Sz1n0RHwLqA      5  \n",
       "1       https://thebrokenrail.com/2022/12/31/xfinity-s...    144  \n",
       "2                                                    None      4  \n",
       "3             https://www.youtube.com/watch?v=q2A-MkGjvmI      4  \n",
       "4       https://www.amazon.com/Because-Internet-Unders...      2  \n",
       "...                                                   ...    ...  \n",
       "549327                   https://arxiv.org/abs/2405.00814      2  \n",
       "549328  https://www.khronos.org/blog/gltf-interactivit...      1  \n",
       "549329  https://arstechnica.com/space/2024/06/family-w...      6  \n",
       "549330  https://techcrunch.com/2024/06/20/youtube-conf...      2  \n",
       "549331        https://www.youtube.com/watch?v=BIK3jfFvm_8      1  \n",
       "\n",
       "[549332 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the database URI directly\n",
    "# !! In real projects, manage credentials securely (e.g., env variables, secrets manager) !!\n",
    "DB_URI = \"postgresql://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\"\n",
    "\n",
    "engine = create_engine(DB_URI)\n",
    "# --- Optional: Set up logging ---\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# Example: Show tables (PostgreSQL metadata)\n",
    "res = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        time\n",
    "        , title\n",
    "        , url\n",
    "        , score  \n",
    "    FROM \"hacker_news\".\"items\" a\n",
    "    WHERE a.type = 'story'\n",
    "        AND a.time >= '2023-01-01 00:00:00'\n",
    "        AND a.dead IS NOT TRUE\n",
    "        AND LENGTH(a.title) > 0\n",
    "\"\"\", engine)\n",
    "\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a76ff6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The physics of entropy and the origin of life ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xfinity Stream on Linux: A Tale of Widevine, C...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ask HN: Examples of successful, small companie...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let‚Äôs try ChatGPT. Is it any good? (Bisqwit)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Because Internet: Understanding the New Rules ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Solar thermal storage using lunar regolith</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The craft of SwiftUI API design: Progressive d...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Worst interview questions for software developers</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Running Advent of Code on a $2 microcontroller</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OpenBSD KDE Status Report 2022</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Australia mandates Covid tests for Chinese tou...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Position with the most possible checkmates in 1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>As a Boy in Rural Mexico, His Life Was Changed...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Alt-Right Manipulated My Comic. Then AI Cl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SWA meltdown: technical debt from short term f...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Defining ‚ÄúEnough‚Äù</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A Particle That May Fill ‚ÄòEmpty‚Äô Space</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AMD Continues Working Toward HDR Display Suppo...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Serverless data pipelines for Data Engineers, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Four-year-old Poppy becomes trapped in skill t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>My Thoughts about Editors in 2022</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Best major for good money while also having a ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Competitive programming in Haskell: better bin...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The Computer Made Me Do It ‚Äì ChatGPT Writes an...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Alan MacMasters: How the great online toaster ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Matrix Community Year in Review 2022</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The Third Magic</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Working on Composite Video Output for the MEGA...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Greatest innovations of 2022: The 35th annual ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The Lisa Computer: A Retrospective [pdf]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Ray Tracer Construction Kit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Ask HN: Any advice on becoming more organized?</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Tipp10 ‚Äì Free Touch Typing Tutor</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Investing for a World Transformed by AI</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Typesafe DB Like Prisma for Rust</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Meta set to make divisive decision on Trump‚Äôs ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MilkyTracker: Open-source, multi-platform appl...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>As Big Tech retrenches, a tech talent shift ac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>The Dark Risk of Large Language Models</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Twitter Search Cheat Sheet (2018)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Show HN: I created my AI clone based on 600.00...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Show HN: Quick Steps in Lambdacalc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>When Place of Birth Is at Sea</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Ask HN: What does 'focus' mean to you?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>How to Go Fast</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>The Eureka Theory of History Is Wrong</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Ultimate guide to creating disks for retro com...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Wikimedia's Abstract Wikipedia project ‚Äúat sub...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>How and why to properly write copyright statem...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>PSA: It's 2023, update the 'Copyright 2022' in...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  score\n",
       "0   The physics of entropy and the origin of life ...      5\n",
       "1   Xfinity Stream on Linux: A Tale of Widevine, C...    144\n",
       "2   Ask HN: Examples of successful, small companie...      4\n",
       "3        Let‚Äôs try ChatGPT. Is it any good? (Bisqwit)      4\n",
       "4   Because Internet: Understanding the New Rules ...      2\n",
       "5          Solar thermal storage using lunar regolith      2\n",
       "6   The craft of SwiftUI API design: Progressive d...      4\n",
       "7   Worst interview questions for software developers    154\n",
       "8      Running Advent of Code on a $2 microcontroller     90\n",
       "9                      OpenBSD KDE Status Report 2022      5\n",
       "10  Australia mandates Covid tests for Chinese tou...      6\n",
       "11    Position with the most possible checkmates in 1      3\n",
       "12  As a Boy in Rural Mexico, His Life Was Changed...      3\n",
       "13  The Alt-Right Manipulated My Comic. Then AI Cl...      4\n",
       "14  SWA meltdown: technical debt from short term f...     10\n",
       "15                                  Defining ‚ÄúEnough‚Äù      3\n",
       "16             A Particle That May Fill ‚ÄòEmpty‚Äô Space      5\n",
       "17  AMD Continues Working Toward HDR Display Suppo...     13\n",
       "18  Serverless data pipelines for Data Engineers, ...      3\n",
       "19  Four-year-old Poppy becomes trapped in skill t...      2\n",
       "20                  My Thoughts about Editors in 2022      5\n",
       "21  Best major for good money while also having a ...      5\n",
       "22  Competitive programming in Haskell: better bin...    131\n",
       "23  The Computer Made Me Do It ‚Äì ChatGPT Writes an...      2\n",
       "24  Alan MacMasters: How the great online toaster ...     11\n",
       "25               Matrix Community Year in Review 2022     86\n",
       "26                                    The Third Magic      4\n",
       "27  Working on Composite Video Output for the MEGA...      6\n",
       "28  Greatest innovations of 2022: The 35th annual ...      4\n",
       "29           The Lisa Computer: A Retrospective [pdf]      3\n",
       "30                        Ray Tracer Construction Kit      2\n",
       "31     Ask HN: Any advice on becoming more organized?     15\n",
       "32                   Tipp10 ‚Äì Free Touch Typing Tutor      2\n",
       "33            Investing for a World Transformed by AI      1\n",
       "34                   Typesafe DB Like Prisma for Rust      4\n",
       "35  Meta set to make divisive decision on Trump‚Äôs ...      4\n",
       "36  MilkyTracker: Open-source, multi-platform appl...      2\n",
       "37  As Big Tech retrenches, a tech talent shift ac...      2\n",
       "38             The Dark Risk of Large Language Models      2\n",
       "39                  Twitter Search Cheat Sheet (2018)      1\n",
       "40  Show HN: I created my AI clone based on 600.00...      3\n",
       "41                 Show HN: Quick Steps in Lambdacalc      2\n",
       "42                      When Place of Birth Is at Sea      4\n",
       "43             Ask HN: What does 'focus' mean to you?      1\n",
       "44                                     How to Go Fast      2\n",
       "45              The Eureka Theory of History Is Wrong     51\n",
       "46  Ultimate guide to creating disks for retro com...      1\n",
       "47  Wikimedia's Abstract Wikipedia project ‚Äúat sub...      4\n",
       "48  How and why to properly write copyright statem...     14\n",
       "49  PSA: It's 2023, update the 'Copyright 2022' in...     19"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_and_scores = res.loc[:, ['title', 'score']].copy()\n",
    "titles_and_scores.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fee70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from utils import logger  # Import the `logger` module from `utils`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f553a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from word2vec.vocabulary import Vocabulary as vocab\n",
    "\n",
    "class TextToRegressionModel(nn.Module):\n",
    "    def __init__(self, vocab_path, cbow_model_path, input_dim, hidden_dims=[128, 64, 32], dropout=0.2):\n",
    "        \"\"\"\n",
    "        Combines vocabulary, CBOW embeddings, and MLP regression model.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the saved vocabulary JSON.\n",
    "            cbow_model_path (str): Path to the saved CBOW model state.\n",
    "            input_dim (int): Dimension of the input embeddings.\n",
    "            hidden_dims (List[int]): List of hidden layer dimensions.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Load vocabulary\n",
    "        self.vocab = vocab.load_vocab(vocab_path)\n",
    "        \n",
    "        # Load CBOW model and extract embedding layer\n",
    "        cbow_state = torch.load(cbow_model_path, map_location=torch.device('cpu'))\n",
    "        self.embedding = nn.Embedding.from_pretrained(cbow_state['embeddings.weight'])\n",
    "        \n",
    "        # Initialize MLP layers\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Add final output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        # Combine all layers\n",
    "        self.regression_model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "        Args:\n",
    "            x (Tensor): Averaged embeddings of shape (batch_size, input_dim).\n",
    "        Returns:\n",
    "            Tensor: Predicted regression values.\n",
    "        \"\"\"\n",
    "        return self.regression_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff21790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, targets, vocab):\n",
    "        \"\"\"\n",
    "        Custom Dataset for text regression.\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): List of input texts.\n",
    "            targets (List[float]): List of target regression values.\n",
    "            vocab (Vocabulary): Vocabulary object for tokenization.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        import re\n",
    "        # Replace any non-alphanumeric character with a space\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "        # Convert to lowercase and split into tokens\n",
    "        tokens = text.lower().split()\n",
    "        return tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        target = self.targets[idx]\n",
    "        # Preprocess the text\n",
    "        tokens = self.preprocess_text(text)\n",
    "        # Convert tokens to indices using the vocabulary\n",
    "        indices = [self.vocab.get_index(token) for token in tokens]\n",
    "        return torch.tensor(indices, dtype=torch.long), target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "785161a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_collate_fn(model, device):\n",
    "    def collate_fn(batch):\n",
    "        sequences, targets = zip(*batch)\n",
    "        targets = torch.stack(targets).to(device)\n",
    "\n",
    "        embedded_sequences = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) == 0:  # Handle empty sequences\n",
    "                embedded_sequences.append(torch.zeros(model.embedding.embedding_dim).to(device))\n",
    "            else:\n",
    "                embeddings = model.embedding(seq.to(device))\n",
    "                avg_embedding = embeddings.mean(dim=0)\n",
    "                embedded_sequences.append(avg_embedding)\n",
    "\n",
    "        embedded_batch = torch.stack(embedded_sequences).to(device)\n",
    "        return embedded_batch, targets\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bc353d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(titles_and_scores, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d614ab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "2025-04-18 12:11:43 | DropoutDisco | INFO     | [vocabulary.py:110] | Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-18 12:11:43 | DropoutDisco | INFO     | [vocabulary.py:123] | üìö Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:üìö Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n",
      "/tmp/ipykernel_7147/277846822.py:18: RuntimeWarning: divide by zero encountered in log\n",
      "  train_df['score'] = train_df['score'].apply(lambda x: np.log(x))  # log1p ensures log(0) is handled\n",
      "/tmp/ipykernel_7147/277846822.py:19: RuntimeWarning: divide by zero encountered in log\n",
      "  test_df['score'] = test_df['score'].apply(lambda x: np.log(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: inf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Detect the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = TextToRegressionModel(\n",
    "    vocab_path=\"../models/word2vec/text8_vocab_NWAll_MF5.json\",  # Replace with your actual path\n",
    "    cbow_model_path=\"../models/word2vec/CBOW_D128_W5_NWAll_MF5_E15_LR0.001_BS512/model_state.pth\",  # Replace with your actual path\n",
    "    input_dim=128,  # Match your CBOW embedding dimension\n",
    "    hidden_dims=[96, 64, 32, 16],  # Example hidden layer dimensions\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Apply log scaling to the scores in the training and testing datasets\n",
    "train_df['score'] = train_df['score'].apply(lambda x: np.log(x))  # log1p ensures log(0) is handled\n",
    "test_df['score'] = test_df['score'].apply(lambda x: np.log(x))\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['title'].tolist(),\n",
    "    targets=train_df['score'].tolist(),\n",
    "    vocab=model.vocab  # Use the model's vocabulary\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    texts=test_df['title'].tolist(),\n",
    "    targets=test_df['score'].tolist(),\n",
    "    vocab=model.vocab\n",
    ")\n",
    "\n",
    "# Create dataloaders with the custom collate function\n",
    "batch_size = 2048\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=make_collate_fn(model, device)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=make_collate_fn(model, device)\n",
    ")\n",
    "\n",
    "\n",
    "# Simplified training loop using train_model function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "d1597ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'regression_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0daf1567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.9888\n",
      "Epoch 2/3, Loss: 0.9829\n",
      "Epoch 3/3, Loss: 0.9794\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0f1296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 5976.3472\n",
      "RMSE: 77.3068\n",
      "MAE: 19.1020\n",
      "R2 Score: -0.0568\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss for this batch\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        test_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "        # Reverse the log transformation\n",
    "        predictions.extend(np.exp(outputs.squeeze().cpu().numpy()))  # Convert predictions back to original scale\n",
    "        actuals.extend(np.exp(targets.cpu().numpy()))  # Convert targets back to original scale\n",
    "\n",
    "# Calculate errors in the original scale\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(actuals, predictions)\n",
    "\n",
    "print(f'MSE: {mse:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'R2 Score: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "837eb34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0299300119627595\n"
     ]
    }
   ],
   "source": [
    "def predict_score(model, text, device):\n",
    "    \"\"\"\n",
    "    Predict score for a single text input.\n",
    "    \n",
    "    Args:\n",
    "        model (TextToRegressionModel): Trained model\n",
    "        text (str): Input text to predict score for\n",
    "        device (str): Device to run prediction on\n",
    "        \n",
    "    Returns:\n",
    "        float: Predicted score\n",
    "    \"\"\"\n",
    "    # Use the preprocess_text function for text preprocessing\n",
    "    tokens = TextDataset.preprocess_text(text)\n",
    "    #tokens = text.lower().split() # old preprocessing that just splits and lowercases\n",
    "    #print(tokens)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Preprocess the text\n",
    "        indices = [model.vocab.get_index(token) for token in tokens]\n",
    "        #print(indices)\n",
    "        token_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get embeddings and average\n",
    "        embeddings = model.embedding(token_tensor)\n",
    "\n",
    "        avg_embedding = embeddings.mean(dim=1)\n",
    "        \n",
    "        # Get prediction\n",
    "        prediction = model(avg_embedding)\n",
    "        return np.exp(prediction.item()).item()\n",
    "\n",
    "\n",
    "print(predict_score(model, \"monarch\", device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc21731",
   "metadata": {},
   "source": [
    "# BELOW ARE EXPLORATORY ITEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e29baf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# find the 10 entries with the highest scores\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m top_10_entries \u001b[38;5;241m=\u001b[39m \u001b[43mtest_df\u001b[49m\u001b[38;5;241m.\u001b[39mnlargest(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Print the top 10 entries\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 10 entries with the highest scores:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "# find the 10 entries with the highest scores\n",
    "top_10_entries = test_df.nlargest(10, 'score')\n",
    "# Print the top 10 entries\n",
    "print(\"Top 10 entries with the highest scores:\")\n",
    "print(top_10_entries[['title', 'score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f48418cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words with the highest scores:\n",
      "besieges: 12.070055859253058\n",
      "fbi: 9.683543925330543\n",
      "humiliated: 9.279914603186548\n",
      "infiltrated: 7.236810527204565\n",
      "tugs: 7.150932142170766\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all words in the vocabulary and predict scores\n",
    "word_scores = []\n",
    "\n",
    "for word in model.vocab.idx2word:\n",
    "    score = predict_score(model, word, device)\n",
    "    word_scores.append((word, score))\n",
    "\n",
    "# Sort the words by their predicted scores in descending order\n",
    "word_scores = sorted(word_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the top 5 words with the highest scores\n",
    "top_5_words = word_scores[:5]\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 5 words with the highest scores:\")\n",
    "for word, score in top_5_words:\n",
    "    print(f\"{word}: {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9dbc4e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centuries populous kwh nebuchadrezzar zionists palestine cormen ungovernable majuscule gurion bethlehem amicable dioceses nehemiah hellenized babylonian gaza plotarea damasus nicea\n"
     ]
    }
   ],
   "source": [
    "words = ['centuries', 'populous', 'kwh', 'nebuchadrezzar', 'zionists', 'palestine', 'cormen', 'ungovernable', 'majuscule', 'gurion', 'bethlehem', 'amicable', 'dioceses', 'nehemiah', 'hellenized', 'babylonian', 'gaza', 'plotarea', 'damasus', 'nicea', 'waismann', 'sephardic', 'century', 'conclave', 'golan', 'antioch', 'teutonic', 'szil', 'elector', 'emesa', 'manasseh', 'phoenicians', 'burkert', 'germain', 'levant', 'anastasius', 'syriac', 'suffrage', 'hierarchs', 'suzerainty', 'geographer', 'codepoint', 'knesset', 'landtag', 'jamnia', 'hyrcanus', 'martyred', 'francia', 'berbers', 'koresh']\n",
    "\n",
    "n = 20\n",
    "print(\" \".join(words[:n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd990b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: Show HN: Buyidentities.com\n",
      "Tokens: ['show', 'hn', 'buyidentities', 'com']\n",
      "2025-04-17 10:38:09 | DropoutDisco | INFO     | [vocabulary.py:110] | Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-17 10:38:09 | DropoutDisco | INFO     | [vocabulary.py:123] | üìö Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:üìö Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token indices: [2194, 38072, 0, 2806]\n",
      "Embeddings: tensor([[ 1.9466e+00,  4.3013e-01, -2.0402e+00,  4.4432e-01, -2.1084e+00,\n",
      "          2.7667e-01,  9.9176e-01,  8.9653e-01,  2.2319e-01, -1.3121e+00,\n",
      "          4.9519e-02, -1.8727e-01,  7.8188e-01, -2.2465e+00, -1.2707e-01,\n",
      "         -3.1723e-03,  1.3502e+00,  1.2984e+00, -1.1449e-01, -1.8985e-01,\n",
      "         -9.1769e-01, -1.3087e-01,  2.9257e-01, -8.9975e-01, -5.0819e-01,\n",
      "          8.5919e-01, -3.4376e+00,  7.6520e-01,  1.3828e+00,  2.9100e-01,\n",
      "          1.2134e+00,  7.9240e-01,  4.1146e-01, -1.7667e+00, -2.8801e+00,\n",
      "          2.5893e-01, -1.6380e-01,  3.6928e-01, -5.3449e-01, -1.8259e-01,\n",
      "          2.1736e+00, -2.1261e+00,  1.3076e+00, -3.2017e+00,  3.3742e-01,\n",
      "         -1.2346e+00,  1.3087e+00,  2.1186e+00, -1.7250e+00,  4.6178e-01,\n",
      "         -5.8093e-01, -1.4234e-01,  1.6505e+00, -1.9864e-02,  1.4873e-01,\n",
      "         -6.8776e-01,  1.3533e+00, -8.1330e-01,  9.0072e-01,  7.5148e-01,\n",
      "          7.8481e-01, -8.7399e-01, -4.5775e-02, -1.1776e-01, -1.1477e+00,\n",
      "         -1.0841e+00, -2.2613e-01, -1.0361e+00, -2.7476e+00, -1.9721e-01,\n",
      "          3.0830e-01,  1.5420e+00,  5.3548e-01, -8.6643e-01, -1.3576e+00,\n",
      "         -3.4131e-01, -7.8010e-01,  2.7584e-01, -2.1082e+00,  1.4421e+00,\n",
      "         -3.5891e-01,  3.4663e-01, -2.2047e+00,  2.2207e-01,  1.0311e+00,\n",
      "          4.5479e-02, -1.8573e-01,  3.2395e+00,  2.2179e+00, -3.6438e-01,\n",
      "         -3.9641e+00, -1.1545e+00, -1.8063e+00, -5.1812e-01, -1.2691e+00,\n",
      "         -1.1358e+00, -1.8693e+00, -4.1736e-01, -1.6325e+00,  1.2383e+00,\n",
      "          1.8732e+00,  2.1580e+00,  2.0025e+00,  1.4012e+00,  5.4559e-01,\n",
      "          3.0559e+00, -6.0173e-01, -1.1741e-01,  7.7093e-01, -1.5293e+00,\n",
      "         -8.6734e-01, -3.1346e-01,  3.7775e-01,  2.7541e-02,  2.9125e+00,\n",
      "          1.9542e+00,  4.5796e-01,  1.0115e+00,  2.5596e+00, -3.1819e+00,\n",
      "          5.9765e-01,  2.4012e-01, -1.5479e+00, -9.3829e-01, -1.4166e+00,\n",
      "         -4.5177e-01, -5.5938e-01, -1.1085e+00],\n",
      "        [-1.5667e+00,  7.8214e-01, -5.7691e-01,  1.7717e+00, -1.2619e+00,\n",
      "         -7.4635e-01, -2.0530e+00, -3.2675e+00,  2.2365e+00,  1.7965e+00,\n",
      "          1.8474e+00, -1.2452e+00, -1.0552e+00, -1.3828e+00, -1.2093e-01,\n",
      "          5.2575e-01, -7.3024e-01,  1.7249e+00, -9.0794e-01,  2.0740e+00,\n",
      "         -1.3004e+00,  9.9530e-01,  2.8094e+00,  2.0003e+00,  1.4203e+00,\n",
      "          1.2538e+00,  1.8174e+00, -4.1344e-01, -1.5901e+00, -7.2424e-01,\n",
      "         -1.8028e+00,  7.6485e-01,  3.0785e+00, -2.1158e+00, -2.5810e+00,\n",
      "         -1.7968e+00,  1.2634e+00, -5.6055e-01,  2.6529e+00,  1.6614e+00,\n",
      "          8.8387e-01,  1.1162e+00, -1.9246e+00,  2.0495e+00,  2.5541e+00,\n",
      "         -1.8288e-01, -3.1952e-01,  1.9389e-01,  2.9295e-01,  2.3871e+00,\n",
      "         -1.5910e-01, -8.3851e-02, -1.1991e+00,  1.3769e+00,  1.8589e-01,\n",
      "          1.5373e+00, -1.0242e-01,  1.6163e+00, -3.8749e-01, -1.8334e+00,\n",
      "         -1.2870e+00, -1.3007e+00,  1.5842e+00, -3.4426e+00,  4.2744e-01,\n",
      "         -1.9654e-01, -8.9006e-01,  9.2072e-01, -1.2219e+00, -1.3493e+00,\n",
      "         -5.0283e-01,  1.8578e-01, -1.8086e+00,  1.3997e+00, -6.6792e-01,\n",
      "          1.0227e+00, -2.5120e-01,  3.6870e-01, -2.4321e-01,  3.1586e+00,\n",
      "         -2.3257e+00, -1.1646e+00, -1.4097e+00,  7.3020e-01, -1.6028e+00,\n",
      "         -3.1185e+00,  5.9540e-01,  2.1864e-01, -3.5172e+00, -1.1444e+00,\n",
      "         -8.4306e-01,  1.1800e+00, -8.7577e-01, -3.7724e+00,  2.4406e+00,\n",
      "         -4.1250e+00, -1.6616e+00, -1.6737e+00,  2.8017e+00, -1.3613e+00,\n",
      "          5.0336e-01,  3.3219e+00, -2.1909e+00, -3.0694e+00,  1.1974e+00,\n",
      "          9.7304e-01,  1.8434e+00,  2.3926e-01,  2.2560e+00, -1.3331e+00,\n",
      "         -1.8256e+00,  2.3941e+00,  1.9991e+00, -1.8125e+00, -1.2573e+00,\n",
      "         -4.1430e-01, -9.1206e-01,  2.1679e+00, -1.0121e+00, -4.3239e-01,\n",
      "          2.2385e+00, -1.9689e+00, -1.2162e+00, -4.4275e-01, -1.4251e-02,\n",
      "          3.4801e-01,  3.3441e-01, -1.9466e+00],\n",
      "        [ 1.0666e-01, -4.4749e-01,  3.0592e-03,  3.5487e+00,  7.1915e-01,\n",
      "         -1.5275e-01, -9.7117e-02, -2.9817e-01, -8.1239e-02, -9.7092e-01,\n",
      "          2.6279e-01, -7.2066e-02, -1.0161e-01, -4.9066e-01, -3.5642e-01,\n",
      "         -3.6854e-01, -4.6020e-01,  5.0026e+00,  5.7813e-01,  3.1274e-01,\n",
      "         -4.7810e-01,  2.0687e-01,  2.1394e-01, -2.1041e-01, -2.7540e-01,\n",
      "          6.5973e-01, -1.8037e-01, -8.4731e-01, -2.4666e+00, -3.1706e-02,\n",
      "         -2.1351e-01, -8.8217e-02,  4.6630e-01,  5.3028e-01,  8.1138e-02,\n",
      "         -3.4815e-01, -8.5351e-02,  2.3976e-01,  2.2374e+00, -9.5145e-02,\n",
      "          3.7266e+00,  2.1168e-01,  2.7202e-01,  4.3517e-01,  5.4033e-02,\n",
      "          6.7890e-02,  4.5308e-02, -3.9729e-01,  2.5600e+00,  4.9277e-01,\n",
      "          5.0605e-01, -3.1364e-01,  1.3167e-01, -7.4294e-01, -2.0006e-01,\n",
      "          7.5181e-01, -3.0951e-01, -6.3007e-01, -6.4775e-01,  1.0061e-01,\n",
      "          1.7763e-01, -7.7472e-01, -2.7303e-01, -6.9010e-01, -6.1744e-01,\n",
      "         -3.3275e-01,  1.1180e-01,  4.3017e-01, -5.2812e-01,  6.3297e-01,\n",
      "          1.3869e-01,  1.8332e+00,  5.1611e-01, -4.1663e-01, -1.2248e-01,\n",
      "         -7.2120e-01,  8.8940e-02, -8.7361e-01, -5.3072e-01, -5.1299e-01,\n",
      "          4.1828e-01,  4.1859e-02,  1.2541e+00,  1.2910e-01, -1.6070e+00,\n",
      "         -2.5055e-01,  2.0293e-01,  6.7893e-01, -3.0937e-01,  2.0754e-02,\n",
      "         -5.6050e+00,  2.6165e+00,  2.7026e-01, -6.5599e-01, -5.9977e-01,\n",
      "         -1.5575e-01,  6.7848e-01, -1.9105e-01,  1.0062e+00,  1.6024e+00,\n",
      "         -5.8674e-01,  1.4691e-01,  2.3959e-01, -3.4561e-01,  2.2185e-01,\n",
      "         -2.2944e-01, -5.1091e-01,  2.4504e-01, -1.2289e+00,  1.1568e-01,\n",
      "          2.3749e-01, -6.5189e-01, -1.4281e-01,  4.9357e-01, -1.0489e-01,\n",
      "         -4.2840e-02,  6.0504e-01,  7.5009e-02, -1.8288e-01, -1.7912e+00,\n",
      "         -3.2186e-01, -3.6232e-02, -2.0779e-01,  5.1928e-01,  3.7479e-01,\n",
      "          4.1132e-01, -3.7719e-01, -6.8777e-01],\n",
      "        [-7.2112e-01, -3.9909e-01, -5.9451e-01,  2.5383e+00, -2.0145e+00,\n",
      "          6.0613e-01,  3.6847e-01, -2.9590e-01,  9.0987e-01, -7.8571e-01,\n",
      "          7.4591e-01, -4.2301e+00,  1.4296e+00, -1.0402e+00,  2.6900e+00,\n",
      "          4.5009e-01, -1.1203e+00,  7.7857e-01, -1.3797e+00,  1.1300e+00,\n",
      "         -5.1332e-01, -4.8499e-01,  5.6609e-01,  4.0330e+00, -2.8951e+00,\n",
      "         -1.6638e+00, -1.6608e+00, -1.7574e+00, -2.0807e+00, -1.1936e-01,\n",
      "         -1.2634e+00, -1.6363e+00, -1.5997e+00, -3.2901e+00, -1.6680e+00,\n",
      "         -2.7372e-01,  4.6559e-01,  5.4463e+00, -2.8337e-01, -1.7683e+00,\n",
      "          6.7869e-01,  2.1238e+00,  1.5891e-01, -3.4390e-04,  1.6032e+00,\n",
      "         -1.2022e+00,  2.7277e-01,  5.1740e-01,  1.5432e+00,  3.8428e-01,\n",
      "          1.9242e-02,  2.8965e+00, -1.7932e-01,  3.3554e-01,  2.0593e+00,\n",
      "         -6.0717e-01, -1.8294e+00, -2.9545e+00,  1.2066e+00, -2.3541e+00,\n",
      "          1.7533e+00, -1.2602e+00,  2.1070e-01,  2.0741e+00, -1.9738e+00,\n",
      "          5.3876e-01,  7.5917e-01,  1.6107e+00,  1.9651e+00,  1.3642e+00,\n",
      "         -2.8806e-01,  1.7388e+00, -1.6522e-02,  2.5029e-01, -1.2816e+00,\n",
      "         -4.1292e-01, -1.2569e-01, -1.3274e+00, -1.1128e+00,  4.8661e-01,\n",
      "         -1.1139e+00, -2.6534e+00, -1.5986e+00,  2.1205e-01, -5.9854e-01,\n",
      "          1.2666e-01, -1.2050e+00, -9.7301e-01,  4.9259e-01,  5.0122e-01,\n",
      "         -5.0559e+00,  1.8880e+00, -4.3579e+00,  1.0886e+00,  1.9161e+00,\n",
      "          1.3028e-01,  2.3832e+00, -3.0725e-01,  2.1250e+00, -2.3578e+00,\n",
      "          1.9045e+00,  5.4339e-01, -2.1213e+00,  9.4435e-01, -1.7967e+00,\n",
      "          2.1092e-01, -3.0715e-01,  7.5448e-01, -5.7632e-01, -5.8077e-01,\n",
      "         -1.6826e+00,  1.8996e+00, -2.4193e+00,  1.4491e+00, -1.2426e+00,\n",
      "         -1.6369e+00, -8.9155e-01,  5.1826e-01,  2.1166e+00, -1.4683e+00,\n",
      "          5.2440e-01,  1.3618e+00, -3.4798e-01, -2.4109e+00, -2.2999e+00,\n",
      "          8.1323e-03, -1.9149e-01, -3.8314e-02]], device='cuda:0')\n",
      "Averaged embedding: torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "train_df.head(5)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['title'].tolist(),\n",
    "    targets=train_df['score'].tolist(),\n",
    "    vocab=model.vocab  # Use the model's vocabulary\n",
    ")\n",
    "\n",
    "# Create dataloaders with the custom collate function\n",
    "batch_size = 10\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=make_collate_fn(model, device)\n",
    ")\n",
    "\n",
    "item = 2\n",
    "\n",
    "# Inspect raw text inputs\n",
    "sample_text = train_df['title'].iloc[item]\n",
    "print(f\"Raw text: {sample_text}\")\n",
    "\n",
    "# Tokenize the text\n",
    "#tokens = sample_text.lower().split()\n",
    "tokens = preprocess_text(sample_text)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Convert tokens to indices using the vocabulary\n",
    "vocab = vocab.load_vocab(\"../models/word2vec/text8_vocab_NWAll_MF5.json\")\n",
    "indices = [vocab.get_index(token) for token in tokens]\n",
    "print(f\"Token indices: {indices}\")\n",
    "\n",
    "# Get embeddings for the token indices\n",
    "token_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "embeddings = model.embedding(token_tensor)\n",
    "print(f\"Embeddings: {embeddings[0]}\")\n",
    "\n",
    "# Average the embeddings\n",
    "avg_embedding = embeddings.mean(dim=1)\n",
    "print(f\"Averaged embedding: {avg_embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "cd93f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:55:04 | DropoutDisco | INFO     | [vocabulary.py:110] | Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:55:04 | DropoutDisco | INFO     | [vocabulary.py:123] | üìö Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:üìö Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'prince' and 'queen': 0.3710\n"
     ]
    }
   ],
   "source": [
    "# Load the vocabulary\n",
    "vocab = vocab.load_vocab(\"../models/word2vec/text8_vocab_NWAll_MF5.json\")\n",
    "\n",
    "# Load the CBOW model state\n",
    "cbow_state = torch.load(\"../models/word2vec/CBOW_D128_W5_NWAll_MF5_E15_LR0.001_BS512/model_state.pth\", map_location=torch.device('cpu'))\n",
    "\n",
    "# Extract the embeddings\n",
    "embeddings = cbow_state['embeddings.weight']\n",
    "\n",
    "# Define a function to compute cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return torch.dot(vec1, vec2) / (torch.norm(vec1) * torch.norm(vec2))\n",
    "\n",
    "# Check similarity between words\n",
    "def check_word_similarity(word1, word2, vocab, embeddings):\n",
    "    idx1 = vocab.get_index(word1)\n",
    "    idx2 = vocab.get_index(word2)\n",
    "    vec1 = embeddings[idx1]\n",
    "    vec2 = embeddings[idx2]\n",
    "    similarity = cosine_similarity(vec1, vec2)\n",
    "    return similarity.item()\n",
    "\n",
    "# Example: Check similarity between two words\n",
    "word1 = \"prince\"\n",
    "word2 = \"queen\"\n",
    "similarity = check_word_similarity(word1, word2, vocab, embeddings)\n",
    "print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a9049f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the two sentences: 0.3710\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Compare the mean embeddings of two user-input sentences\n",
    "sentence1 = \"prince\"\n",
    "sentence2 = \"queen\"\n",
    "\n",
    "# Preprocess and tokenize the sentences\n",
    "tokens1 = preprocess_text(sentence1)\n",
    "tokens2 = preprocess_text(sentence2)\n",
    "\n",
    "# Convert tokens to indices using the vocabulary\n",
    "indices1 = [vocab.get_index(token) for token in tokens1]\n",
    "indices2 = [vocab.get_index(token) for token in tokens2]\n",
    "\n",
    "# Get embeddings for the token indices\n",
    "token_tensor1 = torch.tensor(indices1, dtype=torch.long).unsqueeze(0).to(device)\n",
    "token_tensor2 = torch.tensor(indices2, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "embeddings1 = model.embedding(token_tensor1)\n",
    "embeddings2 = model.embedding(token_tensor2)\n",
    "\n",
    "# Average the embeddings\n",
    "avg_embedding1 = embeddings1.mean(dim=1)\n",
    "avg_embedding2 = embeddings2.mean(dim=1)\n",
    "\n",
    "# Print the averaged embeddings\n",
    "#print(f\"Averaged embedding for sentence 1: {avg_embedding1}\")\n",
    "#print(f\"Averaged embedding for sentence 2: {avg_embedding2}\")\n",
    "\n",
    "# Compute cosine similarity between the two averaged embeddings\n",
    "cos_sim = cosine_similarity(avg_embedding1.squeeze(), avg_embedding2.squeeze())\n",
    "print(f\"Cosine similarity between the two sentences: {cos_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model state pth\n",
    "model_path = \"../models/text_regression_model.pth\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
