{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "254f8697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dead</th>\n",
       "      <th>type</th>\n",
       "      <th>by</th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "      <th>parent</th>\n",
       "      <th>kids</th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>descendants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34202102</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>viewtransform</td>\n",
       "      <td>2023-01-01 00:07:29</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[34202608]</td>\n",
       "      <td>https://www.youtube.com/watch?v=Sz1n0RHwLqA</td>\n",
       "      <td>5</td>\n",
       "      <td>The physics of entropy and the origin of life ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34202107</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>TheBrokenRail</td>\n",
       "      <td>2023-01-01 00:08:15</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[34203377, 34205953, 34206965, 34204095, 34203...</td>\n",
       "      <td>https://thebrokenrail.com/2022/12/31/xfinity-s...</td>\n",
       "      <td>144</td>\n",
       "      <td>Xfinity Stream on Linux: A Tale of Widevine, C...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34202114</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>forte124</td>\n",
       "      <td>2023-01-01 00:09:17</td>\n",
       "      <td>What types of businesses most likely fall in t...</td>\n",
       "      <td>None</td>\n",
       "      <td>[34203714]</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>Ask HN: Examples of successful, small companie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34202138</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>todsacerdoti</td>\n",
       "      <td>2023-01-01 00:12:29</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.youtube.com/watch?v=q2A-MkGjvmI</td>\n",
       "      <td>4</td>\n",
       "      <td>Let’s try ChatGPT. Is it any good? (Bisqwit)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34202154</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>lisper</td>\n",
       "      <td>2023-01-01 00:14:17</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.amazon.com/Because-Internet-Unders...</td>\n",
       "      <td>2</td>\n",
       "      <td>Because Internet: Understanding the New Rules ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549327</th>\n",
       "      <td>41830822</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>popcalc</td>\n",
       "      <td>2024-10-13 20:30:13</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.youtube.com/watch?v=g-OT4XDqY-o</td>\n",
       "      <td>1</td>\n",
       "      <td>A special packet that can wake-up your PC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549328</th>\n",
       "      <td>41830856</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>todsacerdoti</td>\n",
       "      <td>2024-10-13 20:35:17</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://nolanlawson.com/2024/10/13/the-greatne...</td>\n",
       "      <td>1</td>\n",
       "      <td>The greatness and limitations of the JavaScrip...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549329</th>\n",
       "      <td>41830876</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>thunderbong</td>\n",
       "      <td>2024-10-13 20:37:46</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://slate.com/life/2024/10/tinder-bumble-d...</td>\n",
       "      <td>2</td>\n",
       "      <td>Dating apps destroyed in-person romance. Now t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549330</th>\n",
       "      <td>41830882</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>synthc</td>\n",
       "      <td>2024-10-13 20:38:22</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[41832306, 41830883, 41831437]</td>\n",
       "      <td>https://fireinabottle.net/introducing-the-croi...</td>\n",
       "      <td>5</td>\n",
       "      <td>The Croissant Diet</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549331</th>\n",
       "      <td>41830895</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>amichail</td>\n",
       "      <td>2024-10-13 20:39:50</td>\n",
       "      <td>If you are sick and can&amp;#x27;t go to work, for...</td>\n",
       "      <td>None</td>\n",
       "      <td>[41831168]</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Ask HN: How do you transport telepresence robo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549332 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  dead   type             by                time  \\\n",
       "0       34202102  None  story  viewtransform 2023-01-01 00:07:29   \n",
       "1       34202107  None  story  TheBrokenRail 2023-01-01 00:08:15   \n",
       "2       34202114  None  story       forte124 2023-01-01 00:09:17   \n",
       "3       34202138  None  story   todsacerdoti 2023-01-01 00:12:29   \n",
       "4       34202154  None  story         lisper 2023-01-01 00:14:17   \n",
       "...          ...   ...    ...            ...                 ...   \n",
       "549327  41830822  None  story        popcalc 2024-10-13 20:30:13   \n",
       "549328  41830856  None  story   todsacerdoti 2024-10-13 20:35:17   \n",
       "549329  41830876  None  story    thunderbong 2024-10-13 20:37:46   \n",
       "549330  41830882  None  story         synthc 2024-10-13 20:38:22   \n",
       "549331  41830895  None  story       amichail 2024-10-13 20:39:50   \n",
       "\n",
       "                                                     text parent  \\\n",
       "0                                                    None   None   \n",
       "1                                                    None   None   \n",
       "2       What types of businesses most likely fall in t...   None   \n",
       "3                                                    None   None   \n",
       "4                                                    None   None   \n",
       "...                                                   ...    ...   \n",
       "549327                                               None   None   \n",
       "549328                                               None   None   \n",
       "549329                                               None   None   \n",
       "549330                                               None   None   \n",
       "549331  If you are sick and can&#x27;t go to work, for...   None   \n",
       "\n",
       "                                                     kids  \\\n",
       "0                                              [34202608]   \n",
       "1       [34203377, 34205953, 34206965, 34204095, 34203...   \n",
       "2                                              [34203714]   \n",
       "3                                                    None   \n",
       "4                                                    None   \n",
       "...                                                   ...   \n",
       "549327                                               None   \n",
       "549328                                               None   \n",
       "549329                                               None   \n",
       "549330                     [41832306, 41830883, 41831437]   \n",
       "549331                                         [41831168]   \n",
       "\n",
       "                                                      url  score  \\\n",
       "0             https://www.youtube.com/watch?v=Sz1n0RHwLqA      5   \n",
       "1       https://thebrokenrail.com/2022/12/31/xfinity-s...    144   \n",
       "2                                                    None      4   \n",
       "3             https://www.youtube.com/watch?v=q2A-MkGjvmI      4   \n",
       "4       https://www.amazon.com/Because-Internet-Unders...      2   \n",
       "...                                                   ...    ...   \n",
       "549327        https://www.youtube.com/watch?v=g-OT4XDqY-o      1   \n",
       "549328  https://nolanlawson.com/2024/10/13/the-greatne...      1   \n",
       "549329  https://slate.com/life/2024/10/tinder-bumble-d...      2   \n",
       "549330  https://fireinabottle.net/introducing-the-croi...      5   \n",
       "549331                                               None      1   \n",
       "\n",
       "                                                    title  descendants  \n",
       "0       The physics of entropy and the origin of life ...            1  \n",
       "1       Xfinity Stream on Linux: A Tale of Widevine, C...           71  \n",
       "2       Ask HN: Examples of successful, small companie...            1  \n",
       "3            Let’s try ChatGPT. Is it any good? (Bisqwit)            0  \n",
       "4       Because Internet: Understanding the New Rules ...            0  \n",
       "...                                                   ...          ...  \n",
       "549327          A special packet that can wake-up your PC            0  \n",
       "549328  The greatness and limitations of the JavaScrip...            0  \n",
       "549329  Dating apps destroyed in-person romance. Now t...            0  \n",
       "549330                                 The Croissant Diet            3  \n",
       "549331  Ask HN: How do you transport telepresence robo...            1  \n",
       "\n",
       "[549332 rows x 12 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the database URI directly\n",
    "# !! In real projects, manage credentials securely (e.g., env variables, secrets manager) !!\n",
    "DB_URI = \"postgresql://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\"\n",
    "\n",
    "engine = create_engine(DB_URI)\n",
    "# --- Optional: Set up logging ---\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# Example: Show tables (PostgreSQL metadata)\n",
    "res = pd.read_sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM \"hacker_news\".\"items\" a\n",
    "    WHERE a.type = 'story'\n",
    "        AND a.time >= '2023-01-01 00:00:00'\n",
    "        AND a.dead IS NOT TRUE\n",
    "        AND LENGTH(a.title) > 0\n",
    "        --LIMIT 10\n",
    "\"\"\", engine)\n",
    "\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a76ff6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The physics of entropy and the origin of life ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xfinity Stream on Linux: A Tale of Widevine, C...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ask HN: Examples of successful, small companie...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let’s try ChatGPT. Is it any good? (Bisqwit)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Because Internet: Understanding the New Rules ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Solar thermal storage using lunar regolith</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The craft of SwiftUI API design: Progressive d...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Worst interview questions for software developers</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Running Advent of Code on a $2 microcontroller</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OpenBSD KDE Status Report 2022</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Australia mandates Covid tests for Chinese tou...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Position with the most possible checkmates in 1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>As a Boy in Rural Mexico, His Life Was Changed...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Alt-Right Manipulated My Comic. Then AI Cl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SWA meltdown: technical debt from short term f...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Defining “Enough”</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A Particle That May Fill ‘Empty’ Space</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AMD Continues Working Toward HDR Display Suppo...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Serverless data pipelines for Data Engineers, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Four-year-old Poppy becomes trapped in skill t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>My Thoughts about Editors in 2022</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Best major for good money while also having a ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Competitive programming in Haskell: better bin...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The Computer Made Me Do It – ChatGPT Writes an...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Alan MacMasters: How the great online toaster ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Matrix Community Year in Review 2022</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The Third Magic</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Working on Composite Video Output for the MEGA...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Greatest innovations of 2022: The 35th annual ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The Lisa Computer: A Retrospective [pdf]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Ray Tracer Construction Kit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Ask HN: Any advice on becoming more organized?</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Tipp10 – Free Touch Typing Tutor</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Investing for a World Transformed by AI</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Typesafe DB Like Prisma for Rust</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Meta set to make divisive decision on Trump’s ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MilkyTracker: Open-source, multi-platform appl...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>As Big Tech retrenches, a tech talent shift ac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>The Dark Risk of Large Language Models</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Twitter Search Cheat Sheet (2018)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Show HN: I created my AI clone based on 600.00...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Show HN: Quick Steps in Lambdacalc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>When Place of Birth Is at Sea</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Ask HN: What does 'focus' mean to you?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>How to Go Fast</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>The Eureka Theory of History Is Wrong</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Ultimate guide to creating disks for retro com...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Wikimedia's Abstract Wikipedia project “at sub...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>How and why to properly write copyright statem...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>PSA: It's 2023, update the 'Copyright 2022' in...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  score\n",
       "0   The physics of entropy and the origin of life ...      5\n",
       "1   Xfinity Stream on Linux: A Tale of Widevine, C...    144\n",
       "2   Ask HN: Examples of successful, small companie...      4\n",
       "3        Let’s try ChatGPT. Is it any good? (Bisqwit)      4\n",
       "4   Because Internet: Understanding the New Rules ...      2\n",
       "5          Solar thermal storage using lunar regolith      2\n",
       "6   The craft of SwiftUI API design: Progressive d...      4\n",
       "7   Worst interview questions for software developers    154\n",
       "8      Running Advent of Code on a $2 microcontroller     90\n",
       "9                      OpenBSD KDE Status Report 2022      5\n",
       "10  Australia mandates Covid tests for Chinese tou...      6\n",
       "11    Position with the most possible checkmates in 1      3\n",
       "12  As a Boy in Rural Mexico, His Life Was Changed...      3\n",
       "13  The Alt-Right Manipulated My Comic. Then AI Cl...      4\n",
       "14  SWA meltdown: technical debt from short term f...     10\n",
       "15                                  Defining “Enough”      3\n",
       "16             A Particle That May Fill ‘Empty’ Space      5\n",
       "17  AMD Continues Working Toward HDR Display Suppo...     13\n",
       "18  Serverless data pipelines for Data Engineers, ...      3\n",
       "19  Four-year-old Poppy becomes trapped in skill t...      2\n",
       "20                  My Thoughts about Editors in 2022      5\n",
       "21  Best major for good money while also having a ...      5\n",
       "22  Competitive programming in Haskell: better bin...    131\n",
       "23  The Computer Made Me Do It – ChatGPT Writes an...      2\n",
       "24  Alan MacMasters: How the great online toaster ...     11\n",
       "25               Matrix Community Year in Review 2022     86\n",
       "26                                    The Third Magic      4\n",
       "27  Working on Composite Video Output for the MEGA...      6\n",
       "28  Greatest innovations of 2022: The 35th annual ...      4\n",
       "29           The Lisa Computer: A Retrospective [pdf]      3\n",
       "30                        Ray Tracer Construction Kit      2\n",
       "31     Ask HN: Any advice on becoming more organized?     15\n",
       "32                   Tipp10 – Free Touch Typing Tutor      2\n",
       "33            Investing for a World Transformed by AI      1\n",
       "34                   Typesafe DB Like Prisma for Rust      4\n",
       "35  Meta set to make divisive decision on Trump’s ...      4\n",
       "36  MilkyTracker: Open-source, multi-platform appl...      2\n",
       "37  As Big Tech retrenches, a tech talent shift ac...      2\n",
       "38             The Dark Risk of Large Language Models      2\n",
       "39                  Twitter Search Cheat Sheet (2018)      1\n",
       "40  Show HN: I created my AI clone based on 600.00...      3\n",
       "41                 Show HN: Quick Steps in Lambdacalc      2\n",
       "42                      When Place of Birth Is at Sea      4\n",
       "43             Ask HN: What does 'focus' mean to you?      1\n",
       "44                                     How to Go Fast      2\n",
       "45              The Eureka Theory of History Is Wrong     51\n",
       "46  Ultimate guide to creating disks for retro com...      1\n",
       "47  Wikimedia's Abstract Wikipedia project “at sub...      4\n",
       "48  How and why to properly write copyright statem...     14\n",
       "49  PSA: It's 2023, update the 'Copyright 2022' in...     19"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_and_scores = res.loc[:, ['title', 'score']].copy()\n",
    "titles_and_scores.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fee70c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️  Configuring DropoutDisco logging...\n",
      "  Logger 'DropoutDisco' level set to: INFO\n",
      "  Clearing existing logging handlers...\n",
      "  ✅ Console logging handler added.\n",
      "  ✅ File logging handler added: logs/dropout_disco.log\n",
      "2025-04-16 16:52:39 | DropoutDisco | INFO     | [logging.py:102] | 🎉 Logging system initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:🎉 Logging system initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from utils import logger  # Import the `logger` module from `utils`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f553a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from word2vec.vocabulary import Vocabulary as vocab\n",
    "class TextToRegressionModel(nn.Module):\n",
    "    def __init__(self, vocab_path, cbow_model_path, input_dim, hidden_dims=[128, 64, 32], dropout=0.2):\n",
    "        \"\"\"\n",
    "        Combines vocabulary, CBOW embeddings, and MLP regression model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_path (str): Path to the saved vocabulary JSON.\n",
    "            cbow_model_path (str): Path to the saved CBOW model state.\n",
    "            input_dim (int): Dimension of the input embeddings.\n",
    "            hidden_dims (List[int]): List of hidden layer dimensions.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Load vocabulary\n",
    "        self.vocab = vocab.load_vocab(vocab_path)\n",
    "        \n",
    "        # Load CBOW model and extract embedding layer\n",
    "        cbow_state = torch.load(cbow_model_path, map_location=torch.device('cpu'))\n",
    "        self.embedding = nn.Embedding.from_pretrained(cbow_state['embeddings.weight'])\n",
    "        \n",
    "        # Initialize MLP layers\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Add final output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        # Combine all layers\n",
    "        self.regression_model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is already embedded and averaged from the collate function\n",
    "        return self.regression_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff21790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Replace any non-alphanumeric character with a space\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    # Convert to lowercase and split into tokens\n",
    "    tokens = text.lower().split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, targets, vocab):\n",
    "        \"\"\"\n",
    "        Custom Dataset for text regression.\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): List of input texts.\n",
    "            targets (List[float]): List of target regression values.\n",
    "            vocab (Vocabulary): Vocabulary object for tokenization.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        target = self.targets[idx]\n",
    "        # Preprocess the text\n",
    "        tokens = preprocess_text(text)\n",
    "        # Convert tokens to indices using the vocabulary\n",
    "        indices = [self.vocab.get_index(token) for token in tokens]\n",
    "        return torch.tensor(indices, dtype=torch.long), target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "785161a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_collate_fn(model, device):\n",
    "    def collate_fn(batch):\n",
    "        # Separate the sequences and targets\n",
    "        sequences, targets = zip(*batch)\n",
    "        \n",
    "        # Convert targets to tensor and move to the correct device\n",
    "        targets = torch.stack(targets).to(device)\n",
    "        \n",
    "        # Process each sequence through the model's embedding layer\n",
    "        embedded_sequences = []\n",
    "        for seq in sequences:\n",
    "            # Get embeddings for the sequence\n",
    "            embeddings = model.embedding(seq.to(device))  # Move seq to the correct device\n",
    "            # Average the embeddings\n",
    "            avg_embedding = embeddings.mean(dim=0)\n",
    "            embedded_sequences.append(avg_embedding)\n",
    "        \n",
    "        # Stack the averaged embeddings\n",
    "        embedded_batch = torch.stack(embedded_sequences).to(device)\n",
    "        \n",
    "        return embedded_batch, targets\n",
    "    return collate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc353d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf2b9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming titles_and_scores is your DataFrame\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(titles_and_scores, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['title'].tolist(),\n",
    "    targets=train_df['score'].tolist(),\n",
    "    vocab=vocab  # Your existing vocabulary object\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    texts=test_df['title'].tolist(),\n",
    "    targets=test_df['score'].tolist(),\n",
    "    vocab=vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "332db596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.TextDataset object at 0x7c81478de2c0>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d614ab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "2025-04-16 17:17:36 | DropoutDisco | INFO     | [vocabulary.py:110] | Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-16 17:17:36 | DropoutDisco | INFO     | [vocabulary.py:123] | 📚 Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:📚 Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: nan\n"
     ]
    }
   ],
   "source": [
    "# Detect the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = TextToRegressionModel(\n",
    "    vocab_path=\"../models/word2vec/text8_vocab_NWAll_MF5.json\",  # Replace with your actual path\n",
    "    cbow_model_path=\"../models/word2vec/CBOW_D128_W5_NWAll_MF5_E15_LR0.001_BS512/model_state.pth\",  # Replace with your actual path\n",
    "    input_dim=128,  # Match your CBOW embedding dimension\n",
    "    hidden_dims=[128, 96, 64, 32, 16],  # Example hidden layer dimensions\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['title'].tolist(),\n",
    "    targets=train_df['score'].tolist(),\n",
    "    vocab=model.vocab  # Use the model's vocabulary\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    texts=test_df['title'].tolist(),\n",
    "    targets=test_df['score'].tolist(),\n",
    "    vocab=model.vocab\n",
    ")\n",
    "\n",
    "# Create dataloaders with the custom collate function\n",
    "batch_size = 1024\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=make_collate_fn(model, device)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=make_collate_fn(model, device)\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b661efe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid values in inputs!\n",
      "Invalid values in inputs!\n",
      "Invalid values in inputs!\n",
      "Invalid values in inputs!\n",
      "Invalid values in inputs!\n",
      "Invalid values in inputs!\n",
      "Invalid values in inputs!\n",
      "Invalid values in inputs!\n",
      "Invalid values in inputs!\n",
      "Invalid values in inputs!\n",
      "Invalid values in inputs!\n",
      "Invalid values in inputs!\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    if torch.isnan(inputs).any() or torch.isinf(inputs).any():\n",
    "        print(\"Invalid values in inputs!\")\n",
    "    if torch.isnan(targets).any() or torch.isinf(targets).any():\n",
    "        print(\"Invalid values in targets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0daf1567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 18.6140\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0f1296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 18.9806\n",
      "MSE: 6412.3902\n",
      "RMSE: 80.0774\n",
      "MAE: 18.9286\n",
      "R2 Score: -0.0519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_loss += criterion(outputs.squeeze(), targets).item()\n",
    "        \n",
    "        predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "        actuals.extend(targets.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f'Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "# Calculate additional metrics\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(actuals, predictions)\n",
    "\n",
    "print(f'MSE: {mse:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'R2 Score: {r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "837eb34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1064224243164062"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_score(model, text, device):\n",
    "    \"\"\"\n",
    "    Predict score for a single text input.\n",
    "    \n",
    "    Args:\n",
    "        model (TextToRegressionModel): Trained model\n",
    "        text (str): Input text to predict score for\n",
    "        device (str): Device to run prediction on\n",
    "        \n",
    "    Returns:\n",
    "        float: Predicted score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Preprocess the text\n",
    "        tokens = text.lower().split()\n",
    "        indices = [model.vocab.get_index(token) for token in tokens]\n",
    "        token_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get embeddings and average\n",
    "        embeddings = model.embedding(token_tensor)\n",
    "        avg_embedding = embeddings.mean(dim=1)\n",
    "        \n",
    "        # Get prediction\n",
    "        prediction = model.regression_model(avg_embedding)\n",
    "        return prediction.item()\n",
    "    \n",
    "predict_score(model, \"test text\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60fbd084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextToRegressionModel(\n",
      "  (embedding): Embedding(71291, 128)\n",
      "  (regression_model): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=96, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Linear(in_features=96, out_features=64, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.2, inplace=False)\n",
      "    (12): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.2, inplace=False)\n",
      "    (16): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): Dropout(p=0.2, inplace=False)\n",
      "    (20): Linear(in_features=16, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06cd734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd990b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: Math's 'Game of Life' Reveals Long-Sought Repeating Patterns\n",
      "Tokens: ['math', 's', 'game', 'of', 'life', 'reveals', 'long', 'sought', 'repeating', 'patterns']\n",
      "2025-04-16 17:16:33 | DropoutDisco | INFO     | [vocabulary.py:110] | Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-16 17:16:33 | DropoutDisco | INFO     | [vocabulary.py:123] | 📚 Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:📚 Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token indices: [26907, 342, 6576, 6, 863, 8133, 425, 786, 8075, 1850]\n",
      "Embeddings: tensor([[ 0.5263,  0.9586, -1.8299,  ..., -2.0175, -1.5044,  1.4731],\n",
      "        [ 0.2049,  1.3344, -0.9665,  ..., -1.1682,  0.2235,  0.1650],\n",
      "        [-1.6386, -2.0149, -1.0736,  ...,  2.6468,  0.5863,  0.9648],\n",
      "        ...,\n",
      "        [ 1.3525,  3.0522,  1.5153,  ..., -0.1363, -2.0352,  2.7430],\n",
      "        [ 0.1271, -1.0758, -1.9703,  ...,  1.5935, -0.6970,  0.1737],\n",
      "        [-0.1854,  0.2421, -2.2333,  ..., -1.8195,  1.0001,  0.8238]],\n",
      "       device='cuda:0')\n",
      "Averaged embedding: tensor([[ 0.1076,  0.2204, -0.8686,  0.2745, -1.1864, -0.1390,  1.0319,  0.6062,\n",
      "          0.2069,  0.1624,  0.0597,  0.1354,  0.8648,  0.4364,  0.0431,  0.7406,\n",
      "          0.0552,  0.2273,  0.3179,  0.5687,  0.1716, -0.7294, -0.0464, -0.7934,\n",
      "         -0.0365,  0.2184,  0.0496, -0.3312,  0.1448,  0.0438,  0.1571,  0.7432,\n",
      "          0.3607,  0.9029,  0.6433, -0.4903, -0.4284,  1.1146, -0.0296, -0.3438,\n",
      "          1.2740, -0.9281,  0.2797, -0.0417,  0.3224, -0.0529, -0.1593,  0.7520,\n",
      "         -0.0889,  0.1023,  0.6025,  0.1746, -0.0177,  0.5111,  0.7809, -0.3612,\n",
      "          0.3948, -0.3488,  0.1275,  0.7330, -0.0041, -0.4029,  0.2728,  0.5560,\n",
      "         -0.2388,  0.4421,  0.0452,  0.6595, -0.4774, -0.2113,  0.5337,  0.6817,\n",
      "          0.0122, -0.0304,  0.3576, -0.2950,  0.1297,  0.5730, -0.8700, -0.8600,\n",
      "         -0.1045,  0.5114, -0.0816,  0.3022, -0.0547,  0.4823, -0.3321,  0.6322,\n",
      "         -0.0817, -0.0462, -1.3063,  0.1698,  0.5480, -0.4107, -0.1854, -0.1136,\n",
      "          0.3876,  0.7412,  0.6151,  0.6367,  0.4031, -0.4390,  0.0422,  0.1836,\n",
      "         -0.1630, -0.3316, -0.4062,  0.9912, -0.1562, -0.8694,  0.5336, -0.3346,\n",
      "         -0.0305, -0.5805, -0.5891, -0.5233, -0.1607, -0.1261,  1.3918, -0.1381,\n",
      "         -1.0300, -0.1785,  0.6965, -0.8493,  0.1341,  0.3376,  0.2966,  0.3215]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train_df.head(5)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['title'].tolist(),\n",
    "    targets=train_df['score'].tolist(),\n",
    "    vocab=model.vocab  # Use the model's vocabulary\n",
    ")\n",
    "\n",
    "train_dataset\n",
    "\n",
    "# Create dataloaders with the custom collate function\n",
    "batch_size = 10\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=make_collate_fn(model, device)\n",
    ")\n",
    "\n",
    "# Inspect raw text inputs\n",
    "sample_text = train_df['title'].iloc[0]\n",
    "print(f\"Raw text: {sample_text}\")\n",
    "\n",
    "# Tokenize the text\n",
    "#tokens = sample_text.lower().split()\n",
    "tokens = preprocess_text(sample_text)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Convert tokens to indices using the vocabulary\n",
    "vocab = vocab.load_vocab(\"../models/word2vec/text8_vocab_NWAll_MF5.json\")\n",
    "indices = [vocab.get_index(token) for token in tokens]\n",
    "print(f\"Token indices: {indices}\")\n",
    "\n",
    "# Get embeddings for the token indices\n",
    "token_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "embeddings = model.embedding(token_tensor)\n",
    "print(f\"Embeddings: {embeddings[0]}\")\n",
    "\n",
    "# Average the embeddings\n",
    "avg_embedding = embeddings.mean(dim=1)\n",
    "print(f\"Averaged embedding: {avg_embedding}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
