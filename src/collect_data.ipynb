{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254f8697",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the database URI directly\n",
    "# !! In real projects, manage credentials securely (e.g., env variables, secrets manager) !!\n",
    "DB_URI = \"postgresql://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\"\n",
    "\n",
    "engine = create_engine(DB_URI)\n",
    "# --- Optional: Set up logging ---\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# Example: Show tables (PostgreSQL metadata)\n",
    "res = pd.read_sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM \"hacker_news\".\"items\" a\n",
    "    WHERE a.type = 'story'\n",
    "        AND a.time >= '2023-01-01 00:00:00'\n",
    "        AND a.dead IS NOT TRUE\n",
    "        AND LENGTH(a.title) > 0\n",
    "        --LIMIT 10\n",
    "\"\"\", engine)\n",
    "\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ff6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_and_scores = res.loc[:, ['title', 'score']].copy()\n",
    "titles_and_scores.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from utils import logger  # Import the `logger` module from `utils`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edafee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f553a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from word2vec.vocabulary import Vocabulary as vocab\n",
    "class TextToRegressionModel(nn.Module):\n",
    "    def __init__(self, vocab_path, cbow_model_path, input_dim, hidden_dims=[128, 64, 32], dropout=0.2):\n",
    "        \"\"\"\n",
    "        Combines vocabulary, CBOW embeddings, and MLP regression model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_path (str): Path to the saved vocabulary JSON.\n",
    "            cbow_model_path (str): Path to the saved CBOW model state.\n",
    "            input_dim (int): Dimension of the input embeddings.\n",
    "            hidden_dims (List[int]): List of hidden layer dimensions.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Load vocabulary\n",
    "        self.vocab = vocab.load_vocab(vocab_path)\n",
    "        \n",
    "        # Load CBOW model and extract embedding layer\n",
    "        cbow_state = torch.load(cbow_model_path, map_location=torch.device('cpu'))\n",
    "        self.embedding = nn.Embedding.from_pretrained(cbow_state['embeddings.weight'])\n",
    "        \n",
    "        # Initialize MLP layers\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Add final output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        # Combine all layers\n",
    "        self.regression_model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is already embedded and averaged from the collate function\n",
    "        return self.regression_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff21790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, targets, vocab):\n",
    "        \"\"\"\n",
    "        Custom Dataset for text regression.\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): List of input texts.\n",
    "            targets (List[float]): List of target regression values.\n",
    "            vocab (Vocabulary): Vocabulary object for tokenization.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        target = self.targets[idx]\n",
    "        # Convert text to lowercase and split into tokens\n",
    "        tokens = text.lower().split()\n",
    "        # Get indices for each token, handling unknown words\n",
    "        indices = [self.vocab.get_index(token) for token in tokens]\n",
    "        return torch.tensor(indices, dtype=torch.long), target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785161a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_collate_fn(model, device):\n",
    "    def collate_fn(batch):\n",
    "        # Separate the sequences and targets\n",
    "        sequences, targets = zip(*batch)\n",
    "        \n",
    "        # Convert targets to tensor and move to the correct device\n",
    "        targets = torch.stack(targets).to(device)\n",
    "        \n",
    "        # Process each sequence through the model's embedding layer\n",
    "        embedded_sequences = []\n",
    "        for seq in sequences:\n",
    "            # Get embeddings for the sequence\n",
    "            embeddings = model.embedding(seq.to(device))  # Move seq to the correct device\n",
    "            # Average the embeddings\n",
    "            avg_embedding = embeddings.mean(dim=0)\n",
    "            embedded_sequences.append(avg_embedding)\n",
    "        \n",
    "        # Stack the averaged embeddings\n",
    "        embedded_batch = torch.stack(embedded_sequences).to(device)\n",
    "        \n",
    "        return embedded_batch, targets\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc353d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming titles_and_scores is your DataFrame\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(titles_and_scores, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['title'].tolist(),\n",
    "    targets=train_df['score'].tolist(),\n",
    "    vocab=vocab  # Your existing vocabulary object\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    texts=test_df['title'].tolist(),\n",
    "    targets=test_df['score'].tolist(),\n",
    "    vocab=vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332db596",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d614ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = TextToRegressionModel(\n",
    "    vocab_path=\"../models/word2vec/text8_vocab_NWAll_MF5.json\",  # Replace with your actual path\n",
    "    cbow_model_path=\"../models/word2vec/CBOW_D128_W5_NWAll_MF5_E15_LR0.001_BS512/model_state.pth\",  # Replace with your actual path\n",
    "    input_dim=128,  # Match your CBOW embedding dimension\n",
    "    hidden_dims=[128, 96, 64, 32, 16],  # Example hidden layer dimensions\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['title'].tolist(),\n",
    "    targets=train_df['score'].tolist(),\n",
    "    vocab=model.vocab  # Use the model's vocabulary\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    texts=test_df['title'].tolist(),\n",
    "    targets=test_df['score'].tolist(),\n",
    "    vocab=model.vocab\n",
    ")\n",
    "\n",
    "# Create dataloaders with the custom collate function\n",
    "batch_size = 1024\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=make_collate_fn(model, device)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=make_collate_fn(model, device)\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf1567",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_loss += criterion(outputs.squeeze(), targets).item()\n",
    "        \n",
    "        predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "        actuals.extend(targets.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f'Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "# Calculate additional metrics\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(actuals, predictions)\n",
    "\n",
    "print(f'MSE: {mse:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'R2 Score: {r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837eb34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_score(model, text, device):\n",
    "    \"\"\"\n",
    "    Predict score for a single text input.\n",
    "    \n",
    "    Args:\n",
    "        model (TextToRegressionModel): Trained model\n",
    "        text (str): Input text to predict score for\n",
    "        device (str): Device to run prediction on\n",
    "        \n",
    "    Returns:\n",
    "        float: Predicted score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Preprocess the text\n",
    "        tokens = text.lower().split()\n",
    "        indices = [model.vocab.get_index(token) for token in tokens]\n",
    "        token_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get embeddings and average\n",
    "        embeddings = model.embedding(token_tensor)\n",
    "        avg_embedding = embeddings.mean(dim=1)\n",
    "        \n",
    "        # Get prediction\n",
    "        prediction = model.regression_model(avg_embedding)\n",
    "        return prediction.item()\n",
    "    \n",
    "predict_score(model, \"test text\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fbd084",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06cd734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
