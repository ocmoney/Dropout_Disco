{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "254f8697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dead</th>\n",
       "      <th>type</th>\n",
       "      <th>by</th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "      <th>parent</th>\n",
       "      <th>kids</th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>descendants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34202102</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>viewtransform</td>\n",
       "      <td>2023-01-01 00:07:29</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[34202608]</td>\n",
       "      <td>https://www.youtube.com/watch?v=Sz1n0RHwLqA</td>\n",
       "      <td>5</td>\n",
       "      <td>The physics of entropy and the origin of life ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34202107</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>TheBrokenRail</td>\n",
       "      <td>2023-01-01 00:08:15</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[34203377, 34205953, 34206965, 34204095, 34203...</td>\n",
       "      <td>https://thebrokenrail.com/2022/12/31/xfinity-s...</td>\n",
       "      <td>144</td>\n",
       "      <td>Xfinity Stream on Linux: A Tale of Widevine, C...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34202114</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>forte124</td>\n",
       "      <td>2023-01-01 00:09:17</td>\n",
       "      <td>What types of businesses most likely fall in t...</td>\n",
       "      <td>None</td>\n",
       "      <td>[34203714]</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>Ask HN: Examples of successful, small companie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34202138</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>todsacerdoti</td>\n",
       "      <td>2023-01-01 00:12:29</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.youtube.com/watch?v=q2A-MkGjvmI</td>\n",
       "      <td>4</td>\n",
       "      <td>Let‚Äôs try ChatGPT. Is it any good? (Bisqwit)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34202154</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>lisper</td>\n",
       "      <td>2023-01-01 00:14:17</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.amazon.com/Because-Internet-Unders...</td>\n",
       "      <td>2</td>\n",
       "      <td>Because Internet: Understanding the New Rules ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549327</th>\n",
       "      <td>40874571</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>EndXA</td>\n",
       "      <td>2024-07-04 13:58:14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.smithsonianmag.com/innovation/arth...</td>\n",
       "      <td>1</td>\n",
       "      <td>The Mind of a Mathemagician (2015)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549328</th>\n",
       "      <td>40874575</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>luckyou</td>\n",
       "      <td>2024-07-04 13:58:40</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[40874638, 40890409, 40874996]</td>\n",
       "      <td>https://coroot.com/blog/datadog-is-the-new-oracle</td>\n",
       "      <td>8</td>\n",
       "      <td>Datadog Is the New Oracle</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549329</th>\n",
       "      <td>40874587</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>pseudolus</td>\n",
       "      <td>2024-07-04 13:59:24</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[40874642]</td>\n",
       "      <td>https://medicalxpress.com/news/2024-07-air-pol...</td>\n",
       "      <td>6</td>\n",
       "      <td>Air pollution drives 7% of deaths in big India...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549330</th>\n",
       "      <td>40874588</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>belter</td>\n",
       "      <td>2024-07-04 13:59:25</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[40883299]</td>\n",
       "      <td>https://www.youtube.com/watch?v=tsTeEkzO9xc</td>\n",
       "      <td>2</td>\n",
       "      <td>Andrej Karpathy's Keynote and Winner Pitches a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549331</th>\n",
       "      <td>40874592</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "      <td>vincent_s</td>\n",
       "      <td>2024-07-04 13:59:46</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[40874610, 40874662]</td>\n",
       "      <td>https://www.youtube.com/watch?v=hm2IJSKcYvo</td>\n",
       "      <td>2</td>\n",
       "      <td>Moshi, a new 'real-time' AI voice assistant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549332 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  dead   type             by                time  \\\n",
       "0       34202102  None  story  viewtransform 2023-01-01 00:07:29   \n",
       "1       34202107  None  story  TheBrokenRail 2023-01-01 00:08:15   \n",
       "2       34202114  None  story       forte124 2023-01-01 00:09:17   \n",
       "3       34202138  None  story   todsacerdoti 2023-01-01 00:12:29   \n",
       "4       34202154  None  story         lisper 2023-01-01 00:14:17   \n",
       "...          ...   ...    ...            ...                 ...   \n",
       "549327  40874571  None  story          EndXA 2024-07-04 13:58:14   \n",
       "549328  40874575  None  story        luckyou 2024-07-04 13:58:40   \n",
       "549329  40874587  None  story      pseudolus 2024-07-04 13:59:24   \n",
       "549330  40874588  None  story         belter 2024-07-04 13:59:25   \n",
       "549331  40874592  None  story      vincent_s 2024-07-04 13:59:46   \n",
       "\n",
       "                                                     text parent  \\\n",
       "0                                                    None   None   \n",
       "1                                                    None   None   \n",
       "2       What types of businesses most likely fall in t...   None   \n",
       "3                                                    None   None   \n",
       "4                                                    None   None   \n",
       "...                                                   ...    ...   \n",
       "549327                                               None   None   \n",
       "549328                                               None   None   \n",
       "549329                                               None   None   \n",
       "549330                                               None   None   \n",
       "549331                                               None   None   \n",
       "\n",
       "                                                     kids  \\\n",
       "0                                              [34202608]   \n",
       "1       [34203377, 34205953, 34206965, 34204095, 34203...   \n",
       "2                                              [34203714]   \n",
       "3                                                    None   \n",
       "4                                                    None   \n",
       "...                                                   ...   \n",
       "549327                                               None   \n",
       "549328                     [40874638, 40890409, 40874996]   \n",
       "549329                                         [40874642]   \n",
       "549330                                         [40883299]   \n",
       "549331                               [40874610, 40874662]   \n",
       "\n",
       "                                                      url  score  \\\n",
       "0             https://www.youtube.com/watch?v=Sz1n0RHwLqA      5   \n",
       "1       https://thebrokenrail.com/2022/12/31/xfinity-s...    144   \n",
       "2                                                    None      4   \n",
       "3             https://www.youtube.com/watch?v=q2A-MkGjvmI      4   \n",
       "4       https://www.amazon.com/Because-Internet-Unders...      2   \n",
       "...                                                   ...    ...   \n",
       "549327  https://www.smithsonianmag.com/innovation/arth...      1   \n",
       "549328  https://coroot.com/blog/datadog-is-the-new-oracle      8   \n",
       "549329  https://medicalxpress.com/news/2024-07-air-pol...      6   \n",
       "549330        https://www.youtube.com/watch?v=tsTeEkzO9xc      2   \n",
       "549331        https://www.youtube.com/watch?v=hm2IJSKcYvo      2   \n",
       "\n",
       "                                                    title  descendants  \n",
       "0       The physics of entropy and the origin of life ...            1  \n",
       "1       Xfinity Stream on Linux: A Tale of Widevine, C...           71  \n",
       "2       Ask HN: Examples of successful, small companie...            1  \n",
       "3            Let‚Äôs try ChatGPT. Is it any good? (Bisqwit)            0  \n",
       "4       Because Internet: Understanding the New Rules ...            0  \n",
       "...                                                   ...          ...  \n",
       "549327                 The Mind of a Mathemagician (2015)            0  \n",
       "549328                          Datadog Is the New Oracle            6  \n",
       "549329  Air pollution drives 7% of deaths in big India...            0  \n",
       "549330  Andrej Karpathy's Keynote and Winner Pitches a...            1  \n",
       "549331        Moshi, a new 'real-time' AI voice assistant            1  \n",
       "\n",
       "[549332 rows x 12 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the database URI directly\n",
    "# !! In real projects, manage credentials securely (e.g., env variables, secrets manager) !!\n",
    "DB_URI = \"postgresql://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\"\n",
    "\n",
    "engine = create_engine(DB_URI)\n",
    "# --- Optional: Set up logging ---\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# Example: Show tables (PostgreSQL metadata)\n",
    "res = pd.read_sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM \"hacker_news\".\"items\" a\n",
    "    WHERE a.type = 'story'\n",
    "        AND a.time >= '2023-01-01 00:00:00'\n",
    "        AND a.dead IS NOT TRUE\n",
    "        AND LENGTH(a.title) > 0\n",
    "        --LIMIT 10\n",
    "\"\"\", engine)\n",
    "\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a76ff6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The physics of entropy and the origin of life ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xfinity Stream on Linux: A Tale of Widevine, C...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ask HN: Examples of successful, small companie...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let‚Äôs try ChatGPT. Is it any good? (Bisqwit)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Because Internet: Understanding the New Rules ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Solar thermal storage using lunar regolith</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The craft of SwiftUI API design: Progressive d...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Worst interview questions for software developers</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Running Advent of Code on a $2 microcontroller</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OpenBSD KDE Status Report 2022</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Australia mandates Covid tests for Chinese tou...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Position with the most possible checkmates in 1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>As a Boy in Rural Mexico, His Life Was Changed...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Alt-Right Manipulated My Comic. Then AI Cl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SWA meltdown: technical debt from short term f...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Defining ‚ÄúEnough‚Äù</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A Particle That May Fill ‚ÄòEmpty‚Äô Space</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AMD Continues Working Toward HDR Display Suppo...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Serverless data pipelines for Data Engineers, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Four-year-old Poppy becomes trapped in skill t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>My Thoughts about Editors in 2022</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Best major for good money while also having a ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Competitive programming in Haskell: better bin...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The Computer Made Me Do It ‚Äì ChatGPT Writes an...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Alan MacMasters: How the great online toaster ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Matrix Community Year in Review 2022</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The Third Magic</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Working on Composite Video Output for the MEGA...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Greatest innovations of 2022: The 35th annual ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The Lisa Computer: A Retrospective [pdf]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Ray Tracer Construction Kit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Ask HN: Any advice on becoming more organized?</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Tipp10 ‚Äì Free Touch Typing Tutor</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Investing for a World Transformed by AI</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Typesafe DB Like Prisma for Rust</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Meta set to make divisive decision on Trump‚Äôs ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MilkyTracker: Open-source, multi-platform appl...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>As Big Tech retrenches, a tech talent shift ac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>The Dark Risk of Large Language Models</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Twitter Search Cheat Sheet (2018)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Show HN: I created my AI clone based on 600.00...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Show HN: Quick Steps in Lambdacalc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>When Place of Birth Is at Sea</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Ask HN: What does 'focus' mean to you?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>How to Go Fast</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>The Eureka Theory of History Is Wrong</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Ultimate guide to creating disks for retro com...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Wikimedia's Abstract Wikipedia project ‚Äúat sub...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>How and why to properly write copyright statem...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>PSA: It's 2023, update the 'Copyright 2022' in...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  score\n",
       "0   The physics of entropy and the origin of life ...      5\n",
       "1   Xfinity Stream on Linux: A Tale of Widevine, C...    144\n",
       "2   Ask HN: Examples of successful, small companie...      4\n",
       "3        Let‚Äôs try ChatGPT. Is it any good? (Bisqwit)      4\n",
       "4   Because Internet: Understanding the New Rules ...      2\n",
       "5          Solar thermal storage using lunar regolith      2\n",
       "6   The craft of SwiftUI API design: Progressive d...      4\n",
       "7   Worst interview questions for software developers    154\n",
       "8      Running Advent of Code on a $2 microcontroller     90\n",
       "9                      OpenBSD KDE Status Report 2022      5\n",
       "10  Australia mandates Covid tests for Chinese tou...      6\n",
       "11    Position with the most possible checkmates in 1      3\n",
       "12  As a Boy in Rural Mexico, His Life Was Changed...      3\n",
       "13  The Alt-Right Manipulated My Comic. Then AI Cl...      4\n",
       "14  SWA meltdown: technical debt from short term f...     10\n",
       "15                                  Defining ‚ÄúEnough‚Äù      3\n",
       "16             A Particle That May Fill ‚ÄòEmpty‚Äô Space      5\n",
       "17  AMD Continues Working Toward HDR Display Suppo...     13\n",
       "18  Serverless data pipelines for Data Engineers, ...      3\n",
       "19  Four-year-old Poppy becomes trapped in skill t...      2\n",
       "20                  My Thoughts about Editors in 2022      5\n",
       "21  Best major for good money while also having a ...      5\n",
       "22  Competitive programming in Haskell: better bin...    131\n",
       "23  The Computer Made Me Do It ‚Äì ChatGPT Writes an...      2\n",
       "24  Alan MacMasters: How the great online toaster ...     11\n",
       "25               Matrix Community Year in Review 2022     86\n",
       "26                                    The Third Magic      4\n",
       "27  Working on Composite Video Output for the MEGA...      6\n",
       "28  Greatest innovations of 2022: The 35th annual ...      4\n",
       "29           The Lisa Computer: A Retrospective [pdf]      3\n",
       "30                        Ray Tracer Construction Kit      2\n",
       "31     Ask HN: Any advice on becoming more organized?     15\n",
       "32                   Tipp10 ‚Äì Free Touch Typing Tutor      2\n",
       "33            Investing for a World Transformed by AI      1\n",
       "34                   Typesafe DB Like Prisma for Rust      4\n",
       "35  Meta set to make divisive decision on Trump‚Äôs ...      4\n",
       "36  MilkyTracker: Open-source, multi-platform appl...      2\n",
       "37  As Big Tech retrenches, a tech talent shift ac...      2\n",
       "38             The Dark Risk of Large Language Models      2\n",
       "39                  Twitter Search Cheat Sheet (2018)      1\n",
       "40  Show HN: I created my AI clone based on 600.00...      3\n",
       "41                 Show HN: Quick Steps in Lambdacalc      2\n",
       "42                      When Place of Birth Is at Sea      4\n",
       "43             Ask HN: What does 'focus' mean to you?      1\n",
       "44                                     How to Go Fast      2\n",
       "45              The Eureka Theory of History Is Wrong     51\n",
       "46  Ultimate guide to creating disks for retro com...      1\n",
       "47  Wikimedia's Abstract Wikipedia project ‚Äúat sub...      4\n",
       "48  How and why to properly write copyright statem...     14\n",
       "49  PSA: It's 2023, update the 'Copyright 2022' in...     19"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_and_scores = res.loc[:, ['title', 'score']].copy()\n",
    "titles_and_scores.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fee70c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Configuring DropoutDisco logging...\n",
      "  Logger 'DropoutDisco' level set to: INFO\n",
      "  Clearing existing logging handlers...\n",
      "  ‚úÖ Console logging handler added.\n",
      "  ‚úÖ File logging handler added: logs/dropout_disco.log\n",
      "2025-04-17 09:44:14 | DropoutDisco | INFO     | [logging.py:102] | üéâ Logging system initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:üéâ Logging system initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from utils import logger  # Import the `logger` module from `utils`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f553a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-17 09:44:17 | DropoutDisco | INFO     | [trainer.py:21] | wandb not installed, W&B logging disabled in trainer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:wandb not installed, W&B logging disabled in trainer.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from word2vec.vocabulary import Vocabulary as vocab\n",
    "\n",
    "class TextToRegressionModel(nn.Module):\n",
    "    def __init__(self, vocab_path, cbow_model_path, input_dim, hidden_dims=[128, 64, 32], dropout=0.2):\n",
    "        \"\"\"\n",
    "        Combines vocabulary, CBOW embeddings, and MLP regression model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_path (str): Path to the saved vocabulary JSON.\n",
    "            cbow_model_path (str): Path to the saved CBOW model state.\n",
    "            input_dim (int): Dimension of the input embeddings.\n",
    "            hidden_dims (List[int]): List of hidden layer dimensions.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Load vocabulary\n",
    "        self.vocab = vocab.load_vocab(vocab_path)\n",
    "        \n",
    "        # Load CBOW model and extract embedding layer\n",
    "        cbow_state = torch.load(cbow_model_path, map_location=torch.device('cpu'))\n",
    "        self.embedding = nn.Embedding.from_pretrained(cbow_state['embeddings.weight'])\n",
    "        \n",
    "        # Initialize MLP layers\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Add final output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        # Combine all layers\n",
    "        self.regression_model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is already embedded and averaged from the collate function\n",
    "        return self.regression_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ef084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff21790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Replace any non-alphanumeric character with a space\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    # Convert to lowercase and split into tokens\n",
    "    tokens = text.lower().split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, targets, vocab):\n",
    "        \"\"\"\n",
    "        Custom Dataset for text regression.\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): List of input texts.\n",
    "            targets (List[float]): List of target regression values.\n",
    "            vocab (Vocabulary): Vocabulary object for tokenization.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        target = self.targets[idx]\n",
    "        # Preprocess the text\n",
    "        tokens = preprocess_text(text)\n",
    "        # Convert tokens to indices using the vocabulary\n",
    "        indices = [self.vocab.get_index(token) for token in tokens]\n",
    "        return torch.tensor(indices, dtype=torch.long), target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "785161a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_collate_fn(model, device):\n",
    "    def collate_fn(batch):\n",
    "        sequences, targets = zip(*batch)\n",
    "        targets = torch.stack(targets).to(device)\n",
    "\n",
    "        embedded_sequences = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) == 0:  # Handle empty sequences\n",
    "                embedded_sequences.append(torch.zeros(model.embedding.embedding_dim).to(device))\n",
    "            else:\n",
    "                embeddings = model.embedding(seq.to(device))\n",
    "                avg_embedding = embeddings.mean(dim=0)\n",
    "                embedded_sequences.append(avg_embedding)\n",
    "\n",
    "        embedded_batch = torch.stack(embedded_sequences).to(device)\n",
    "        return embedded_batch, targets\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2bc353d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "cf2b9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming titles_and_scores is your DataFrame\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(titles_and_scores, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d614ab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "2025-04-17 14:15:58 | DropoutDisco | INFO     | [vocabulary.py:110] | Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-17 14:15:58 | DropoutDisco | INFO     | [vocabulary.py:123] | üìö Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:üìö Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 1.0577\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Detect the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = TextToRegressionModel(\n",
    "    vocab_path=\"../models/word2vec/text8_vocab_NWAll_MF5.json\",  # Replace with your actual path\n",
    "    cbow_model_path=\"../models/word2vec/CBOW_D128_W5_NWAll_MF5_E15_LR0.001_BS512/model_state.pth\",  # Replace with your actual path\n",
    "    input_dim=128,  # Match your CBOW embedding dimension\n",
    "    hidden_dims=[96, 64, 32, 16],  # Example hidden layer dimensions\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Apply log scaling to the scores in the training and testing datasets\n",
    "train_df['score'] = train_df['score'].apply(lambda x: np.log(x))  # log1p ensures log(0) is handled\n",
    "test_df['score'] = test_df['score'].apply(lambda x: np.log(x))\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['title'].tolist(),\n",
    "    targets=train_df['score'].tolist(),\n",
    "    vocab=model.vocab  # Use the model's vocabulary\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    texts=test_df['title'].tolist(),\n",
    "    targets=test_df['score'].tolist(),\n",
    "    vocab=model.vocab\n",
    ")\n",
    "\n",
    "# Create dataloaders with the custom collate function\n",
    "batch_size = 2048\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=make_collate_fn(model, device)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=make_collate_fn(model, device)\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "d1597ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'regression_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "55f92657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 entries with the highest scores:\n",
      "                                                    title     score\n",
      "140501                Apollo will close down on June 30th  8.137688\n",
      "61062                 FDIC Takes over Silicon Valley Bank  8.062118\n",
      "462822  Julian Assange has reached a plea deal with th...  7.892078\n",
      "109565  Google ‚ÄúWe have no moat, and neither does OpenAI‚Äù  7.805882\n",
      "273913                                   Omegle 2009-2023  7.805475\n",
      "538318                                        Bop Spotter  7.794823\n",
      "514009           The secret inside One Million Checkboxes  7.768533\n",
      "473141  Reverse engineering Ticketmaster's rotating ba...  7.742836\n",
      "156549  macOS command-line tools you might not know about  7.620215\n",
      "133331          Had a call with Reddit to discuss pricing  7.592870\n"
     ]
    }
   ],
   "source": [
    "# find the 10 entries with the highest scores\n",
    "top_10_entries = test_df.nlargest(10, 'score')\n",
    "# Print the top 10 entries\n",
    "print(\"Top 10 entries with the highest scores:\")\n",
    "print(top_10_entries[['title', 'score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "0daf1567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.9861\n",
      "Epoch 2/3, Loss: 0.9802\n",
      "Epoch 3/3, Loss: 0.9759\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "e0f1296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 5605.1650\n",
      "RMSE: 74.8676\n",
      "MAE: 18.4305\n",
      "R2 Score: -0.0569\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Reverse the log transformation\n",
    "        predictions.extend(np.exp(outputs.squeeze().cpu().numpy()))  # Convert predictions back to original scale\n",
    "        actuals.extend(np.exp(targets.cpu().numpy()))  # Convert targets back to original scale\n",
    "\n",
    "# Calculate errors in the original scale\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(actuals, predictions)\n",
    "\n",
    "print(f'MSE: {mse:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'R2 Score: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "837eb34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blogging']\n",
      "[49847]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9770457204957508"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_score(model, text, device):\n",
    "    \"\"\"\n",
    "    Predict score for a single text input.\n",
    "    \n",
    "    Args:\n",
    "        model (TextToRegressionModel): Trained model\n",
    "        text (str): Input text to predict score for\n",
    "        device (str): Device to run prediction on\n",
    "        \n",
    "    Returns:\n",
    "        float: Predicted score\n",
    "    \"\"\"\n",
    "    # Use the preprocess_text function for text preprocessing\n",
    "    tokens = preprocess_text(text)\n",
    "    #tokens = text.lower().split() # old preprocessing that just splits and lowercases\n",
    "    print(tokens)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Preprocess the text\n",
    "        indices = [model.vocab.get_index(token) for token in tokens]\n",
    "        print(indices)\n",
    "        token_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get embeddings and average\n",
    "        embeddings = model.embedding(token_tensor)\n",
    "\n",
    "        avg_embedding = embeddings.mean(dim=1)\n",
    "        \n",
    "        # Get prediction\n",
    "        prediction = model.regression_model(avg_embedding)\n",
    "        return np.exp(prediction.item()).item()\n",
    "\n",
    "predict_score(model, \"blogging\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "60fbd084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7311/1395019529.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  monarch_entries['score'] = np.exp(monarch_entries['score'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192750</th>\n",
       "      <td>The Absolute Worst Way to Read Typed Array Dat...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182379</th>\n",
       "      <td>Absolute Unit NNs: Regression-Based MLPs for E...</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304924</th>\n",
       "      <td>JWST Captures Image of Supernova That 'Absolut...</td>\n",
       "      <td>190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517131</th>\n",
       "      <td>The Co¬≠Initialize¬≠Security function demands an...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307169</th>\n",
       "      <td>Tekken 8 story mode is absolutely insane in a ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375851</th>\n",
       "      <td>Absolute Power Corrupts Absolutely</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162523</th>\n",
       "      <td>Grammys Boss:Music 'with AI-created elements' ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449159</th>\n",
       "      <td>Atoms at temperatures beyond absolute zero may...</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505946</th>\n",
       "      <td>Absolute pitch in involuntary musical imagery</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151861</th>\n",
       "      <td>How my children (n=2) acquired absolute pitch</td>\n",
       "      <td>198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387955</th>\n",
       "      <td>You Absolutely (Do Not) Need to Comment Your Code</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452922</th>\n",
       "      <td>High-precision absolute linear encoder based o...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209640</th>\n",
       "      <td>Football must look at how individuals such Lui...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99415</th>\n",
       "      <td>ReceptioGate and the (Absolute) State of Academia</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93477</th>\n",
       "      <td>Absolute Height of a Mountain</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38037</th>\n",
       "      <td>Twitter recommendation algorithm was using abs...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244915</th>\n",
       "      <td>September‚Äôs record-setting temps were ‚Äúabsolut...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506807</th>\n",
       "      <td>Banks That Financed Elon Musk's Acquisition of...</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303283</th>\n",
       "      <td>SF Supervisor Dean Preston: homeless crisis is...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40558</th>\n",
       "      <td>Why DDR5 Is Absolutely Necessary in Modern Ser...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325503</th>\n",
       "      <td>Software Engineering for Absolute Beginners: G...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338085</th>\n",
       "      <td>Can science ever discover the absolute truth a...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105211</th>\n",
       "      <td>Children aged 2-6 successfully trained to acqu...</td>\n",
       "      <td>421.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482947</th>\n",
       "      <td>I Built My Dream Keyboard from Absolute Scratc...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353949</th>\n",
       "      <td>The Lambda Calculus for Absolute Dummies</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535595</th>\n",
       "      <td>Researchers discover 200-year-old message in a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449049</th>\n",
       "      <td>Absolute for Death?</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546971</th>\n",
       "      <td>Valproate reopens critical-period learning of ...</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186883</th>\n",
       "      <td>Rust Absolutely Positively Sucks [video]</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142530</th>\n",
       "      <td>I Used Apple‚Äôs Vision Pro and It‚Äôs Absolutely ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  score\n",
       "192750  The Absolute Worst Way to Read Typed Array Dat...    1.0\n",
       "182379  Absolute Unit NNs: Regression-Based MLPs for E...   17.0\n",
       "304924  JWST Captures Image of Supernova That 'Absolut...  190.0\n",
       "517131  The Co¬≠Initialize¬≠Security function demands an...    1.0\n",
       "307169  Tekken 8 story mode is absolutely insane in a ...    1.0\n",
       "375851                 Absolute Power Corrupts Absolutely    1.0\n",
       "162523  Grammys Boss:Music 'with AI-created elements' ...    3.0\n",
       "449159  Atoms at temperatures beyond absolute zero may...    6.0\n",
       "505946      Absolute pitch in involuntary musical imagery    1.0\n",
       "151861      How my children (n=2) acquired absolute pitch  198.0\n",
       "387955  You Absolutely (Do Not) Need to Comment Your Code    1.0\n",
       "452922  High-precision absolute linear encoder based o...    1.0\n",
       "209640  Football must look at how individuals such Lui...    3.0\n",
       "99415   ReceptioGate and the (Absolute) State of Academia   77.0\n",
       "93477                       Absolute Height of a Mountain    1.0\n",
       "38037   Twitter recommendation algorithm was using abs...    7.0\n",
       "244915  September‚Äôs record-setting temps were ‚Äúabsolut...    5.0\n",
       "506807  Banks That Financed Elon Musk's Acquisition of...   12.0\n",
       "303283  SF Supervisor Dean Preston: homeless crisis is...    2.0\n",
       "40558   Why DDR5 Is Absolutely Necessary in Modern Ser...    4.0\n",
       "325503  Software Engineering for Absolute Beginners: G...    3.0\n",
       "338085  Can science ever discover the absolute truth a...    2.0\n",
       "105211  Children aged 2-6 successfully trained to acqu...  421.0\n",
       "482947  I Built My Dream Keyboard from Absolute Scratc...    2.0\n",
       "353949           The Lambda Calculus for Absolute Dummies    3.0\n",
       "535595  Researchers discover 200-year-old message in a...    1.0\n",
       "449049                                Absolute for Death?    1.0\n",
       "546971  Valproate reopens critical-period learning of ...   16.0\n",
       "186883           Rust Absolutely Positively Sucks [video]   30.0\n",
       "142530  I Used Apple‚Äôs Vision Pro and It‚Äôs Absolutely ...    5.0"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all entries in train_df that contain \"monarch\" in the title column\n",
    "# and print the first 10\n",
    "monarch_entries = train_df[train_df['title'].str.contains(\"absolute\", case=False)]\n",
    "monarch_entries['score'] = np.exp(monarch_entries['score'])\n",
    "monarch_entries.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd990b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: Show HN: Buyidentities.com\n",
      "Tokens: ['show', 'hn', 'buyidentities', 'com']\n",
      "2025-04-17 10:38:09 | DropoutDisco | INFO     | [vocabulary.py:110] | Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-17 10:38:09 | DropoutDisco | INFO     | [vocabulary.py:123] | üìö Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:üìö Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token indices: [2194, 38072, 0, 2806]\n",
      "Embeddings: tensor([[ 1.9466e+00,  4.3013e-01, -2.0402e+00,  4.4432e-01, -2.1084e+00,\n",
      "          2.7667e-01,  9.9176e-01,  8.9653e-01,  2.2319e-01, -1.3121e+00,\n",
      "          4.9519e-02, -1.8727e-01,  7.8188e-01, -2.2465e+00, -1.2707e-01,\n",
      "         -3.1723e-03,  1.3502e+00,  1.2984e+00, -1.1449e-01, -1.8985e-01,\n",
      "         -9.1769e-01, -1.3087e-01,  2.9257e-01, -8.9975e-01, -5.0819e-01,\n",
      "          8.5919e-01, -3.4376e+00,  7.6520e-01,  1.3828e+00,  2.9100e-01,\n",
      "          1.2134e+00,  7.9240e-01,  4.1146e-01, -1.7667e+00, -2.8801e+00,\n",
      "          2.5893e-01, -1.6380e-01,  3.6928e-01, -5.3449e-01, -1.8259e-01,\n",
      "          2.1736e+00, -2.1261e+00,  1.3076e+00, -3.2017e+00,  3.3742e-01,\n",
      "         -1.2346e+00,  1.3087e+00,  2.1186e+00, -1.7250e+00,  4.6178e-01,\n",
      "         -5.8093e-01, -1.4234e-01,  1.6505e+00, -1.9864e-02,  1.4873e-01,\n",
      "         -6.8776e-01,  1.3533e+00, -8.1330e-01,  9.0072e-01,  7.5148e-01,\n",
      "          7.8481e-01, -8.7399e-01, -4.5775e-02, -1.1776e-01, -1.1477e+00,\n",
      "         -1.0841e+00, -2.2613e-01, -1.0361e+00, -2.7476e+00, -1.9721e-01,\n",
      "          3.0830e-01,  1.5420e+00,  5.3548e-01, -8.6643e-01, -1.3576e+00,\n",
      "         -3.4131e-01, -7.8010e-01,  2.7584e-01, -2.1082e+00,  1.4421e+00,\n",
      "         -3.5891e-01,  3.4663e-01, -2.2047e+00,  2.2207e-01,  1.0311e+00,\n",
      "          4.5479e-02, -1.8573e-01,  3.2395e+00,  2.2179e+00, -3.6438e-01,\n",
      "         -3.9641e+00, -1.1545e+00, -1.8063e+00, -5.1812e-01, -1.2691e+00,\n",
      "         -1.1358e+00, -1.8693e+00, -4.1736e-01, -1.6325e+00,  1.2383e+00,\n",
      "          1.8732e+00,  2.1580e+00,  2.0025e+00,  1.4012e+00,  5.4559e-01,\n",
      "          3.0559e+00, -6.0173e-01, -1.1741e-01,  7.7093e-01, -1.5293e+00,\n",
      "         -8.6734e-01, -3.1346e-01,  3.7775e-01,  2.7541e-02,  2.9125e+00,\n",
      "          1.9542e+00,  4.5796e-01,  1.0115e+00,  2.5596e+00, -3.1819e+00,\n",
      "          5.9765e-01,  2.4012e-01, -1.5479e+00, -9.3829e-01, -1.4166e+00,\n",
      "         -4.5177e-01, -5.5938e-01, -1.1085e+00],\n",
      "        [-1.5667e+00,  7.8214e-01, -5.7691e-01,  1.7717e+00, -1.2619e+00,\n",
      "         -7.4635e-01, -2.0530e+00, -3.2675e+00,  2.2365e+00,  1.7965e+00,\n",
      "          1.8474e+00, -1.2452e+00, -1.0552e+00, -1.3828e+00, -1.2093e-01,\n",
      "          5.2575e-01, -7.3024e-01,  1.7249e+00, -9.0794e-01,  2.0740e+00,\n",
      "         -1.3004e+00,  9.9530e-01,  2.8094e+00,  2.0003e+00,  1.4203e+00,\n",
      "          1.2538e+00,  1.8174e+00, -4.1344e-01, -1.5901e+00, -7.2424e-01,\n",
      "         -1.8028e+00,  7.6485e-01,  3.0785e+00, -2.1158e+00, -2.5810e+00,\n",
      "         -1.7968e+00,  1.2634e+00, -5.6055e-01,  2.6529e+00,  1.6614e+00,\n",
      "          8.8387e-01,  1.1162e+00, -1.9246e+00,  2.0495e+00,  2.5541e+00,\n",
      "         -1.8288e-01, -3.1952e-01,  1.9389e-01,  2.9295e-01,  2.3871e+00,\n",
      "         -1.5910e-01, -8.3851e-02, -1.1991e+00,  1.3769e+00,  1.8589e-01,\n",
      "          1.5373e+00, -1.0242e-01,  1.6163e+00, -3.8749e-01, -1.8334e+00,\n",
      "         -1.2870e+00, -1.3007e+00,  1.5842e+00, -3.4426e+00,  4.2744e-01,\n",
      "         -1.9654e-01, -8.9006e-01,  9.2072e-01, -1.2219e+00, -1.3493e+00,\n",
      "         -5.0283e-01,  1.8578e-01, -1.8086e+00,  1.3997e+00, -6.6792e-01,\n",
      "          1.0227e+00, -2.5120e-01,  3.6870e-01, -2.4321e-01,  3.1586e+00,\n",
      "         -2.3257e+00, -1.1646e+00, -1.4097e+00,  7.3020e-01, -1.6028e+00,\n",
      "         -3.1185e+00,  5.9540e-01,  2.1864e-01, -3.5172e+00, -1.1444e+00,\n",
      "         -8.4306e-01,  1.1800e+00, -8.7577e-01, -3.7724e+00,  2.4406e+00,\n",
      "         -4.1250e+00, -1.6616e+00, -1.6737e+00,  2.8017e+00, -1.3613e+00,\n",
      "          5.0336e-01,  3.3219e+00, -2.1909e+00, -3.0694e+00,  1.1974e+00,\n",
      "          9.7304e-01,  1.8434e+00,  2.3926e-01,  2.2560e+00, -1.3331e+00,\n",
      "         -1.8256e+00,  2.3941e+00,  1.9991e+00, -1.8125e+00, -1.2573e+00,\n",
      "         -4.1430e-01, -9.1206e-01,  2.1679e+00, -1.0121e+00, -4.3239e-01,\n",
      "          2.2385e+00, -1.9689e+00, -1.2162e+00, -4.4275e-01, -1.4251e-02,\n",
      "          3.4801e-01,  3.3441e-01, -1.9466e+00],\n",
      "        [ 1.0666e-01, -4.4749e-01,  3.0592e-03,  3.5487e+00,  7.1915e-01,\n",
      "         -1.5275e-01, -9.7117e-02, -2.9817e-01, -8.1239e-02, -9.7092e-01,\n",
      "          2.6279e-01, -7.2066e-02, -1.0161e-01, -4.9066e-01, -3.5642e-01,\n",
      "         -3.6854e-01, -4.6020e-01,  5.0026e+00,  5.7813e-01,  3.1274e-01,\n",
      "         -4.7810e-01,  2.0687e-01,  2.1394e-01, -2.1041e-01, -2.7540e-01,\n",
      "          6.5973e-01, -1.8037e-01, -8.4731e-01, -2.4666e+00, -3.1706e-02,\n",
      "         -2.1351e-01, -8.8217e-02,  4.6630e-01,  5.3028e-01,  8.1138e-02,\n",
      "         -3.4815e-01, -8.5351e-02,  2.3976e-01,  2.2374e+00, -9.5145e-02,\n",
      "          3.7266e+00,  2.1168e-01,  2.7202e-01,  4.3517e-01,  5.4033e-02,\n",
      "          6.7890e-02,  4.5308e-02, -3.9729e-01,  2.5600e+00,  4.9277e-01,\n",
      "          5.0605e-01, -3.1364e-01,  1.3167e-01, -7.4294e-01, -2.0006e-01,\n",
      "          7.5181e-01, -3.0951e-01, -6.3007e-01, -6.4775e-01,  1.0061e-01,\n",
      "          1.7763e-01, -7.7472e-01, -2.7303e-01, -6.9010e-01, -6.1744e-01,\n",
      "         -3.3275e-01,  1.1180e-01,  4.3017e-01, -5.2812e-01,  6.3297e-01,\n",
      "          1.3869e-01,  1.8332e+00,  5.1611e-01, -4.1663e-01, -1.2248e-01,\n",
      "         -7.2120e-01,  8.8940e-02, -8.7361e-01, -5.3072e-01, -5.1299e-01,\n",
      "          4.1828e-01,  4.1859e-02,  1.2541e+00,  1.2910e-01, -1.6070e+00,\n",
      "         -2.5055e-01,  2.0293e-01,  6.7893e-01, -3.0937e-01,  2.0754e-02,\n",
      "         -5.6050e+00,  2.6165e+00,  2.7026e-01, -6.5599e-01, -5.9977e-01,\n",
      "         -1.5575e-01,  6.7848e-01, -1.9105e-01,  1.0062e+00,  1.6024e+00,\n",
      "         -5.8674e-01,  1.4691e-01,  2.3959e-01, -3.4561e-01,  2.2185e-01,\n",
      "         -2.2944e-01, -5.1091e-01,  2.4504e-01, -1.2289e+00,  1.1568e-01,\n",
      "          2.3749e-01, -6.5189e-01, -1.4281e-01,  4.9357e-01, -1.0489e-01,\n",
      "         -4.2840e-02,  6.0504e-01,  7.5009e-02, -1.8288e-01, -1.7912e+00,\n",
      "         -3.2186e-01, -3.6232e-02, -2.0779e-01,  5.1928e-01,  3.7479e-01,\n",
      "          4.1132e-01, -3.7719e-01, -6.8777e-01],\n",
      "        [-7.2112e-01, -3.9909e-01, -5.9451e-01,  2.5383e+00, -2.0145e+00,\n",
      "          6.0613e-01,  3.6847e-01, -2.9590e-01,  9.0987e-01, -7.8571e-01,\n",
      "          7.4591e-01, -4.2301e+00,  1.4296e+00, -1.0402e+00,  2.6900e+00,\n",
      "          4.5009e-01, -1.1203e+00,  7.7857e-01, -1.3797e+00,  1.1300e+00,\n",
      "         -5.1332e-01, -4.8499e-01,  5.6609e-01,  4.0330e+00, -2.8951e+00,\n",
      "         -1.6638e+00, -1.6608e+00, -1.7574e+00, -2.0807e+00, -1.1936e-01,\n",
      "         -1.2634e+00, -1.6363e+00, -1.5997e+00, -3.2901e+00, -1.6680e+00,\n",
      "         -2.7372e-01,  4.6559e-01,  5.4463e+00, -2.8337e-01, -1.7683e+00,\n",
      "          6.7869e-01,  2.1238e+00,  1.5891e-01, -3.4390e-04,  1.6032e+00,\n",
      "         -1.2022e+00,  2.7277e-01,  5.1740e-01,  1.5432e+00,  3.8428e-01,\n",
      "          1.9242e-02,  2.8965e+00, -1.7932e-01,  3.3554e-01,  2.0593e+00,\n",
      "         -6.0717e-01, -1.8294e+00, -2.9545e+00,  1.2066e+00, -2.3541e+00,\n",
      "          1.7533e+00, -1.2602e+00,  2.1070e-01,  2.0741e+00, -1.9738e+00,\n",
      "          5.3876e-01,  7.5917e-01,  1.6107e+00,  1.9651e+00,  1.3642e+00,\n",
      "         -2.8806e-01,  1.7388e+00, -1.6522e-02,  2.5029e-01, -1.2816e+00,\n",
      "         -4.1292e-01, -1.2569e-01, -1.3274e+00, -1.1128e+00,  4.8661e-01,\n",
      "         -1.1139e+00, -2.6534e+00, -1.5986e+00,  2.1205e-01, -5.9854e-01,\n",
      "          1.2666e-01, -1.2050e+00, -9.7301e-01,  4.9259e-01,  5.0122e-01,\n",
      "         -5.0559e+00,  1.8880e+00, -4.3579e+00,  1.0886e+00,  1.9161e+00,\n",
      "          1.3028e-01,  2.3832e+00, -3.0725e-01,  2.1250e+00, -2.3578e+00,\n",
      "          1.9045e+00,  5.4339e-01, -2.1213e+00,  9.4435e-01, -1.7967e+00,\n",
      "          2.1092e-01, -3.0715e-01,  7.5448e-01, -5.7632e-01, -5.8077e-01,\n",
      "         -1.6826e+00,  1.8996e+00, -2.4193e+00,  1.4491e+00, -1.2426e+00,\n",
      "         -1.6369e+00, -8.9155e-01,  5.1826e-01,  2.1166e+00, -1.4683e+00,\n",
      "          5.2440e-01,  1.3618e+00, -3.4798e-01, -2.4109e+00, -2.2999e+00,\n",
      "          8.1323e-03, -1.9149e-01, -3.8314e-02]], device='cuda:0')\n",
      "Averaged embedding: torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "train_df.head(5)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['title'].tolist(),\n",
    "    targets=train_df['score'].tolist(),\n",
    "    vocab=model.vocab  # Use the model's vocabulary\n",
    ")\n",
    "\n",
    "# Create dataloaders with the custom collate function\n",
    "batch_size = 10\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=make_collate_fn(model, device)\n",
    ")\n",
    "\n",
    "item = 2\n",
    "\n",
    "# Inspect raw text inputs\n",
    "sample_text = train_df['title'].iloc[item]\n",
    "print(f\"Raw text: {sample_text}\")\n",
    "\n",
    "# Tokenize the text\n",
    "#tokens = sample_text.lower().split()\n",
    "tokens = preprocess_text(sample_text)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Convert tokens to indices using the vocabulary\n",
    "vocab = vocab.load_vocab(\"../models/word2vec/text8_vocab_NWAll_MF5.json\")\n",
    "indices = [vocab.get_index(token) for token in tokens]\n",
    "print(f\"Token indices: {indices}\")\n",
    "\n",
    "# Get embeddings for the token indices\n",
    "token_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "embeddings = model.embedding(token_tensor)\n",
    "print(f\"Embeddings: {embeddings[0]}\")\n",
    "\n",
    "# Average the embeddings\n",
    "avg_embedding = embeddings.mean(dim=1)\n",
    "print(f\"Averaged embedding: {avg_embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "cd93f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:55:04 | DropoutDisco | INFO     | [vocabulary.py:110] | Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:55:04 | DropoutDisco | INFO     | [vocabulary.py:123] | üìö Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:üìö Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'prince' and 'queen': 0.3710\n"
     ]
    }
   ],
   "source": [
    "# Load the vocabulary\n",
    "vocab = vocab.load_vocab(\"../models/word2vec/text8_vocab_NWAll_MF5.json\")\n",
    "\n",
    "# Load the CBOW model state\n",
    "cbow_state = torch.load(\"../models/word2vec/CBOW_D128_W5_NWAll_MF5_E15_LR0.001_BS512/model_state.pth\", map_location=torch.device('cpu'))\n",
    "\n",
    "# Extract the embeddings\n",
    "embeddings = cbow_state['embeddings.weight']\n",
    "\n",
    "# Define a function to compute cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return torch.dot(vec1, vec2) / (torch.norm(vec1) * torch.norm(vec2))\n",
    "\n",
    "# Check similarity between words\n",
    "def check_word_similarity(word1, word2, vocab, embeddings):\n",
    "    idx1 = vocab.get_index(word1)\n",
    "    idx2 = vocab.get_index(word2)\n",
    "    vec1 = embeddings[idx1]\n",
    "    vec2 = embeddings[idx2]\n",
    "    similarity = cosine_similarity(vec1, vec2)\n",
    "    return similarity.item()\n",
    "\n",
    "# Example: Check similarity between two words\n",
    "word1 = \"prince\"\n",
    "word2 = \"queen\"\n",
    "similarity = check_word_similarity(word1, word2, vocab, embeddings)\n",
    "print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a9049f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the two sentences: 0.3710\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Compare the mean embeddings of two user-input sentences\n",
    "sentence1 = \"prince\"\n",
    "sentence2 = \"queen\"\n",
    "\n",
    "# Preprocess and tokenize the sentences\n",
    "tokens1 = preprocess_text(sentence1)\n",
    "tokens2 = preprocess_text(sentence2)\n",
    "\n",
    "# Convert tokens to indices using the vocabulary\n",
    "indices1 = [vocab.get_index(token) for token in tokens1]\n",
    "indices2 = [vocab.get_index(token) for token in tokens2]\n",
    "\n",
    "# Get embeddings for the token indices\n",
    "token_tensor1 = torch.tensor(indices1, dtype=torch.long).unsqueeze(0).to(device)\n",
    "token_tensor2 = torch.tensor(indices2, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "embeddings1 = model.embedding(token_tensor1)\n",
    "embeddings2 = model.embedding(token_tensor2)\n",
    "\n",
    "# Average the embeddings\n",
    "avg_embedding1 = embeddings1.mean(dim=1)\n",
    "avg_embedding2 = embeddings2.mean(dim=1)\n",
    "\n",
    "# Print the averaged embeddings\n",
    "#print(f\"Averaged embedding for sentence 1: {avg_embedding1}\")\n",
    "#print(f\"Averaged embedding for sentence 2: {avg_embedding2}\")\n",
    "\n",
    "# Compute cosine similarity between the two averaged embeddings\n",
    "cos_sim = cosine_similarity(avg_embedding1.squeeze(), avg_embedding2.squeeze())\n",
    "print(f\"Cosine similarity between the two sentences: {cos_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model state pth\n",
    "model_path = \"../models/text_regression_model.pth\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
