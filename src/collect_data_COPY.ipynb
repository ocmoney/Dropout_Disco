{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "254f8697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:07:29</td>\n",
       "      <td>The physics of entropy and the origin of life ...</td>\n",
       "      <td>https://www.youtube.com/watch?v=Sz1n0RHwLqA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 00:08:15</td>\n",
       "      <td>Xfinity Stream on Linux: A Tale of Widevine, C...</td>\n",
       "      <td>https://thebrokenrail.com/2022/12/31/xfinity-s...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 00:12:29</td>\n",
       "      <td>Let’s try ChatGPT. Is it any good? (Bisqwit)</td>\n",
       "      <td>https://www.youtube.com/watch?v=q2A-MkGjvmI</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 00:14:17</td>\n",
       "      <td>Because Internet: Understanding the New Rules ...</td>\n",
       "      <td>https://www.amazon.com/Because-Internet-Unders...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 00:14:24</td>\n",
       "      <td>Solar thermal storage using lunar regolith</td>\n",
       "      <td>https://www.esa.int/Enabling_Support/Preparing...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514994</th>\n",
       "      <td>2024-02-17 12:43:45</td>\n",
       "      <td>Show HN: Simple 1-Rep Max Calculator for Stren...</td>\n",
       "      <td>https://calcolomassimale.it/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514995</th>\n",
       "      <td>2024-02-17 12:44:34</td>\n",
       "      <td>Visualize Latent Spaces</td>\n",
       "      <td>https://github.com/enjalot/latent-scope</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514996</th>\n",
       "      <td>2024-02-17 12:44:52</td>\n",
       "      <td>You don't need LPM Tries (2023)</td>\n",
       "      <td>https://cookie.engineer/weblog/articles/you-do...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514997</th>\n",
       "      <td>2024-02-17 12:45:49</td>\n",
       "      <td>Show HN: ANXRacers – A 2D top-down time-attack...</td>\n",
       "      <td>https://studios.aeonax.com/racers/</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514998</th>\n",
       "      <td>2024-02-17 12:48:04</td>\n",
       "      <td>New World: Upgraded Combat and Animation Syste...</td>\n",
       "      <td>https://www.newworld.com/en-us/news/articles/u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>514999 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      time                                              title  \\\n",
       "0      2023-01-01 00:07:29  The physics of entropy and the origin of life ...   \n",
       "1      2023-01-01 00:08:15  Xfinity Stream on Linux: A Tale of Widevine, C...   \n",
       "2      2023-01-01 00:12:29       Let’s try ChatGPT. Is it any good? (Bisqwit)   \n",
       "3      2023-01-01 00:14:17  Because Internet: Understanding the New Rules ...   \n",
       "4      2023-01-01 00:14:24         Solar thermal storage using lunar regolith   \n",
       "...                    ...                                                ...   \n",
       "514994 2024-02-17 12:43:45  Show HN: Simple 1-Rep Max Calculator for Stren...   \n",
       "514995 2024-02-17 12:44:34                            Visualize Latent Spaces   \n",
       "514996 2024-02-17 12:44:52                    You don't need LPM Tries (2023)   \n",
       "514997 2024-02-17 12:45:49  Show HN: ANXRacers – A 2D top-down time-attack...   \n",
       "514998 2024-02-17 12:48:04  New World: Upgraded Combat and Animation Syste...   \n",
       "\n",
       "                                                      url  score  \n",
       "0             https://www.youtube.com/watch?v=Sz1n0RHwLqA      5  \n",
       "1       https://thebrokenrail.com/2022/12/31/xfinity-s...    144  \n",
       "2             https://www.youtube.com/watch?v=q2A-MkGjvmI      4  \n",
       "3       https://www.amazon.com/Because-Internet-Unders...      2  \n",
       "4       https://www.esa.int/Enabling_Support/Preparing...      2  \n",
       "...                                                   ...    ...  \n",
       "514994                       https://calcolomassimale.it/      1  \n",
       "514995            https://github.com/enjalot/latent-scope    116  \n",
       "514996  https://cookie.engineer/weblog/articles/you-do...      2  \n",
       "514997                 https://studios.aeonax.com/racers/      2  \n",
       "514998  https://www.newworld.com/en-us/news/articles/u...      1  \n",
       "\n",
       "[514999 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the database URI directly\n",
    "# !! In real projects, manage credentials securely (e.g., env variables, secrets manager) !!\n",
    "DB_URI = \"postgresql://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\"\n",
    "\n",
    "engine = create_engine(DB_URI)\n",
    "# --- Optional: Set up logging ---\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# Example: Show tables (PostgreSQL metadata)\n",
    "res = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        time\n",
    "        , title\n",
    "        , url\n",
    "        , score  \n",
    "    FROM \"hacker_news\".\"items\" a\n",
    "    WHERE a.type = 'story'\n",
    "        AND a.time >= '2023-01-01 00:00:00'\n",
    "        AND a.dead IS NOT TRUE\n",
    "        AND LENGTH(a.title) > 0\n",
    "        AND url IS NOT NULL\n",
    "\"\"\", engine)\n",
    "\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a76ff6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The physics of entropy and the origin of life ...</td>\n",
       "      <td>https://www.youtube.com/watch?v=Sz1n0RHwLqA</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xfinity Stream on Linux: A Tale of Widevine, C...</td>\n",
       "      <td>https://thebrokenrail.com/2022/12/31/xfinity-s...</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Let’s try ChatGPT. Is it any good? (Bisqwit)</td>\n",
       "      <td>https://www.youtube.com/watch?v=q2A-MkGjvmI</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Because Internet: Understanding the New Rules ...</td>\n",
       "      <td>https://www.amazon.com/Because-Internet-Unders...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Solar thermal storage using lunar regolith</td>\n",
       "      <td>https://www.esa.int/Enabling_Support/Preparing...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The craft of SwiftUI API design: Progressive d...</td>\n",
       "      <td>https://developer.apple.com/videos/play/wwdc20...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Worst interview questions for software developers</td>\n",
       "      <td>https://fibery.io/blog/worst-interview-questio...</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Running Advent of Code on a $2 microcontroller</td>\n",
       "      <td>https://medium.com/@erik_68861/running-advent-...</td>\n",
       "      <td>90</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OpenBSD KDE Status Report 2022</td>\n",
       "      <td>https://www.sizeofvoid.org/posts/2022-26-12-op...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Australia mandates Covid tests for Chinese tou...</td>\n",
       "      <td>https://www.smh.com.au/national/covid-test-to-...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Position with the most possible checkmates in 1</td>\n",
       "      <td>https://www.chess.com/forum/view/general/what-...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>As a Boy in Rural Mexico, His Life Was Changed...</td>\n",
       "      <td>https://now.tufts.edu/2021/09/28/boy-rural-mex...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Alt-Right Manipulated My Comic. Then AI Cl...</td>\n",
       "      <td>https://www.nytimes.com/2022/12/31/opinion/sar...</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SWA meltdown: technical debt from short term f...</td>\n",
       "      <td>https://www.facebook.com/1315600950/posts/what...</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Defining “Enough”</td>\n",
       "      <td>https://www.youngmoney.co/p/defining-enough</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A Particle That May Fill ‘Empty’ Space</td>\n",
       "      <td>https://www.wsj.com/articles/a-particle-that-m...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AMD Continues Working Toward HDR Display Suppo...</td>\n",
       "      <td>https://www.phoronix.com/news/AMD-2022-Linux-H...</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Serverless data pipelines for Data Engineers, ...</td>\n",
       "      <td>https://typhoondata.io/</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Four-year-old Poppy becomes trapped in skill t...</td>\n",
       "      <td>https://7news.com.au/lifestyle/parenting/four-...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>My Thoughts about Editors in 2022</td>\n",
       "      <td>https://phaazon.net/blog/editors-in-2022</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Competitive programming in Haskell: better bin...</td>\n",
       "      <td>https://byorgey.wordpress.com/2023/01/01/compe...</td>\n",
       "      <td>131</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The Computer Made Me Do It – ChatGPT Writes an...</td>\n",
       "      <td>https://www.youtube.com/watch?v=COeIxQt5fXM</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Alan MacMasters: How the great online toaster ...</td>\n",
       "      <td>https://www.bbc.com/news/the-reporters-63622746</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Matrix Community Year in Review 2022</td>\n",
       "      <td>https://blog.neko.dev/posts/matrix-year-in-rev...</td>\n",
       "      <td>86</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The Third Magic</td>\n",
       "      <td>https://noahpinion.substack.com/p/the-third-magic</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Working on Composite Video Output for the MEGA...</td>\n",
       "      <td>https://c65gs.blogspot.com/2023/01/working-on-...</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Greatest innovations of 2022: The 35th annual ...</td>\n",
       "      <td>https://www.popsci.com/technology/best-of-what...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>The Lisa Computer: A Retrospective [pdf]</td>\n",
       "      <td>http://bitsavers.org/pdf/apple/lisa/developmen...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Ray Tracer Construction Kit</td>\n",
       "      <td>https://matklad.github.io/2022/12/31/raytracer...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Tipp10 – Free Touch Typing Tutor</td>\n",
       "      <td>https://www.tipp10.com/en/index/</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Investing for a World Transformed by AI</td>\n",
       "      <td>https://www.lesswrong.com/posts/jvHLBEXXEtZtt4...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Typesafe DB Like Prisma for Rust</td>\n",
       "      <td>https://users.rust-lang.org/t/typesafe-db-like...</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Meta set to make divisive decision on Trump’s ...</td>\n",
       "      <td>https://www.ft.com/content/9ba505e2-b4fb-48fb-...</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MilkyTracker: Open-source, multi-platform appl...</td>\n",
       "      <td>https://milkytracker.org/about/</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>As Big Tech retrenches, a tech talent shift ac...</td>\n",
       "      <td>https://www.nytimes.com/2022/12/29/business/si...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>The Dark Risk of Large Language Models</td>\n",
       "      <td>https://www.wired.com/story/large-language-mod...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Twitter Search Cheat Sheet (2018)</td>\n",
       "      <td>https://twitter.com/osintstash/status/10753972...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Show HN: I created my AI clone based on 600.00...</td>\n",
       "      <td>https://twitter.com/louis030195/status/1609487...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Show HN: Quick Steps in Lambdacalc</td>\n",
       "      <td>http://lambdaway.free.fr/lambdawalks/?view=lam...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>When Place of Birth Is at Sea</td>\n",
       "      <td>https://apl.wisc.edu/shared/tad/sea-birth</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>How to Go Fast</td>\n",
       "      <td>https://quii.dev/How_to_go_fast</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>The Eureka Theory of History Is Wrong</td>\n",
       "      <td>https://www.theatlantic.com/magazine/archive/2...</td>\n",
       "      <td>51</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Ultimate guide to creating disks for retro com...</td>\n",
       "      <td>https://www.youtube.com/watch?v=fRZVlsxSDw0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Wikimedia's Abstract Wikipedia project “at sub...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Wikipe...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>How and why to properly write copyright statem...</td>\n",
       "      <td>https://matija.suklje.name/how-and-why-to-prop...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>The state of the art of designing electroacous...</td>\n",
       "      <td>https://econtact.ca/12_3/applebaum_soundsculpt...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Crypto trading using reinforcement learning</td>\n",
       "      <td>https://arxiv.org/abs/2209.05559</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Mastodon Roadmap</td>\n",
       "      <td>https://joinmastodon.org/roadmap</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Intrusion Detection Systems (IDS)</td>\n",
       "      <td>https://zouhairj.com/notes/intrusion-detection...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>A Fake British Accent Took Old Hollywood by Storm</td>\n",
       "      <td>https://www.atlasobscura.com/articles/how-a-fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   The physics of entropy and the origin of life ...   \n",
       "1   Xfinity Stream on Linux: A Tale of Widevine, C...   \n",
       "2        Let’s try ChatGPT. Is it any good? (Bisqwit)   \n",
       "3   Because Internet: Understanding the New Rules ...   \n",
       "4          Solar thermal storage using lunar regolith   \n",
       "5   The craft of SwiftUI API design: Progressive d...   \n",
       "6   Worst interview questions for software developers   \n",
       "7      Running Advent of Code on a $2 microcontroller   \n",
       "8                      OpenBSD KDE Status Report 2022   \n",
       "9   Australia mandates Covid tests for Chinese tou...   \n",
       "10    Position with the most possible checkmates in 1   \n",
       "11  As a Boy in Rural Mexico, His Life Was Changed...   \n",
       "12  The Alt-Right Manipulated My Comic. Then AI Cl...   \n",
       "13  SWA meltdown: technical debt from short term f...   \n",
       "14                                  Defining “Enough”   \n",
       "15             A Particle That May Fill ‘Empty’ Space   \n",
       "16  AMD Continues Working Toward HDR Display Suppo...   \n",
       "17  Serverless data pipelines for Data Engineers, ...   \n",
       "18  Four-year-old Poppy becomes trapped in skill t...   \n",
       "19                  My Thoughts about Editors in 2022   \n",
       "20  Competitive programming in Haskell: better bin...   \n",
       "21  The Computer Made Me Do It – ChatGPT Writes an...   \n",
       "22  Alan MacMasters: How the great online toaster ...   \n",
       "23               Matrix Community Year in Review 2022   \n",
       "24                                    The Third Magic   \n",
       "25  Working on Composite Video Output for the MEGA...   \n",
       "26  Greatest innovations of 2022: The 35th annual ...   \n",
       "27           The Lisa Computer: A Retrospective [pdf]   \n",
       "28                        Ray Tracer Construction Kit   \n",
       "29                   Tipp10 – Free Touch Typing Tutor   \n",
       "30            Investing for a World Transformed by AI   \n",
       "31                   Typesafe DB Like Prisma for Rust   \n",
       "32  Meta set to make divisive decision on Trump’s ...   \n",
       "33  MilkyTracker: Open-source, multi-platform appl...   \n",
       "34  As Big Tech retrenches, a tech talent shift ac...   \n",
       "35             The Dark Risk of Large Language Models   \n",
       "36                  Twitter Search Cheat Sheet (2018)   \n",
       "37  Show HN: I created my AI clone based on 600.00...   \n",
       "38                 Show HN: Quick Steps in Lambdacalc   \n",
       "39                      When Place of Birth Is at Sea   \n",
       "40                                     How to Go Fast   \n",
       "41              The Eureka Theory of History Is Wrong   \n",
       "42  Ultimate guide to creating disks for retro com...   \n",
       "43  Wikimedia's Abstract Wikipedia project “at sub...   \n",
       "44  How and why to properly write copyright statem...   \n",
       "45  The state of the art of designing electroacous...   \n",
       "46        Crypto trading using reinforcement learning   \n",
       "47                                   Mastodon Roadmap   \n",
       "48                  Intrusion Detection Systems (IDS)   \n",
       "49  A Fake British Accent Took Old Hollywood by Storm   \n",
       "\n",
       "                                                  url  score  hour  \n",
       "0         https://www.youtube.com/watch?v=Sz1n0RHwLqA      5     0  \n",
       "1   https://thebrokenrail.com/2022/12/31/xfinity-s...    144     0  \n",
       "2         https://www.youtube.com/watch?v=q2A-MkGjvmI      4     0  \n",
       "3   https://www.amazon.com/Because-Internet-Unders...      2     0  \n",
       "4   https://www.esa.int/Enabling_Support/Preparing...      2     0  \n",
       "5   https://developer.apple.com/videos/play/wwdc20...      4     0  \n",
       "6   https://fibery.io/blog/worst-interview-questio...    154     0  \n",
       "7   https://medium.com/@erik_68861/running-advent-...     90     7  \n",
       "8   https://www.sizeofvoid.org/posts/2022-26-12-op...      5     7  \n",
       "9   https://www.smh.com.au/national/covid-test-to-...      6     7  \n",
       "10  https://www.chess.com/forum/view/general/what-...      3     7  \n",
       "11  https://now.tufts.edu/2021/09/28/boy-rural-mex...      3     7  \n",
       "12  https://www.nytimes.com/2022/12/31/opinion/sar...      4     7  \n",
       "13  https://www.facebook.com/1315600950/posts/what...     10     7  \n",
       "14        https://www.youngmoney.co/p/defining-enough      3     7  \n",
       "15  https://www.wsj.com/articles/a-particle-that-m...      5     8  \n",
       "16  https://www.phoronix.com/news/AMD-2022-Linux-H...     13     8  \n",
       "17                            https://typhoondata.io/      3     8  \n",
       "18  https://7news.com.au/lifestyle/parenting/four-...      2     8  \n",
       "19           https://phaazon.net/blog/editors-in-2022      5     8  \n",
       "20  https://byorgey.wordpress.com/2023/01/01/compe...    131     8  \n",
       "21        https://www.youtube.com/watch?v=COeIxQt5fXM      2     8  \n",
       "22    https://www.bbc.com/news/the-reporters-63622746     11     8  \n",
       "23  https://blog.neko.dev/posts/matrix-year-in-rev...     86     8  \n",
       "24  https://noahpinion.substack.com/p/the-third-magic      4     8  \n",
       "25  https://c65gs.blogspot.com/2023/01/working-on-...      6     8  \n",
       "26  https://www.popsci.com/technology/best-of-what...      4     8  \n",
       "27  http://bitsavers.org/pdf/apple/lisa/developmen...      3     8  \n",
       "28  https://matklad.github.io/2022/12/31/raytracer...      2     8  \n",
       "29                   https://www.tipp10.com/en/index/      2     9  \n",
       "30  https://www.lesswrong.com/posts/jvHLBEXXEtZtt4...      1     9  \n",
       "31  https://users.rust-lang.org/t/typesafe-db-like...      4     9  \n",
       "32  https://www.ft.com/content/9ba505e2-b4fb-48fb-...      4     9  \n",
       "33                    https://milkytracker.org/about/      2     9  \n",
       "34  https://www.nytimes.com/2022/12/29/business/si...      2     9  \n",
       "35  https://www.wired.com/story/large-language-mod...      2     9  \n",
       "36  https://twitter.com/osintstash/status/10753972...      1     9  \n",
       "37  https://twitter.com/louis030195/status/1609487...      3     9  \n",
       "38  http://lambdaway.free.fr/lambdawalks/?view=lam...      2     9  \n",
       "39          https://apl.wisc.edu/shared/tad/sea-birth      4     9  \n",
       "40                    https://quii.dev/How_to_go_fast      2    10  \n",
       "41  https://www.theatlantic.com/magazine/archive/2...     51    10  \n",
       "42        https://www.youtube.com/watch?v=fRZVlsxSDw0      1    10  \n",
       "43  https://en.wikipedia.org/wiki/Wikipedia:Wikipe...      4    10  \n",
       "44  https://matija.suklje.name/how-and-why-to-prop...     14    10  \n",
       "45  https://econtact.ca/12_3/applebaum_soundsculpt...      1    10  \n",
       "46                   https://arxiv.org/abs/2209.05559      2    10  \n",
       "47                   https://joinmastodon.org/roadmap      4    10  \n",
       "48  https://zouhairj.com/notes/intrusion-detection...      1    10  \n",
       "49  https://www.atlasobscura.com/articles/how-a-fa...      2    10  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = res.copy()\n",
    "# convert time into hour of day\n",
    "data['hour'] = data['time'].dt.hour\n",
    "data.drop(columns=['time'], inplace=True)\n",
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fee70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from utils import logger  # Import the `logger` module from `utils`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f553a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from word2vec.vocabulary import Vocabulary as vocab\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class TextUrlRegressionModel(nn.Module):\n",
    "    def __init__(self, vocab_path, cbow_model_path, input_dim, hidden_dims=[128, 64, 32], dropout=0.2, domain_embedding_dim=32):\n",
    "        \"\"\"\n",
    "        Combines vocabulary, CBOW embeddings, and MLP regression model.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the saved vocabulary JSON.\n",
    "            cbow_model_path (str): Path to the saved CBOW model state.\n",
    "            input_dim (int): Dimension of the input embeddings.\n",
    "            hidden_dims (List[int]): List of hidden layer dimensions.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Load vocabulary\n",
    "        self.vocab = vocab.load_vocab(vocab_path)\n",
    "        \n",
    "        # Load CBOW model and extract embedding layer\n",
    "        cbow_state = torch.load(cbow_model_path, map_location=torch.device('cpu'))\n",
    "        self.embedding = nn.Embedding.from_pretrained(cbow_state['embeddings.weight'])\n",
    "\n",
    "        # domain handling\n",
    "        self.domain_embedding_dim = domain_embedding_dim\n",
    "        self.domain_dict = {} # will map domains to indices\n",
    "        self.unknown_domain_idx = 0 # index for unknown domain\n",
    "        self.next_domain_idx = 1 # index for next domain to be added        \n",
    "\n",
    "        # Initialize domain embedding layer\n",
    "        self.domain_embedding = nn.Embedding(1, domain_embedding_dim)\n",
    "        \n",
    "        # Initialize MLP layers\n",
    "        layers = []\n",
    "        combined_dim = input_dim + domain_embedding_dim  # Combined dimension for text + domain\n",
    "        prev_dim = combined_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Add final output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        # Combine all layers\n",
    "        self.regression_model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def extract_domain(self, url):\n",
    "        \"\"\"Extract base domain from URL using urlparse.\"\"\"\n",
    "        if not url or not isinstance(url, str):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Parse the URL\n",
    "            parsed_url = urlparse(url)\n",
    "            \n",
    "            # Get the netloc part (e.g., 'www.example.com:80')\n",
    "            netloc = parsed_url.netloc\n",
    "            \n",
    "            # If netloc is empty (might happen with malformed URLs), try path\n",
    "            if not netloc and parsed_url.path:\n",
    "                netloc = parsed_url.path.split('/')[0]\n",
    "                \n",
    "            # Remove port number if present\n",
    "            netloc = netloc.split(':')[0]\n",
    "            return netloc\n",
    "        except:\n",
    "            # Handle any parsing errors\n",
    "            logger.error(f\"Error parsing URL: {url}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def get_domain_index(self, url, update=False):\n",
    "        \"\"\"Convert URL to domain index.\"\"\"\n",
    "        domain = self.extract_domain(url)\n",
    "        \n",
    "        if not domain:\n",
    "            return self.unknown_domain_idx\n",
    "            \n",
    "        if domain not in self.domain_dict:\n",
    "            if update:\n",
    "                self.domain_dict[domain] = self.next_domain_idx\n",
    "                self.next_domain_idx += 1\n",
    "                # Expand embedding layer if needed\n",
    "                if self.next_domain_idx > self.domain_embedding.num_embeddings:\n",
    "                    old_embedding = self.domain_embedding\n",
    "                    new_size = max(self.next_domain_idx * 2, 100)  # Double size or at least 100\n",
    "                    self.domain_embedding = nn.Embedding(new_size, self.domain_embedding_dim)\n",
    "                    self.domain_embedding = self.domain_embedding.to(next(self.parameters()).device)\n",
    "                    # Copy existing embeddings\n",
    "                    with torch.no_grad():\n",
    "                        self.domain_embedding.weight[:old_embedding.num_embeddings] = old_embedding.weight\n",
    "            else:\n",
    "                return self.unknown_domain_idx\n",
    "                \n",
    "        return self.domain_dict[domain]\n",
    "\n",
    "\n",
    "    def initialize_domain_dict(self, urls):\n",
    "        \"\"\"Initialize domain dictionary from a list of URLs.\"\"\"\n",
    "        for url in urls:\n",
    "            self.get_domain_index(url, update=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, text_embeddings, urls):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "        Args:\n",
    "            text_embeddings (Tensor): Averaged embeddings of shape (batch_size, input_dim).\n",
    "            urls (List[str]): List of URLs for the batch.\n",
    "        Returns:\n",
    "            Tensor: Predicted regression values.\n",
    "        \"\"\"\n",
    "        batch_size = text_embeddings.shape[0]\n",
    "        device = text_embeddings.device\n",
    "        \n",
    "        # Get domain indices for each URL\n",
    "        domain_indices = [self.get_domain_index(url) for url in urls]\n",
    "        domain_indices_tensor = torch.tensor(domain_indices, dtype=torch.long).to(device)\n",
    "        \n",
    "        # Get domain embeddings\n",
    "        domain_embeddings = self.domain_embedding(domain_indices_tensor)\n",
    "        \n",
    "        # Combine text and domain embeddings\n",
    "        combined_embeddings = torch.cat([text_embeddings, domain_embeddings], dim=1)\n",
    "        \n",
    "        # Pass through regression model\n",
    "        return self.regression_model(combined_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff21790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, urls, targets, vocab):\n",
    "        \"\"\"\n",
    "        Custom Dataset for text regression.\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): List of input texts.\n",
    "            targets (List[float]): List of target regression values.\n",
    "            vocab (Vocabulary): Vocabulary object for tokenization.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.urls = urls\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        import re\n",
    "        # Replace any non-alphanumeric character with a space\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "        # Convert to lowercase and split into tokens\n",
    "        tokens = text.lower().split()\n",
    "        return tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        url = self.urls[idx] if self.urls is not None else None\n",
    "        target = self.targets[idx]\n",
    "        # Preprocess the text\n",
    "        tokens = self.preprocess_text(text)\n",
    "        # Convert tokens to indices using the vocabulary\n",
    "        indices = [self.vocab.get_index(token) for token in tokens]\n",
    "        return torch.tensor(indices, dtype=torch.long), url, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "785161a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_collate_fn(model, device):\n",
    "    def collate_fn(batch):\n",
    "        sequences, urls, targets = zip(*batch)\n",
    "        targets = torch.stack(targets).to(device)\n",
    "\n",
    "        embedded_sequences = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) == 0:  # Handle empty sequences\n",
    "                embedded_sequences.append(torch.zeros(model.embedding.embedding_dim).to(device))\n",
    "            else:\n",
    "                embeddings = model.embedding(seq.to(device))\n",
    "                avg_embedding = embeddings.mean(dim=0)\n",
    "                embedded_sequences.append(avg_embedding)\n",
    "\n",
    "        embedded_batch = torch.stack(embedded_sequences).to(device)\n",
    "        return embedded_batch, urls, targets\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2bc353d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for text_inputs, urls, targets in dataloader:\n",
    "        text_inputs, targets = text_inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass with both text and URLs\n",
    "        outputs = model(text_inputs, urls)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text_inputs, urls, targets in dataloader:\n",
    "            text_inputs, targets = text_inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(text_inputs, urls)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Reverse log1p transform\n",
    "            predictions.extend(np.expm1(outputs.squeeze().cpu().numpy()))\n",
    "            actuals.extend(np.expm1(targets.cpu().numpy()))\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, predictions, actuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf2b9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d614ab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "2025-04-18 14:53:44 | DropoutDisco | INFO     | [vocabulary.py:110] | Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-18 14:53:44 | DropoutDisco | INFO     | [vocabulary.py:123] | 📚 Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:📚 Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 0.9365\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Detect the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = TextUrlRegressionModel(\n",
    "    vocab_path=\"../models/word2vec/text8_vocab_NWAll_MF5.json\",\n",
    "    cbow_model_path=\"../models/word2vec/CBOW_D128_W5_NWAll_MF5_E15_LR0.001_BS512/model_state.pth\",\n",
    "    input_dim=128,\n",
    "    hidden_dims=[96, 64, 32, 16],\n",
    "    domain_embedding_dim=32\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# initialize domain dictionary with training URLs\n",
    "model.initialize_domain_dict(train_df['url'].tolist())\n",
    "\n",
    "# Apply log scaling to the scores in the training and testing datasets\n",
    "train_df['score'] = train_df['score'].apply(lambda x: np.log1p(x))  # log1p ensures log(0) is handled\n",
    "test_df['score'] = test_df['score'].apply(lambda x: np.log1p(x))\n",
    "\n",
    "\n",
    "# Create datasets including URLs\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['title'].tolist(),\n",
    "    urls=train_df['url'].tolist(),\n",
    "    targets=train_df['score'].tolist(),\n",
    "    vocab=model.vocab\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    texts=test_df['title'].tolist(),\n",
    "    urls=test_df['url'].tolist(),\n",
    "    targets=test_df['score'].tolist(),\n",
    "    vocab=model.vocab\n",
    ")\n",
    "\n",
    "# Create dataloaders with the custom collate function\n",
    "batch_size = 2048\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=make_collate_fn(model, device)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=make_collate_fn(model, device)\n",
    ")\n",
    "\n",
    "\n",
    "# Simplified training loop using train_model function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "d1597ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'regression_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55f92657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 entries with the highest scores:\n",
      "                                                    title     score\n",
      "281486                OpenAI's board has fired Sam Altman  8.649974\n",
      "140506                Apollo will close down on June 30th  8.137688\n",
      "427865                                             GPT-4o  8.051341\n",
      "363454                                            Airfoil  7.841493\n",
      "109675  Google “We have no moat, and neither does OpenAI”  7.805882\n",
      "285829                   Reflecting on 18 Years at Google  7.702104\n",
      "307641            Figma and Adobe abandon proposed merger  7.697121\n",
      "513519       Bypassing airport security via SQL injection  7.602900\n",
      "285557  We have reached an agreement in principle for ...  7.590852\n",
      "315983                  In 2024, please switch to Firefox  7.482119\n"
     ]
    }
   ],
   "source": [
    "# find the 10 entries with the highest scores\n",
    "top_10_entries = test_df.nlargest(10, 'score')\n",
    "# Print the top 10 entries\n",
    "print(\"Top 10 entries with the highest scores:\")\n",
    "print(top_10_entries[['title', 'score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0daf1567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 0.7659\n",
      "Epoch 2/2, Loss: 0.7484\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e0f1296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7859\n",
      "MSE: 6249.0697\n",
      "RMSE: 79.0511\n",
      "MAE: 19.1072\n",
      "R2 Score: -0.0493\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, predictions, actuals = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "# Calculate error metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(actuals, predictions)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'MSE: {mse:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'R2 Score: {r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837eb34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual input and prediction: \n",
      "\n",
      "Text: test sentence with a random website\n",
      "URL: https://example.com\n",
      "Score: 4.766228419170504\n",
      "\n",
      "Iterating through a few samples from the test df: \n",
      "\n",
      "Text: Three class society\n",
      "URL: https://geohot.github.io//blog/jekyll/update/2023/10/03/three-class-society.html\n",
      "Predicted Score: 1.1060\n",
      "\n",
      "Text: Google Slashes Most Jobs at Area 120 Incubator as Part of Cuts\n",
      "URL: https://www.bloomberg.com/news/articles/2023-01-20/google-slashes-most-jobs-at-incubator-area-120-as-part-of-cuts\n",
      "Predicted Score: 3.2320\n",
      "\n",
      "Text: Show HN: Supabase Admin Dashboard\n",
      "URL: https://uibakery.io/templates/supabase-admin\n",
      "Predicted Score: 14.6956\n",
      "\n",
      "Text: Artificial intelligence in drug discovery: what is realistic, what are illusions\n",
      "URL: https://www.sciencedirect.com/science/article/pii/S1359644620305274\n",
      "Predicted Score: 2.0266\n",
      "\n",
      "Text: The settlers brought the lottery to America. It's had a long, uneven history\n",
      "URL: https://text.npr.org/1192893936\n",
      "Predicted Score: 2.5163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_score(model, text, url, device):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    tokens = TextDataset.preprocess_text(text)\n",
    "    indices = [model.vocab.get_index(token) for token in tokens]\n",
    "    token_tensor = torch.tensor(indices, dtype=torch.long).to(device)\n",
    "\n",
    "    # Get the embedding for each token\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.embedding(token_tensor)  # shape: (seq_len, embedding_dim)\n",
    "        averaged_embedding = embeddings.mean(dim=0).unsqueeze(0)  # shape: (1, embedding_dim)\n",
    "\n",
    "        prediction = model(averaged_embedding, [url])  # Note: [url] to match batch size 1\n",
    "        return np.expm1(prediction.item())\n",
    "\n",
    "\n",
    "print(\"Manual input and prediction: \\n\")\n",
    "# fix the above issue\n",
    "print(\"Text: test sentence with a random website\")\n",
    "print(\"URL: https://example.com\")\n",
    "print(\"Score: \" + str(predict_score(model, \"test sentence with a random website\", \"https://google.com\", device)))\n",
    "\n",
    "print(\"\\nIterating through a few samples from the test df: \\n\")\n",
    "\n",
    "# iterate through a few samples with urls, showing samples, urls, and predictions\n",
    "for i in range(10,15):\n",
    "    text = test_df.iloc[i]['title']\n",
    "    url = test_df.iloc[i]['url']\n",
    "    prediction = predict_score(model, text, url, device)\n",
    "    print(f\"Text: {text}\\nURL: {url}\\nPredicted Score: {prediction:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bf5e6d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The physics of entropy and the origin of life ...</td>\n",
       "      <td>https://www.youtube.com/watch?v=Sz1n0RHwLqA</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xfinity Stream on Linux: A Tale of Widevine, C...</td>\n",
       "      <td>https://thebrokenrail.com/2022/12/31/xfinity-s...</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Let’s try ChatGPT. Is it any good? (Bisqwit)</td>\n",
       "      <td>https://www.youtube.com/watch?v=q2A-MkGjvmI</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Because Internet: Understanding the New Rules ...</td>\n",
       "      <td>https://www.amazon.com/Because-Internet-Unders...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Solar thermal storage using lunar regolith</td>\n",
       "      <td>https://www.esa.int/Enabling_Support/Preparing...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514994</th>\n",
       "      <td>Show HN: Simple 1-Rep Max Calculator for Stren...</td>\n",
       "      <td>https://calcolomassimale.it/</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514995</th>\n",
       "      <td>Visualize Latent Spaces</td>\n",
       "      <td>https://github.com/enjalot/latent-scope</td>\n",
       "      <td>116</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514996</th>\n",
       "      <td>You don't need LPM Tries (2023)</td>\n",
       "      <td>https://cookie.engineer/weblog/articles/you-do...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514997</th>\n",
       "      <td>Show HN: ANXRacers – A 2D top-down time-attack...</td>\n",
       "      <td>https://studios.aeonax.com/racers/</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514998</th>\n",
       "      <td>New World: Upgraded Combat and Animation Syste...</td>\n",
       "      <td>https://www.newworld.com/en-us/news/articles/u...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>514999 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "0       The physics of entropy and the origin of life ...   \n",
       "1       Xfinity Stream on Linux: A Tale of Widevine, C...   \n",
       "2            Let’s try ChatGPT. Is it any good? (Bisqwit)   \n",
       "3       Because Internet: Understanding the New Rules ...   \n",
       "4              Solar thermal storage using lunar regolith   \n",
       "...                                                   ...   \n",
       "514994  Show HN: Simple 1-Rep Max Calculator for Stren...   \n",
       "514995                            Visualize Latent Spaces   \n",
       "514996                    You don't need LPM Tries (2023)   \n",
       "514997  Show HN: ANXRacers – A 2D top-down time-attack...   \n",
       "514998  New World: Upgraded Combat and Animation Syste...   \n",
       "\n",
       "                                                      url  score  hour  \n",
       "0             https://www.youtube.com/watch?v=Sz1n0RHwLqA      5     0  \n",
       "1       https://thebrokenrail.com/2022/12/31/xfinity-s...    144     0  \n",
       "2             https://www.youtube.com/watch?v=q2A-MkGjvmI      4     0  \n",
       "3       https://www.amazon.com/Because-Internet-Unders...      2     0  \n",
       "4       https://www.esa.int/Enabling_Support/Preparing...      2     0  \n",
       "...                                                   ...    ...   ...  \n",
       "514994                       https://calcolomassimale.it/      1    12  \n",
       "514995            https://github.com/enjalot/latent-scope    116    12  \n",
       "514996  https://cookie.engineer/weblog/articles/you-do...      2    12  \n",
       "514997                 https://studios.aeonax.com/racers/      2    12  \n",
       "514998  https://www.newworld.com/en-us/news/articles/u...      1    12  \n",
       "\n",
       "[514999 rows x 4 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f48418cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words with the highest scores:\n",
      "besieges: 12.070055859253058\n",
      "fbi: 9.683543925330543\n",
      "humiliated: 9.279914603186548\n",
      "infiltrated: 7.236810527204565\n",
      "tugs: 7.150932142170766\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all words in the vocabulary and predict scores\n",
    "word_scores = []\n",
    "\n",
    "for word in model.vocab.idx2word:\n",
    "    score = predict_score(model, word, device)\n",
    "    word_scores.append((word, score))\n",
    "\n",
    "# Sort the words by their predicted scores in descending order\n",
    "word_scores = sorted(word_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the top 5 words with the highest scores\n",
    "top_5_words = word_scores[:5]\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 5 words with the highest scores:\")\n",
    "for word, score in top_5_words:\n",
    "    print(f\"{word}: {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9dbc4e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centuries populous kwh nebuchadrezzar zionists palestine cormen ungovernable majuscule gurion bethlehem amicable dioceses nehemiah hellenized babylonian gaza plotarea damasus nicea\n"
     ]
    }
   ],
   "source": [
    "words = ['centuries', 'populous', 'kwh', 'nebuchadrezzar', 'zionists', 'palestine', 'cormen', 'ungovernable', 'majuscule', 'gurion', 'bethlehem', 'amicable', 'dioceses', 'nehemiah', 'hellenized', 'babylonian', 'gaza', 'plotarea', 'damasus', 'nicea', 'waismann', 'sephardic', 'century', 'conclave', 'golan', 'antioch', 'teutonic', 'szil', 'elector', 'emesa', 'manasseh', 'phoenicians', 'burkert', 'germain', 'levant', 'anastasius', 'syriac', 'suffrage', 'hierarchs', 'suzerainty', 'geographer', 'codepoint', 'knesset', 'landtag', 'jamnia', 'hyrcanus', 'martyred', 'francia', 'berbers', 'koresh']\n",
    "\n",
    "n = 20\n",
    "print(\" \".join(words[:n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd990b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: Show HN: Buyidentities.com\n",
      "Tokens: ['show', 'hn', 'buyidentities', 'com']\n",
      "2025-04-17 10:38:09 | DropoutDisco | INFO     | [vocabulary.py:110] | Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-17 10:38:09 | DropoutDisco | INFO     | [vocabulary.py:123] | 📚 Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:📚 Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token indices: [2194, 38072, 0, 2806]\n",
      "Embeddings: tensor([[ 1.9466e+00,  4.3013e-01, -2.0402e+00,  4.4432e-01, -2.1084e+00,\n",
      "          2.7667e-01,  9.9176e-01,  8.9653e-01,  2.2319e-01, -1.3121e+00,\n",
      "          4.9519e-02, -1.8727e-01,  7.8188e-01, -2.2465e+00, -1.2707e-01,\n",
      "         -3.1723e-03,  1.3502e+00,  1.2984e+00, -1.1449e-01, -1.8985e-01,\n",
      "         -9.1769e-01, -1.3087e-01,  2.9257e-01, -8.9975e-01, -5.0819e-01,\n",
      "          8.5919e-01, -3.4376e+00,  7.6520e-01,  1.3828e+00,  2.9100e-01,\n",
      "          1.2134e+00,  7.9240e-01,  4.1146e-01, -1.7667e+00, -2.8801e+00,\n",
      "          2.5893e-01, -1.6380e-01,  3.6928e-01, -5.3449e-01, -1.8259e-01,\n",
      "          2.1736e+00, -2.1261e+00,  1.3076e+00, -3.2017e+00,  3.3742e-01,\n",
      "         -1.2346e+00,  1.3087e+00,  2.1186e+00, -1.7250e+00,  4.6178e-01,\n",
      "         -5.8093e-01, -1.4234e-01,  1.6505e+00, -1.9864e-02,  1.4873e-01,\n",
      "         -6.8776e-01,  1.3533e+00, -8.1330e-01,  9.0072e-01,  7.5148e-01,\n",
      "          7.8481e-01, -8.7399e-01, -4.5775e-02, -1.1776e-01, -1.1477e+00,\n",
      "         -1.0841e+00, -2.2613e-01, -1.0361e+00, -2.7476e+00, -1.9721e-01,\n",
      "          3.0830e-01,  1.5420e+00,  5.3548e-01, -8.6643e-01, -1.3576e+00,\n",
      "         -3.4131e-01, -7.8010e-01,  2.7584e-01, -2.1082e+00,  1.4421e+00,\n",
      "         -3.5891e-01,  3.4663e-01, -2.2047e+00,  2.2207e-01,  1.0311e+00,\n",
      "          4.5479e-02, -1.8573e-01,  3.2395e+00,  2.2179e+00, -3.6438e-01,\n",
      "         -3.9641e+00, -1.1545e+00, -1.8063e+00, -5.1812e-01, -1.2691e+00,\n",
      "         -1.1358e+00, -1.8693e+00, -4.1736e-01, -1.6325e+00,  1.2383e+00,\n",
      "          1.8732e+00,  2.1580e+00,  2.0025e+00,  1.4012e+00,  5.4559e-01,\n",
      "          3.0559e+00, -6.0173e-01, -1.1741e-01,  7.7093e-01, -1.5293e+00,\n",
      "         -8.6734e-01, -3.1346e-01,  3.7775e-01,  2.7541e-02,  2.9125e+00,\n",
      "          1.9542e+00,  4.5796e-01,  1.0115e+00,  2.5596e+00, -3.1819e+00,\n",
      "          5.9765e-01,  2.4012e-01, -1.5479e+00, -9.3829e-01, -1.4166e+00,\n",
      "         -4.5177e-01, -5.5938e-01, -1.1085e+00],\n",
      "        [-1.5667e+00,  7.8214e-01, -5.7691e-01,  1.7717e+00, -1.2619e+00,\n",
      "         -7.4635e-01, -2.0530e+00, -3.2675e+00,  2.2365e+00,  1.7965e+00,\n",
      "          1.8474e+00, -1.2452e+00, -1.0552e+00, -1.3828e+00, -1.2093e-01,\n",
      "          5.2575e-01, -7.3024e-01,  1.7249e+00, -9.0794e-01,  2.0740e+00,\n",
      "         -1.3004e+00,  9.9530e-01,  2.8094e+00,  2.0003e+00,  1.4203e+00,\n",
      "          1.2538e+00,  1.8174e+00, -4.1344e-01, -1.5901e+00, -7.2424e-01,\n",
      "         -1.8028e+00,  7.6485e-01,  3.0785e+00, -2.1158e+00, -2.5810e+00,\n",
      "         -1.7968e+00,  1.2634e+00, -5.6055e-01,  2.6529e+00,  1.6614e+00,\n",
      "          8.8387e-01,  1.1162e+00, -1.9246e+00,  2.0495e+00,  2.5541e+00,\n",
      "         -1.8288e-01, -3.1952e-01,  1.9389e-01,  2.9295e-01,  2.3871e+00,\n",
      "         -1.5910e-01, -8.3851e-02, -1.1991e+00,  1.3769e+00,  1.8589e-01,\n",
      "          1.5373e+00, -1.0242e-01,  1.6163e+00, -3.8749e-01, -1.8334e+00,\n",
      "         -1.2870e+00, -1.3007e+00,  1.5842e+00, -3.4426e+00,  4.2744e-01,\n",
      "         -1.9654e-01, -8.9006e-01,  9.2072e-01, -1.2219e+00, -1.3493e+00,\n",
      "         -5.0283e-01,  1.8578e-01, -1.8086e+00,  1.3997e+00, -6.6792e-01,\n",
      "          1.0227e+00, -2.5120e-01,  3.6870e-01, -2.4321e-01,  3.1586e+00,\n",
      "         -2.3257e+00, -1.1646e+00, -1.4097e+00,  7.3020e-01, -1.6028e+00,\n",
      "         -3.1185e+00,  5.9540e-01,  2.1864e-01, -3.5172e+00, -1.1444e+00,\n",
      "         -8.4306e-01,  1.1800e+00, -8.7577e-01, -3.7724e+00,  2.4406e+00,\n",
      "         -4.1250e+00, -1.6616e+00, -1.6737e+00,  2.8017e+00, -1.3613e+00,\n",
      "          5.0336e-01,  3.3219e+00, -2.1909e+00, -3.0694e+00,  1.1974e+00,\n",
      "          9.7304e-01,  1.8434e+00,  2.3926e-01,  2.2560e+00, -1.3331e+00,\n",
      "         -1.8256e+00,  2.3941e+00,  1.9991e+00, -1.8125e+00, -1.2573e+00,\n",
      "         -4.1430e-01, -9.1206e-01,  2.1679e+00, -1.0121e+00, -4.3239e-01,\n",
      "          2.2385e+00, -1.9689e+00, -1.2162e+00, -4.4275e-01, -1.4251e-02,\n",
      "          3.4801e-01,  3.3441e-01, -1.9466e+00],\n",
      "        [ 1.0666e-01, -4.4749e-01,  3.0592e-03,  3.5487e+00,  7.1915e-01,\n",
      "         -1.5275e-01, -9.7117e-02, -2.9817e-01, -8.1239e-02, -9.7092e-01,\n",
      "          2.6279e-01, -7.2066e-02, -1.0161e-01, -4.9066e-01, -3.5642e-01,\n",
      "         -3.6854e-01, -4.6020e-01,  5.0026e+00,  5.7813e-01,  3.1274e-01,\n",
      "         -4.7810e-01,  2.0687e-01,  2.1394e-01, -2.1041e-01, -2.7540e-01,\n",
      "          6.5973e-01, -1.8037e-01, -8.4731e-01, -2.4666e+00, -3.1706e-02,\n",
      "         -2.1351e-01, -8.8217e-02,  4.6630e-01,  5.3028e-01,  8.1138e-02,\n",
      "         -3.4815e-01, -8.5351e-02,  2.3976e-01,  2.2374e+00, -9.5145e-02,\n",
      "          3.7266e+00,  2.1168e-01,  2.7202e-01,  4.3517e-01,  5.4033e-02,\n",
      "          6.7890e-02,  4.5308e-02, -3.9729e-01,  2.5600e+00,  4.9277e-01,\n",
      "          5.0605e-01, -3.1364e-01,  1.3167e-01, -7.4294e-01, -2.0006e-01,\n",
      "          7.5181e-01, -3.0951e-01, -6.3007e-01, -6.4775e-01,  1.0061e-01,\n",
      "          1.7763e-01, -7.7472e-01, -2.7303e-01, -6.9010e-01, -6.1744e-01,\n",
      "         -3.3275e-01,  1.1180e-01,  4.3017e-01, -5.2812e-01,  6.3297e-01,\n",
      "          1.3869e-01,  1.8332e+00,  5.1611e-01, -4.1663e-01, -1.2248e-01,\n",
      "         -7.2120e-01,  8.8940e-02, -8.7361e-01, -5.3072e-01, -5.1299e-01,\n",
      "          4.1828e-01,  4.1859e-02,  1.2541e+00,  1.2910e-01, -1.6070e+00,\n",
      "         -2.5055e-01,  2.0293e-01,  6.7893e-01, -3.0937e-01,  2.0754e-02,\n",
      "         -5.6050e+00,  2.6165e+00,  2.7026e-01, -6.5599e-01, -5.9977e-01,\n",
      "         -1.5575e-01,  6.7848e-01, -1.9105e-01,  1.0062e+00,  1.6024e+00,\n",
      "         -5.8674e-01,  1.4691e-01,  2.3959e-01, -3.4561e-01,  2.2185e-01,\n",
      "         -2.2944e-01, -5.1091e-01,  2.4504e-01, -1.2289e+00,  1.1568e-01,\n",
      "          2.3749e-01, -6.5189e-01, -1.4281e-01,  4.9357e-01, -1.0489e-01,\n",
      "         -4.2840e-02,  6.0504e-01,  7.5009e-02, -1.8288e-01, -1.7912e+00,\n",
      "         -3.2186e-01, -3.6232e-02, -2.0779e-01,  5.1928e-01,  3.7479e-01,\n",
      "          4.1132e-01, -3.7719e-01, -6.8777e-01],\n",
      "        [-7.2112e-01, -3.9909e-01, -5.9451e-01,  2.5383e+00, -2.0145e+00,\n",
      "          6.0613e-01,  3.6847e-01, -2.9590e-01,  9.0987e-01, -7.8571e-01,\n",
      "          7.4591e-01, -4.2301e+00,  1.4296e+00, -1.0402e+00,  2.6900e+00,\n",
      "          4.5009e-01, -1.1203e+00,  7.7857e-01, -1.3797e+00,  1.1300e+00,\n",
      "         -5.1332e-01, -4.8499e-01,  5.6609e-01,  4.0330e+00, -2.8951e+00,\n",
      "         -1.6638e+00, -1.6608e+00, -1.7574e+00, -2.0807e+00, -1.1936e-01,\n",
      "         -1.2634e+00, -1.6363e+00, -1.5997e+00, -3.2901e+00, -1.6680e+00,\n",
      "         -2.7372e-01,  4.6559e-01,  5.4463e+00, -2.8337e-01, -1.7683e+00,\n",
      "          6.7869e-01,  2.1238e+00,  1.5891e-01, -3.4390e-04,  1.6032e+00,\n",
      "         -1.2022e+00,  2.7277e-01,  5.1740e-01,  1.5432e+00,  3.8428e-01,\n",
      "          1.9242e-02,  2.8965e+00, -1.7932e-01,  3.3554e-01,  2.0593e+00,\n",
      "         -6.0717e-01, -1.8294e+00, -2.9545e+00,  1.2066e+00, -2.3541e+00,\n",
      "          1.7533e+00, -1.2602e+00,  2.1070e-01,  2.0741e+00, -1.9738e+00,\n",
      "          5.3876e-01,  7.5917e-01,  1.6107e+00,  1.9651e+00,  1.3642e+00,\n",
      "         -2.8806e-01,  1.7388e+00, -1.6522e-02,  2.5029e-01, -1.2816e+00,\n",
      "         -4.1292e-01, -1.2569e-01, -1.3274e+00, -1.1128e+00,  4.8661e-01,\n",
      "         -1.1139e+00, -2.6534e+00, -1.5986e+00,  2.1205e-01, -5.9854e-01,\n",
      "          1.2666e-01, -1.2050e+00, -9.7301e-01,  4.9259e-01,  5.0122e-01,\n",
      "         -5.0559e+00,  1.8880e+00, -4.3579e+00,  1.0886e+00,  1.9161e+00,\n",
      "          1.3028e-01,  2.3832e+00, -3.0725e-01,  2.1250e+00, -2.3578e+00,\n",
      "          1.9045e+00,  5.4339e-01, -2.1213e+00,  9.4435e-01, -1.7967e+00,\n",
      "          2.1092e-01, -3.0715e-01,  7.5448e-01, -5.7632e-01, -5.8077e-01,\n",
      "         -1.6826e+00,  1.8996e+00, -2.4193e+00,  1.4491e+00, -1.2426e+00,\n",
      "         -1.6369e+00, -8.9155e-01,  5.1826e-01,  2.1166e+00, -1.4683e+00,\n",
      "          5.2440e-01,  1.3618e+00, -3.4798e-01, -2.4109e+00, -2.2999e+00,\n",
      "          8.1323e-03, -1.9149e-01, -3.8314e-02]], device='cuda:0')\n",
      "Averaged embedding: torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "train_df.head(5)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['title'].tolist(),\n",
    "    targets=train_df['score'].tolist(),\n",
    "    vocab=model.vocab  # Use the model's vocabulary\n",
    ")\n",
    "\n",
    "# Create dataloaders with the custom collate function\n",
    "batch_size = 10\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=make_collate_fn(model, device)\n",
    ")\n",
    "\n",
    "item = 2\n",
    "\n",
    "# Inspect raw text inputs\n",
    "sample_text = train_df['title'].iloc[item]\n",
    "print(f\"Raw text: {sample_text}\")\n",
    "\n",
    "# Tokenize the text\n",
    "#tokens = sample_text.lower().split()\n",
    "tokens = preprocess_text(sample_text)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Convert tokens to indices using the vocabulary\n",
    "vocab = vocab.load_vocab(\"../models/word2vec/text8_vocab_NWAll_MF5.json\")\n",
    "indices = [vocab.get_index(token) for token in tokens]\n",
    "print(f\"Token indices: {indices}\")\n",
    "\n",
    "# Get embeddings for the token indices\n",
    "token_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "embeddings = model.embedding(token_tensor)\n",
    "print(f\"Embeddings: {embeddings[0]}\")\n",
    "\n",
    "# Average the embeddings\n",
    "avg_embedding = embeddings.mean(dim=1)\n",
    "print(f\"Averaged embedding: {avg_embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "cd93f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:55:04 | DropoutDisco | INFO     | [vocabulary.py:110] | Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:Attempting to load vocabulary from: ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:55:04 | DropoutDisco | INFO     | [vocabulary.py:123] | 📚 Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DropoutDisco:📚 Vocab loaded (71,291 words) from ../models/word2vec/text8_vocab_NWAll_MF5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'prince' and 'queen': 0.3710\n"
     ]
    }
   ],
   "source": [
    "# Load the vocabulary\n",
    "vocab = vocab.load_vocab(\"../models/word2vec/text8_vocab_NWAll_MF5.json\")\n",
    "\n",
    "# Load the CBOW model state\n",
    "cbow_state = torch.load(\"../models/word2vec/CBOW_D128_W5_NWAll_MF5_E15_LR0.001_BS512/model_state.pth\", map_location=torch.device('cpu'))\n",
    "\n",
    "# Extract the embeddings\n",
    "embeddings = cbow_state['embeddings.weight']\n",
    "\n",
    "# Define a function to compute cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return torch.dot(vec1, vec2) / (torch.norm(vec1) * torch.norm(vec2))\n",
    "\n",
    "# Check similarity between words\n",
    "def check_word_similarity(word1, word2, vocab, embeddings):\n",
    "    idx1 = vocab.get_index(word1)\n",
    "    idx2 = vocab.get_index(word2)\n",
    "    vec1 = embeddings[idx1]\n",
    "    vec2 = embeddings[idx2]\n",
    "    similarity = cosine_similarity(vec1, vec2)\n",
    "    return similarity.item()\n",
    "\n",
    "# Example: Check similarity between two words\n",
    "word1 = \"prince\"\n",
    "word2 = \"queen\"\n",
    "similarity = check_word_similarity(word1, word2, vocab, embeddings)\n",
    "print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a9049f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the two sentences: 0.3710\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Compare the mean embeddings of two user-input sentences\n",
    "sentence1 = \"prince\"\n",
    "sentence2 = \"queen\"\n",
    "\n",
    "# Preprocess and tokenize the sentences\n",
    "tokens1 = preprocess_text(sentence1)\n",
    "tokens2 = preprocess_text(sentence2)\n",
    "\n",
    "# Convert tokens to indices using the vocabulary\n",
    "indices1 = [vocab.get_index(token) for token in tokens1]\n",
    "indices2 = [vocab.get_index(token) for token in tokens2]\n",
    "\n",
    "# Get embeddings for the token indices\n",
    "token_tensor1 = torch.tensor(indices1, dtype=torch.long).unsqueeze(0).to(device)\n",
    "token_tensor2 = torch.tensor(indices2, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "embeddings1 = model.embedding(token_tensor1)\n",
    "embeddings2 = model.embedding(token_tensor2)\n",
    "\n",
    "# Average the embeddings\n",
    "avg_embedding1 = embeddings1.mean(dim=1)\n",
    "avg_embedding2 = embeddings2.mean(dim=1)\n",
    "\n",
    "# Print the averaged embeddings\n",
    "#print(f\"Averaged embedding for sentence 1: {avg_embedding1}\")\n",
    "#print(f\"Averaged embedding for sentence 2: {avg_embedding2}\")\n",
    "\n",
    "# Compute cosine similarity between the two averaged embeddings\n",
    "cos_sim = cosine_similarity(avg_embedding1.squeeze(), avg_embedding2.squeeze())\n",
    "print(f\"Cosine similarity between the two sentences: {cos_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model state pth\n",
    "model_path = \"../models/text_regression_model.pth\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
