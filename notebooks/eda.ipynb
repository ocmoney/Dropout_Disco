{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Disco - Hacker News EDA\n",
    "\n",
    "## üìä Exploratory Data Analysis (EDA)\n",
    "\n",
    "This notebook provides an exploratory data analysis of the Hacker News dataset, focusing on post titles and their upvote scores. The goal is to understand the data distribution, identify potential issues, and gather insights to inform the modeling process for predicting scores from titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÖ Table of Contents\n",
    "\n",
    "1. [Setup and Imports](#setup-and-imports)\n",
    "2. [Data Loading](#data-loading)\n",
    "3. [Basic Data Inspection](#basic-data-inspection)\n",
    "4. [Data Cleaning](#data-cleaning)\n",
    "5. [Feature Engineering](#feature-engineering)\n",
    "6. [Exploratory Data Analysis (EDA)](#exploratory-data-analysis-eda)\n",
    "7. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup and Imports\n",
    "\n",
    "Import necessary libraries and configure settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure plots\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "warnings.filterwarnings('ignore', category=UserWarning) # Hide specific warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Data Loading\n",
    "\n",
    "Connect to the PostgreSQL database and fetch the Hacker News data. \n",
    "\n",
    "**‚ö†Ô∏è Important:** \n",
    "1.  **Credentials:** Database credentials are loaded from environment variables for security.\n",
    "2.  **Data Size:** We start with a small sample size to ensure quick exploration, then gradually increase as needed.\n",
    "3.  **Error Handling:** Robust error handling ensures graceful failure in case of connection issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original connection string\n",
    "CONN_STRING = \"postgresql+psycopg2://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\"\n",
    "\n",
    "# Extract components\n",
    "conn_parts = CONN_STRING.split(\"://\")[1]  # Remove the protocol part\n",
    "user_pass, host_port_db = conn_parts.split(\"@\")\n",
    "\n",
    "# Split user and password\n",
    "user, password = user_pass.split(\":\")\n",
    "\n",
    "# Split host, port, and database name\n",
    "host_port, db_name = host_port_db.split(\"/\")\n",
    "host, port = host_port.split(\":\")\n",
    "\n",
    "# Store the extracted values\n",
    "DB_USER = user            # sy91dhb\n",
    "DB_PASSWORD = password    # g5t49ao\n",
    "DB_HOST = host            # 178.156.142.230\n",
    "DB_PORT = port            # 5432\n",
    "DB_NAME = db_name         # hd64m1ki\n",
    "\n",
    "print(f\"User: {DB_USER}\")\n",
    "print(f\"Password: {DB_PASSWORD}\")\n",
    "print(f\"Host: {DB_HOST}\")\n",
    "print(f\"Port: {DB_PORT}\")\n",
    "print(f\"Database: {DB_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout Disco - Database Connection & Table Listing\n",
    "# Copyright (c) 2024 Dropout Disco Team (Yurii, James, Ollie, Emil)\n",
    "# File: eda/00_db_connect_list_tables.py (or corresponding notebook cell)\n",
    "# Description: Connects to the database and lists available tables.\n",
    "# Created: 2024-07-27\n",
    "# Updated: 2024-07-27\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the database URI directly\n",
    "# !! In real projects, manage credentials securely (e.g., env variables, secrets manager) !!\n",
    "DB_URI = \"postgresql+psycopg2://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\"\n",
    "\n",
    "engine = create_engine(DB_URI)\n",
    "# --- Optional: Set up logging ---\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# Example: Show tables (PostgreSQL metadata)\n",
    "tables = pd.read_sql(\"\"\"\n",
    "    SELECT table_name\n",
    "    FROM information_schema.tables\n",
    "\"\"\", engine)\n",
    "\n",
    "# Display the tables\n",
    "print(\"Available tables:\")\n",
    "for index, row in tables.iterrows():\n",
    "    print(f\"- {row['table_name']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Basic Data Inspection\n",
    "\n",
    "Get a first look at the data structure, types, and check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_sql(\"\"\"\n",
    "  SELECT * FROM \"hacker_news\".\"items\" LIMIT 5\n",
    "\"\"\", engine)\n",
    "\n",
    "res.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùì Investigating Table Structure and Size (using `text()` and Pandas)\n",
    "\n",
    "Let's re-check the table sizes. We'll use SQLAlchemy's `connection.execute()` with the `text()` construct for getting scalar counts, as this is the standard way for direct execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text # Ensure text is imported\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# --- Configuration ---\n",
    "DB_URI = \"postgresql+psycopg2://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\"\n",
    "engine = create_engine(DB_URI)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# --- Get Counts using connection.execute() with text() ---\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        # Get total count\n",
    "        total_items_query = text(\"SELECT COUNT(*) FROM hacker_news.items;\")\n",
    "        total_items_count = connection.execute(total_items_query).scalar()\n",
    "        print(f\"Total rows in 'hacker_news.items' (using text()): {total_items_count:,}\")\n",
    "\n",
    "        # Get story count\n",
    "        story_items_query = text(\"SELECT COUNT(*) FROM hacker_news.items WHERE type = 'story';\")\n",
    "        story_items_count = connection.execute(story_items_query).scalar()\n",
    "        print(f\"Rows with type='story' in 'hacker_news.items' (using text()): {story_items_count:,}\")\n",
    "\n",
    "        # Optional: Check a partition count\n",
    "        # partition_name = 'items_by_month_2023_12'\n",
    "        # partition_story_query = text(f\"SELECT COUNT(*) FROM hacker_news.\\\"{partition_name}\\\" WHERE type = 'story';\")\n",
    "        # partition_story_count = connection.execute(partition_story_query).scalar()\n",
    "        # print(f\"Rows with type='story' in partition '{partition_name}' (using text()): {partition_story_count:,}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error querying table counts using text(): {e}\")\n",
    "\n",
    "# --- Alternative: Get Counts using pandas.read_sql_query ---\n",
    "# Pandas often handles raw strings directly for SELECT queries\n",
    "try:\n",
    "    total_count_df = pd.read_sql_query(\"SELECT COUNT(*) as count FROM hacker_news.items;\", engine)\n",
    "    total_items_count_pd = total_count_df['count'].iloc[0]\n",
    "    print(f\"Total rows in 'hacker_news.items' (using pandas): {total_items_count_pd:,}\")\n",
    "\n",
    "    story_count_df = pd.read_sql_query(\"SELECT COUNT(*) as count FROM hacker_news.items WHERE type = 'story';\", engine)\n",
    "    story_items_count_pd = story_count_df['count'].iloc[0]\n",
    "    print(f\"Rows with type='story' in 'hacker_news.items' (using pandas): {story_items_count_pd:,}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error querying table counts using pandas: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Data Loading and Initial Inspection\n",
    "\n",
    "Load a sample of `story` items from the `hacker_news.items` table using Pandas. Then perform a basic inspection: view the first few rows, check data types, and count missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text # Ensure text is available if needed elsewhere\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Database Connection (Assuming 'engine' is already created) ---\n",
    "# Ensure 'engine' is defined from your previous connection setup cells.\n",
    "\n",
    "# --- Parameters ---\n",
    "SAMPLE_SIZE = 1_000_000 # Adjust as needed\n",
    "\n",
    "# --- Define the query string - ADDED 'by' column ---\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    id,\n",
    "    title,\n",
    "    score,\n",
    "    \"by\"  -- Include the author column (use quotes if the name is reserved/case-sensitive)\n",
    "FROM\n",
    "    hacker_news.items\n",
    "WHERE\n",
    "    type = 'story'\n",
    "ORDER BY\n",
    "    RANDOM() -- Use RANDOM() for a more representative random sample\n",
    "LIMIT {SAMPLE_SIZE};\n",
    "\"\"\"\n",
    "\n",
    "# --- Load data using pandas ---\n",
    "print(f\"Attempting to load {SAMPLE_SIZE} random stories including author...\")\n",
    "try:\n",
    "    df_stories = pd.read_sql(query, engine)\n",
    "    print(f\"Successfully loaded {len(df_stories)} stories using pandas.\")\n",
    "\n",
    "    # --- Basic Inspection ---\n",
    "    print(\"\\n--- First 5 Stories ---\")\n",
    "    display(df_stories.head())\n",
    "\n",
    "    print(\"\\n--- Data Info ---\")\n",
    "    df_stories.info() # Check dtypes and non-null counts including 'by'\n",
    "\n",
    "    print(\"\\n--- Missing Values ---\")\n",
    "    display(df_stories.isnull().sum()) # Check missing values including 'by'\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading stories using pandas: {e}\")\n",
    "    df_stories = pd.DataFrame() # Initialize empty df on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Ensure numpy is imported for log1p\n",
    "\n",
    "# --- Clean DataFrame for Analysis ---\n",
    "# Create a new DataFrame containing only rows with non-missing 'title' and 'score'\n",
    "# as these are essential for the primary goal (predicting score from title) and\n",
    "# for analyzing their distributions meaningfully.\n",
    "\n",
    "if 'df_stories' in locals() and not df_stories.empty:\n",
    "    initial_rows = len(df_stories)\n",
    "    print(f\"Starting with {initial_rows:,} stories in the loaded sample.\")\n",
    "\n",
    "    # Drop rows where either 'title' or 'score' is missing\n",
    "    df_stories_cleaned = df_stories.dropna(subset=['title', 'score']).copy() # Use .copy() to avoid SettingWithCopyWarning later\n",
    "\n",
    "    rows_after_cleaning = len(df_stories_cleaned)\n",
    "    rows_dropped = initial_rows - rows_after_cleaning\n",
    "    print(f\"Removed {rows_dropped:,} rows with missing 'title' or 'score'.\")\n",
    "    print(f\"Working with {rows_after_cleaning:,} cleaned stories for subsequent analysis.\")\n",
    "\n",
    "    # Optional: Check missing values again on the cleaned DataFrame (should be 0 for title/score)\n",
    "    # print(\"\\n--- Missing Values in Cleaned Data ---\")\n",
    "    # display(df_stories_cleaned.isnull().sum())\n",
    "\n",
    "    # Prepare log-transformed score (often needed for analysis/modeling due to skew)\n",
    "    # Add this column early so it's available for all subsequent steps\n",
    "    df_stories_cleaned['score_log1p'] = np.log1p(df_stories_cleaned['score'])\n",
    "\n",
    "else:\n",
    "    print(\"Error: df_stories DataFrame not found or is empty. Cannot proceed with cleaning.\")\n",
    "    # Ensure df_stories_cleaned exists but is empty to prevent errors below\n",
    "    df_stories_cleaned = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Analyze Score Distribution ---\n",
    "# Now use the 'df_stories_cleaned' DataFrame for analysis\n",
    "\n",
    "if 'df_stories_cleaned' in locals() and not df_stories_cleaned.empty:\n",
    "    print(\"\\n--- Score Summary Statistics (Cleaned Data) ---\")\n",
    "    # Display descriptive statistics for both raw and log-transformed scores\n",
    "    display(df_stories_cleaned[['score', 'score_log1p']].describe())\n",
    "\n",
    "    # --- Plotting Distributions ---\n",
    "    print(\"\\n--- Plotting Score Distributions ---\")\n",
    "    plt.figure(figsize=(18, 6)) # Use a wider figure for side-by-side plots\n",
    "\n",
    "    # Plot 1: Raw Score Distribution\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.histplot(df_stories_cleaned['score'], bins=50, kde=False) # kde=False might be clearer for highly skewed data\n",
    "    plt.title('Distribution of Raw Scores')\n",
    "    plt.xlabel('Upvote Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.yscale('log') # Use log scale on y-axis to see smaller bins better\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "\n",
    "\n",
    "    # Plot 2: Log-Transformed Score Distribution\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.histplot(df_stories_cleaned['score_log1p'], bins=50, kde=True)\n",
    "    plt.title('Distribution of Log(Score + 1)')\n",
    "    plt.xlabel('Log(Upvote Score + 1)')\n",
    "    plt.ylabel('Frequency') # Standard y-axis scale here\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "\n",
    "    # Plot 3: Box Plot of Log-Transformed Score\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.boxplot(y=df_stories_cleaned['score_log1p'])\n",
    "    plt.title('Box Plot of Log(Score + 1)')\n",
    "    plt.ylabel('Log(Upvote Score + 1)')\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Quantify Skewness ---\n",
    "    print(f\"\\nSkewness of raw scores: {df_stories_cleaned['score'].skew():.2f}\")\n",
    "    print(f\"Skewness of log(scores+1): {df_stories_cleaned['score_log1p'].skew():.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: df_stories_cleaned DataFrame not found or is empty. Cannot analyze score distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Score Statistics Deep Dive (1M Sample)\n",
    "\n",
    "Let's dissect the numbers from our **915,981** cleaned stories! ü§ì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*   **üî¢ Count:** A hefty dataset size (N=915,981) gives us solid ground for analysis.\n",
    "\n",
    "*   **‚è´ Raw Score (`score`):**\n",
    "    *   **Mean (13.97) vs. Median (2.0):** Big difference! The average score is pulled way up by a few high-flyers üöÄ compared to the typical post score. Clear sign of **positive skew**.\n",
    "    *   **Spread (Std Dev 57.6):** Huge variation! Scores are all over the place, mostly low but with some hitting the stratosphere ü™ê.\n",
    "    *   **Quartiles:** 75% of stories get 4 points or fewer (Q3=4.0). Half get just 2 points or less (Median=2.0). Low scores are definitely the norm! üëçüëé\n",
    "    *   **Range (0 to 3636):** Wow! From zero to hero, the score range is massive. Outliers are definitely a thing.\n",
    "\n",
    "*   **üìê Log-Transformed Score (`score_log1p`):**\n",
    "    *   **Mean (1.42) vs. Median (1.10):** Much closer now! The log transform did its magic ‚ú®, making the distribution more balanced.\n",
    "    *   **Spread (Std Dev 1.12):** Variation is now more in line with the average value. Much more manageable.\n",
    "    *   **Range (0 to ~8.2):** The scores are nicely compressed onto a smaller, more workable scale.\n",
    "\n",
    "*   **‚öñÔ∏è Skewness Values:**\n",
    "    *   Raw: **12.18** (Super skewed! üìà)\n",
    "    *   Log: **2.17** (Massive improvement! Still a slight positive skew, but way better. ‚úÖ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìâ Visualizing the Score Distributions\n",
    "\n",
    "Let's look at the pretty pictures! üñºÔ∏è\n",
    "\n",
    "*   **Raw Score Histogram (Log Y-axis):**\n",
    "    *   Confirms the skew visually. A huge pile-up of posts at the very bottom (scores 0-~50). üìâ\n",
    "    *   The log scale on the frequency (Y-axis) is key! It lets us see the *long tail* ‚Äì posts exist across all scores, but their numbers drop super fast. Like looking at city populations! üèôÔ∏è -> üèòÔ∏è -> üè†\n",
    "*   **Log(Score + 1) Histogram:**\n",
    "    *   Much more like a \"hill\" shape now! ‚õ∞Ô∏è Still leaning a bit right, but centered nicely around the median log score (~1.1).\n",
    "    *   This view makes it much easier to understand the \"typical\" transformed score and its spread. Much clearer! üëÄ\n",
    "*   **Log(Score + 1) Box Plot:**\n",
    "    *   The blue box shows where the middle 50% of posts lie (IQR ~0.7 to 1.6 in log-score). üì¶\n",
    "    *   The line inside is the median (~1.1).\n",
    "    *   Those little dots above the top line? Those are **outliers**, *even after* the log transform! üòÆ These posts significantly outperformed the rest. They are the superstars ‚ú®."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ú® Overall Summary & Implications\n",
    "\n",
    "So, what's the big picture for scores? ü§î\n",
    "\n",
    "*   **Concentration:** Most HN stories get very few upvotes. Success (high score) is rare. üéØ\n",
    "*   **Transformation Power:** Using `np.log1p` is **essential**! üôè It tames the wild skewness, making scores much easier to analyze and model. We'll definitely want to predict the log-transformed score or use models robust to skew.\n",
    "*   **Outlier Challenge:** Even log scores have outliers. Predicting these \"viral hits\" üí• will be tough and might skew some model error metrics (like RMSE). We need to keep this in mind during evaluation.\n",
    "\n",
    "This solid understanding of our target variable sets us up for the next phase: exploring the features! üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps: Feature Exploration!\n",
    "\n",
    "We've got a grip on the scores, now let's check out the ingredients we have to predict them:\n",
    "\n",
    "1.  **üì∞ Analyze Title Characteristics:**\n",
    "    *   How long are titles typically (chars & words)? Let's plot the distributions for `df_stories_cleaned`.\n",
    "    *   What are the most common words and phrases (unigrams, bigrams)? Let's run frequency analysis. üí¨\n",
    "2.  **‚úçÔ∏è Analyze Author (`by`) Characteristics:**\n",
    "    *   Who posts the most? (`value_counts()` on `by`).\n",
    "    *   Did cleaning remove any rows with missing authors? (Check `isnull().sum()` on `df_stories_cleaned['by']`).\n",
    "    *   Do top authors get better scores on average? (Group by `by`, then aggregate `score_log1p`). üßë‚Äçüíª -> üèÜ ?\n",
    "3.  **‚ÜîÔ∏è Explore Initial Relationships:**\n",
    "    *   Does title length actually correlate with `score_log1p` in our bigger sample? (Re-plot scatter).\n",
    "    *   Do titles with keywords like \"Show HN\", \"Ask HN\", \"Python\", \"AI\" get different scores? (Group by keyword presence, aggregate `score_log1p`). üè∑Ô∏è\n",
    "\n",
    "Let's dive into the titles next! üëá"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure df_stories_cleaned exists and is not empty\n",
    "if 'df_stories_cleaned' in locals() and not df_stories_cleaned.empty:\n",
    "\n",
    "    print(\"\\n--- Analyzing Title Length Characteristics ---\")\n",
    "\n",
    "    # Calculate lengths if not already present (should be from previous potential steps, but recalculate/ensure)\n",
    "    # Ensure title is string type before applying string methods\n",
    "    df_stories_cleaned['title_length_char'] = df_stories_cleaned['title'].astype(str).str.len()\n",
    "    df_stories_cleaned['title_length_words'] = df_stories_cleaned['title'].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "    # --- Display Summary Statistics for Lengths ---\n",
    "    print(\"\\n--- Title Length Summary Statistics ---\")\n",
    "    display(df_stories_cleaned[['title_length_char', 'title_length_words']].describe())\n",
    "\n",
    "    # --- Plotting Length Distributions ---\n",
    "    print(\"\\n--- Plotting Title Length Distributions ---\")\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Plot 1: Character Length\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df_stories_cleaned['title_length_char'], bins=50, kde=False)\n",
    "    plt.title('Distribution of Title Length (Characters)')\n",
    "    plt.xlabel('Number of Characters')\n",
    "    plt.ylabel('Frequency')\n",
    "    # Optional: Zoom in if the tail is too long\n",
    "    plt.xlim(0, df_stories_cleaned['title_length_char'].quantile(0.99))\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "\n",
    "    # Plot 2: Word Length\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(df_stories_cleaned['title_length_words'], bins=40, kde=False) # Adjust bins as needed\n",
    "    plt.title('Distribution of Title Length (Words)')\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    # Optional: Zoom in if the tail is too long\n",
    "    plt.xlim(0, df_stories_cleaned['title_length_words'].quantile(0.99))\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Error: df_stories_cleaned DataFrame not found or is empty. Cannot analyze title length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Ensure df_stories_cleaned exists and is not empty\n",
    "if 'df_stories_cleaned' in locals() and not df_stories_cleaned.empty:\n",
    "\n",
    "    print(\"\\n--- Analyzing Title Word Frequency (Unigrams) ---\")\n",
    "\n",
    "    # --- Basic Tokenization ---\n",
    "    # (Lowercase, find sequences of alphanumeric characters)\n",
    "    def simple_tokenize(text):\n",
    "        text = str(text).lower()\n",
    "        tokens = re.findall(r'\\b[a-z0-9]+\\b', text)\n",
    "        return tokens\n",
    "\n",
    "    # Apply tokenization to all titles and flatten the list\n",
    "    print(\"Tokenizing titles...\")\n",
    "    all_tokens = [token for title_tokens in df_stories_cleaned['title'].apply(simple_tokenize) for token in title_tokens]\n",
    "    print(f\"Found {len(all_tokens):,} total tokens.\")\n",
    "\n",
    "    # --- Count Token Frequencies ---\n",
    "    if all_tokens:\n",
    "        print(\"Counting token frequencies...\")\n",
    "        token_counts = Counter(all_tokens)\n",
    "        print(f\"Found {len(token_counts):,} unique tokens (vocabulary size).\")\n",
    "\n",
    "        # --- Display Top 50 Most Common Words ---\n",
    "        print(\"\\n--- Top 50 Most Common Words in Titles ---\")\n",
    "        # Convert Counter to DataFrame for better display in notebooks\n",
    "        common_words_df = pd.DataFrame(token_counts.most_common(50), columns=['Word', 'Frequency'])\n",
    "        display(common_words_df)\n",
    "\n",
    "        # --- Plot Word Frequency Distribution (Zipf's Law check) ---\n",
    "        print(\"\\n--- Plotting Word Frequency Distribution ---\")\n",
    "        word_freq = sorted(token_counts.values(), reverse=True)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.loglog(range(1, len(word_freq) + 1), word_freq) # Use log-log scale\n",
    "        plt.title('Word Frequency Distribution (Log-Log Scale)')\n",
    "        plt.xlabel('Rank of Word (Log)')\n",
    "        plt.ylabel('Frequency of Word (Log)')\n",
    "        plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "        plt.show()\n",
    "        print(\"Check if the plot roughly follows a straight line (indicative of Zipf's Law).\")\n",
    "\n",
    "    else:\n",
    "        print(\"No tokens were generated from the titles.\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: df_stories_cleaned DataFrame not found or is empty. Cannot analyze word frequency.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
